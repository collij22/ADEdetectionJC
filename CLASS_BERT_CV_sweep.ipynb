{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file variables\n",
    "\n",
    "#pretrained_bert = 'allenai/scibert_scivocab_uncased'\n",
    "#pretrained_bert = 'dmis-lab/biobert-base-cased-v1.2'\n",
    "#pretrained_bert = 'cimm-kzn/endr-bert'\n",
    "#pretrained_bert = 'SpanBERT/spanbert-base-cased'\n",
    "#pretrained_bert = 'emilyalsentzer/Bio_ClinicalBERT'\n",
    "#pretrained_bert = 'dmis-lab/biobert-v1.1'\n",
    "#pretrained_bert = 'allenai/biomed_roberta_base' \n",
    "#pretrained_bert = 'bert-large-uncased'\n",
    "#pretrained_bert = 'microsoft/BiomedNLP-BiomedBERT-large-uncased-abstract'\n",
    "pretrained_bert = 'michiyasunaga/BioLinkBERT-base'\n",
    "#pretrained_bert = 'microsoft/BiomedNLP-BiomedBERT-large-uncased-abstract'\n",
    "#pretrained_bert = 'microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext'\n",
    "#pretrained_bert = 'bert-base-uncased'\n",
    "#pretrained_bert = 'monologg/biobert_v1.1 pubmed'\n",
    "\n",
    "#TRAINING FILE\n",
    "use_huggingface = True\n",
    "training_file = \"**training**.csv\"\n",
    "# training_file = \"**training**.csv\"\n",
    "column1_AEtext = \"text\" #\"Example\"\n",
    "column2_AEflag = \"label\" #\"AE Flag\"\n",
    "train_test_split_percent = 0.2\n",
    "balance_multiplier = 1.0\n",
    "#batch_size = 3\n",
    "balance_me = True\n",
    "#balance_me = False\n",
    "\n",
    "#TEST FILE\n",
    "test_file = \"**test**.csv\"\n",
    "# test_file = \"**test**.csv\"\n",
    "test_column1_AEtext = \"Example\"\n",
    "test_column2_AEflag = \"AE Flag\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "#os.environ['WANDB_NOTEBOOK_NAME'] = 'BERTcomparison.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjon-l-collins\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'random'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = {\n",
    "        'name': 'val_loss',\n",
    "        'goal': 'minimize'\n",
    "}\n",
    "sweep_config['metric'] = metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-02-29 22:43:10.104756\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "print(str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: h06m7k9y\n",
      "Sweep URL: https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y\n"
     ]
    }
   ],
   "source": [
    "parameters_dict = {\n",
    "    'learning_rate': {\n",
    "        'values': [5e-6,5e-5]\n",
    "    },\n",
    "    'batch_size': {\n",
    "        'values': [8,16]\n",
    "        },\n",
    "    \n",
    "    'dropout': {\n",
    "          'values': [0.15,0.2,0.25]\n",
    "        },\n",
    "    \n",
    "    'epochs': {\n",
    "        'values': [10]},\n",
    "    \n",
    "    'weight_decay': {\n",
    "        'values': [0.01,0.001]\n",
    "    }   \n",
    " \n",
    "}\n",
    "sweep_config['parameters'] = parameters_dict\n",
    "pretrained_bert2 = pretrained_bert.replace('/','')\n",
    "sweep_id = wandb.sweep(sweep_config, project=f\"{pretrained_bert2}_{str(datetime.datetime.now())[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "#from tqdm.notebook import tqdm\n",
    "#from tqdm import tqdm\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, BertConfig, AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "use_huggingface = True\n",
    "if use_huggingface == True:\n",
    "    dataset = load_dataset(\"ade_corpus_v2\",\"Ade_corpus_v2_classification\")\n",
    "else:\n",
    "    dataset = pd.read_csv(training_file, encoding='unicode_escape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_huggingface == True:\n",
    "    #convert to pandas dataframe\n",
    "    df = pd.DataFrame(dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    16695\n",
       "1     6821\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[column2_AEflag].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_AEs = df[column2_AEflag].value_counts()[1]\n",
    "balance_number = int(num_AEs * balance_multiplier)\n",
    "\n",
    "# num_AEs = 3000\n",
    "# balance_number = int(num_AEs * balance_multiplier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_AEs = 6821\n",
      "balance_number = 6821\n",
      "balance_multiplier = 1.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"num_AEs = {num_AEs}\\nbalance_number = {balance_number}\\nbalance_multiplier = {balance_multiplier}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(balance_me == True):\n",
    "    d_NoAE = df[df[column2_AEflag]==0]\n",
    "    shuffle_d_NoAE = d_NoAE.sample(frac=1,random_state=42)[:balance_number]\n",
    "    d_AE = df[df[column2_AEflag]==1]\n",
    "    #d_AE = df[df[column2_AEflag]==1][:3000]\n",
    "    df = pd.concat([shuffle_d_NoAE,d_AE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    6821\n",
       "1    6821\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[column2_AEflag].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[column1_AEtext]\n",
    "y = df[column2_AEflag]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train, y_test = train_test_split(X,y, test_size=train_test_split_percent,stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2729,), (2729,))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape , y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val,X_test_data,y_val, y_test_data = train_test_split(X_test,y_test, test_size=0.1,stratify=y_test, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10913,), (273,), (10913,), (273,), (2456,), (2456,))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape,X_test_data.shape, y_train.shape,y_test_data.shape, X_val.shape,y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad41e40925514ee8bccada4dfca3c782",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/379 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\jonlc\\.cache\\huggingface\\hub\\models--michiyasunaga--BioLinkBERT-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e1afb671b264e9999420e2e54305a86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/225k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fd6b6f4af734918aa2dd7018d826c24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#tokenizer = BertTokenizer.from_pretrained(pretrained_bert, do_lower_case=True)\n",
    "#tokenizer = AutoTokenizer.from_pretrained(pretrained_bert, do_lower_case=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_bert, add_prefix_space=True, use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['We present a fatal case of subacute methanol toxicity with associated diffuse brain involvement, including bilateral putaminal necrosis and cerebral edema with ventricular compression.',\n",
       "       'The evaluation, causes, and differential diagnosis of alopecia in a black woman are discussed.',\n",
       "       'The development of safer and more effective means for VT control is progressing and is needed.',\n",
       "       ...,\n",
       "       'We report the case of a patient who developed polyserositis (pericardial effusion, pleural effusion, and pericarditis) after being started on clozapine, and whose symptoms remitted upon discontinuation of clozapine.',\n",
       "       'This is a case of pseudoephedrine-induced intracerebral hemorrhage in a patient with an underlying vascular malformation.',\n",
       "       'METHOD: Report of a case.'], dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "encoded_data_train = tokenizer.batch_encode_plus(X_train.values,\n",
    "                                                    add_special_tokens=True,\n",
    "                                                    return_attention_mask=True,\n",
    "                                                    pad_to_max_length=True,\n",
    "                                                    max_length=512,\n",
    "                                                    return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data_val = tokenizer.batch_encode_plus(X_val.values,\n",
    "                                                    add_special_tokens=True,\n",
    "                                                    return_attention_mask=True,\n",
    "                                                    pad_to_max_length=True,\n",
    "                                                    max_length=512,\n",
    "                                                    return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data_test = tokenizer.batch_encode_plus(X_test_data.values,\n",
    "                                                    add_special_tokens=True,\n",
    "                                                    return_attention_mask=True,\n",
    "                                                    pad_to_max_length=True,\n",
    "                                                    max_length=512,\n",
    "                                                    return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_train = encoded_data_train['input_ids']\n",
    "attention_masks_train = encoded_data_train['attention_mask']\n",
    "#labels = torch.tensor(dataset['AE Flag'].replace(label_dict).values)\n",
    "labels_train = torch.tensor(y_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_val = encoded_data_val['input_ids']\n",
    "attention_masks_val = encoded_data_val['attention_mask']\n",
    "#labels = torch.tensor(dataset['AE Flag'].replace(label_dict).values)\n",
    "labels_val = torch.tensor(y_val.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_test = encoded_data_test['input_ids']\n",
    "attention_masks_test = encoded_data_test['attention_mask']\n",
    "#labels = torch.tensor(dataset['AE Flag'].replace(label_dict).values)\n",
    "labels_test = torch.tensor(y_test_data.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, random_split\n",
    "dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n",
    "dataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)\n",
    "dataset_test = TensorDataset(input_ids_test, attention_masks_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ret_dataloader():\n",
    "    batch_size = wandb.config.batch_size\n",
    "    print(f\"batch_size = {batch_size}\")\n",
    "    dataloader_train = DataLoader(dataset_train, sampler=RandomSampler(dataset_train), batch_size=batch_size)\n",
    "    dataloader_validation = DataLoader(dataset_val, sampler = SequentialSampler(dataset_val), batch_size=batch_size)\n",
    "    #dataloader_test = DataLoader(dataset_test, sampler = SequentialSampler(dataset_test), batch_size=batch_size)\n",
    "    return dataloader_train, dataloader_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ret_model():\n",
    "    \n",
    "    configuration = AutoConfig.from_pretrained(pretrained_bert)\n",
    "    configuration.hidden_dropout_prob = wandb.config.dropout\n",
    "    configuration.attention_probs_dropout_prob = wandb.config.dropout\n",
    "    configuration.num_labels = 2\n",
    "    configuration.output_attentions = False\n",
    "    configuration.output_hidden_states = False\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(pretrained_bert,config=configuration)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "def ret_optim(model):\n",
    "    print(f\"learning_rate = {wandb.config.learning_rate}\")\n",
    "    optimizer = AdamW(model.parameters(),\n",
    "                      lr = wandb.config.learning_rate, \n",
    "                      eps = 1e-8,weight_decay = wandb.config.weight_decay)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ret_scheduler(optimizer, dataloader_train):\n",
    "    epochs = wandb.config.epochs\n",
    "    total_steps = len(dataloader_train) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                                num_warmup_steps = 0, \n",
    "                                                num_training_steps = total_steps)\n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(elapsed):\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def f1_score_func(preds,labels):\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return f1_score(labels_flat, preds_flat, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def train():\n",
    "    wandb.init()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model=ret_model()\n",
    "    model.to(device)\n",
    "    dataloader_train, dataloader_validation = ret_dataloader()\n",
    "    optimizer = ret_optim(model)\n",
    "    scheduler = ret_scheduler(optimizer, dataloader_train)\n",
    "    training_stats = []\n",
    "    total_t0 = time.time()\n",
    "    epochs = wandb.config.epochs\n",
    "    for epoch_i in range(0, epochs):\n",
    "        print(\"\")\n",
    "        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "        print('Training...')\n",
    "        t0 = time.time()\n",
    "        total_train_loss = 0\n",
    "        model.train()\n",
    "        for step, batch in enumerate(dataloader_train):\n",
    "            if step % 200 == 0 and not step == 0:\n",
    "                elapsed = format_time(time.time() - t0)\n",
    "                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(dataloader_train), elapsed))\n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "            model.zero_grad()\n",
    "            outputs = model(b_input_ids,\n",
    "                            token_type_ids=None,\n",
    "                            attention_mask=b_input_mask,\n",
    "                            labels=b_labels)\n",
    "            loss = outputs[0]\n",
    "            wandb.log({\"train_batch_loss\":loss.item()})\n",
    "            total_train_loss += loss.item()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        avg_train_loss = total_train_loss / len(dataloader_train)\n",
    "        training_time = format_time(time.time() - t0)\n",
    "        wandb.log({\"train_loss\": avg_train_loss, \"epoch\": epoch_i})\n",
    "        print(\"\")\n",
    "        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "        print(\"  Training epoch took: {:}\".format(training_time))\n",
    "        print(\"\")\n",
    "        print(\"Running Validation...\")\n",
    "        t0 = time.time()\n",
    "        model.eval()\n",
    "        total_eval_loss = 0\n",
    "        predictions, true_vals = [], []\n",
    "        #nb_eval_steps = 0\n",
    "        for batch in dataloader_validation:\n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(b_input_ids,\n",
    "                                token_type_ids=None,\n",
    "                                attention_mask=b_input_mask,\n",
    "                                labels=b_labels)\n",
    "                loss=outputs[0] #checkleul from here downwards\n",
    "                logits=outputs[1] \n",
    "            total_eval_loss += loss.item()\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "            predictions.append(logits)\n",
    "            true_vals.append(label_ids)\n",
    "\n",
    "        predictions = np.concatenate(predictions, axis=0) #added\n",
    "        true_vals = np.concatenate(true_vals, axis=0) #added\n",
    "        val_f1 = f1_score_func(predictions, true_vals) #added\n",
    "        wandb.log({\"val_f1\": val_f1, \"epoch\": epoch_i}) #added\n",
    "        print(\"  Average validation f1: {0:.2f}\".format(val_f1))\n",
    "        avg_val_loss = total_eval_loss / len(dataloader_validation)\n",
    "        wandb.log({\"val_loss\": avg_val_loss,\"epoch\": epoch_i})\n",
    "        print(\"  Average validation loss: {0:.2f}\".format(avg_val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: j8xw6ljy with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-06\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.001\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\AI projects\\Project-BERTcomparison\\wandb\\run-20240229_224335-j8xw6ljy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/j8xw6ljy' target=\"_blank\">divine-sweep-1</a></strong> to <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/j8xw6ljy' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/j8xw6ljy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21eebca163784e0894cf93f9cdadcc96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/559 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db7fc3715ec94d19ab1acdf4d9b640b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/433M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at michiyasunaga/BioLinkBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size = 8\n",
      "learning_rate = 5e-06\n",
      "\n",
      "======== Epoch 1 / 10 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of  1,365.    Elapsed: 0:01:00.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:43.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:27.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:03:10.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:53.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:35.\n",
      "\n",
      "  Average training loss: 0.35\n",
      "  Training epoch took: 0:05:09\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.94\n",
      "  Average validation loss: 0.26\n",
      "\n",
      "======== Epoch 2 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:42.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:25.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:07.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:02:50.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:31.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:13.\n",
      "\n",
      "  Average training loss: 0.24\n",
      "  Training epoch took: 0:04:47\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.94\n",
      "  Average validation loss: 0.30\n",
      "\n",
      "======== Epoch 3 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:42.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:24.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:05.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:02:48.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:30.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:11.\n",
      "\n",
      "  Average training loss: 0.19\n",
      "  Training epoch took: 0:04:46\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.23\n",
      "\n",
      "======== Epoch 4 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:41.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:23.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:05.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:02:46.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:28.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:10.\n",
      "\n",
      "  Average training loss: 0.17\n",
      "  Training epoch took: 0:04:46\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.26\n",
      "\n",
      "======== Epoch 5 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:44.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:29.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:13.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:02:57.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:41.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:24.\n",
      "\n",
      "  Average training loss: 0.15\n",
      "  Training epoch took: 0:04:59\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.26\n",
      "\n",
      "======== Epoch 6 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:43.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:25.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:07.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:02:49.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:31.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:14.\n",
      "\n",
      "  Average training loss: 0.13\n",
      "  Training epoch took: 0:04:49\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.23\n",
      "\n",
      "======== Epoch 7 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:42.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:24.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:06.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:02:48.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:30.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:12.\n",
      "\n",
      "  Average training loss: 0.12\n",
      "  Training epoch took: 0:04:47\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.28\n",
      "\n",
      "======== Epoch 8 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:42.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:28.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:14.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:03:00.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:46.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:32.\n",
      "\n",
      "  Average training loss: 0.12\n",
      "  Training epoch took: 0:05:10\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.25\n",
      "\n",
      "======== Epoch 9 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:46.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:31.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:15.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:02:59.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:42.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:27.\n",
      "\n",
      "  Average training loss: 0.11\n",
      "  Training epoch took: 0:05:01\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.27\n",
      "\n",
      "======== Epoch 10 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:43.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:27.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:11.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:02:54.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:37.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:20.\n",
      "\n",
      "  Average training loss: 0.10\n",
      "  Training epoch took: 0:04:55\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.28\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▃▃▃▄▄▄▅▅▅▆▆▆▆▆▆▇▇▇███</td></tr><tr><td>train_batch_loss</td><td>▄▃▁▁▄█▁▃▂▁▁▁▁▂▁▁▁▁▁▄▁▁▁▁▁▁▁▂▁▆▁▁▁▄▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▅▄▃▃▂▂▂▁▁</td></tr><tr><td>val_f1</td><td>▁▁▇▆▅█▆█▇▇</td></tr><tr><td>val_loss</td><td>▅█▁▅▄▂▆▃▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_batch_loss</td><td>0.00127</td></tr><tr><td>train_loss</td><td>0.09527</td></tr><tr><td>val_f1</td><td>0.95271</td></tr><tr><td>val_loss</td><td>0.27799</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">divine-sweep-1</strong> at: <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/j8xw6ljy' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/j8xw6ljy</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240229_224335-j8xw6ljy\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 0qswb39k with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.15\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-06\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.01\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\AI projects\\Project-BERTcomparison\\wandb\\run-20240229_233647-0qswb39k</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/0qswb39k' target=\"_blank\">wandering-sweep-2</a></strong> to <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/0qswb39k' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/0qswb39k</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at michiyasunaga/BioLinkBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size = 16\n",
      "learning_rate = 5e-06\n",
      "\n",
      "======== Epoch 1 / 10 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    683.    Elapsed: 0:03:02.\n",
      "  Batch   400  of    683.    Elapsed: 0:04:39.\n",
      "  Batch   600  of    683.    Elapsed: 0:06:00.\n",
      "\n",
      "  Average training loss: 0.34\n",
      "  Training epoch took: 0:06:34\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.93\n",
      "  Average validation loss: 0.19\n",
      "\n",
      "======== Epoch 2 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:22.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:43.\n",
      "  Batch   600  of    683.    Elapsed: 0:04:02.\n",
      "\n",
      "  Average training loss: 0.17\n",
      "  Training epoch took: 0:04:35\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.19\n",
      "\n",
      "======== Epoch 3 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:21.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:41.\n",
      "  Batch   600  of    683.    Elapsed: 0:04:01.\n",
      "\n",
      "  Average training loss: 0.14\n",
      "  Training epoch took: 0:04:35\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.20\n",
      "\n",
      "======== Epoch 4 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:21.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:41.\n",
      "  Batch   600  of    683.    Elapsed: 0:04:00.\n",
      "\n",
      "  Average training loss: 0.12\n",
      "  Training epoch took: 0:04:33\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.21\n",
      "\n",
      "======== Epoch 5 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:20.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:39.\n",
      "  Batch   600  of    683.    Elapsed: 0:04:00.\n",
      "\n",
      "  Average training loss: 0.11\n",
      "  Training epoch took: 0:04:35\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.21\n",
      "\n",
      "======== Epoch 6 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:26.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:52.\n",
      "  Batch   600  of    683.    Elapsed: 0:04:12.\n",
      "\n",
      "  Average training loss: 0.09\n",
      "  Training epoch took: 0:04:45\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.96\n",
      "  Average validation loss: 0.21\n",
      "\n",
      "======== Epoch 7 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:20.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:40.\n",
      "  Batch   600  of    683.    Elapsed: 0:04:00.\n",
      "\n",
      "  Average training loss: 0.09\n",
      "  Training epoch took: 0:04:32\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.24\n",
      "\n",
      "======== Epoch 8 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:19.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:38.\n",
      "  Batch   600  of    683.    Elapsed: 0:03:57.\n",
      "\n",
      "  Average training loss: 0.08\n",
      "  Training epoch took: 0:04:29\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.23\n",
      "\n",
      "======== Epoch 9 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:19.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:37.\n",
      "  Batch   600  of    683.    Elapsed: 0:03:56.\n",
      "\n",
      "  Average training loss: 0.07\n",
      "  Training epoch took: 0:04:29\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.24\n",
      "\n",
      "======== Epoch 10 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:20.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:39.\n",
      "  Batch   600  of    683.    Elapsed: 0:03:58.\n",
      "\n",
      "  Average training loss: 0.06\n",
      "  Training epoch took: 0:04:30\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.24\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▃▃▃▄▄▄▅▅▅▆▆▆▆▆▆▇▇▇███</td></tr><tr><td>train_batch_loss</td><td>▇▄▄▄▄▂▁▂▁▁▂▂▂▁▂▁█▂▁▁▁▆▁▁▅▁▁▂▅▃▁▁▁▂▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▂▂▁▁▁</td></tr><tr><td>val_f1</td><td>▁▆▆▇▇█▇▇▇▇</td></tr><tr><td>val_loss</td><td>▂▁▂▃▄▄█▇▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_batch_loss</td><td>0.00104</td></tr><tr><td>train_loss</td><td>0.0642</td></tr><tr><td>val_f1</td><td>0.95438</td></tr><tr><td>val_loss</td><td>0.23889</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">wandering-sweep-2</strong> at: <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/0qswb39k' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/0qswb39k</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240229_233647-0qswb39k\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: qsnenpn7 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-06\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.01\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\AI projects\\Project-BERTcomparison\\wandb\\run-20240301_002819-qsnenpn7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/qsnenpn7' target=\"_blank\">balmy-sweep-3</a></strong> to <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/qsnenpn7' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/qsnenpn7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at michiyasunaga/BioLinkBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size = 16\n",
      "learning_rate = 5e-06\n",
      "\n",
      "======== Epoch 1 / 10 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    683.    Elapsed: 0:01:22.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:44.\n",
      "  Batch   600  of    683.    Elapsed: 0:04:05.\n",
      "\n",
      "  Average training loss: 0.41\n",
      "  Training epoch took: 0:04:39\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.91\n",
      "  Average validation loss: 0.25\n",
      "\n",
      "======== Epoch 2 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:20.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:39.\n",
      "  Batch   600  of    683.    Elapsed: 0:04:01.\n",
      "\n",
      "  Average training loss: 0.20\n",
      "  Training epoch took: 0:04:34\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.94\n",
      "  Average validation loss: 0.19\n",
      "\n",
      "======== Epoch 3 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:20.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:42.\n",
      "  Batch   600  of    683.    Elapsed: 0:04:04.\n",
      "\n",
      "  Average training loss: 0.18\n",
      "  Training epoch took: 0:04:38\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.20\n",
      "\n",
      "======== Epoch 4 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:21.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:41.\n",
      "  Batch   600  of    683.    Elapsed: 0:04:00.\n",
      "\n",
      "  Average training loss: 0.15\n",
      "  Training epoch took: 0:04:33\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.26\n",
      "\n",
      "======== Epoch 5 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:19.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:39.\n",
      "  Batch   600  of    683.    Elapsed: 0:03:57.\n",
      "\n",
      "  Average training loss: 0.14\n",
      "  Training epoch took: 0:04:30\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.21\n",
      "\n",
      "======== Epoch 6 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:19.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:37.\n",
      "  Batch   600  of    683.    Elapsed: 0:03:56.\n",
      "\n",
      "  Average training loss: 0.13\n",
      "  Training epoch took: 0:04:29\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.23\n",
      "\n",
      "======== Epoch 7 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:20.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:39.\n",
      "  Batch   600  of    683.    Elapsed: 0:03:59.\n",
      "\n",
      "  Average training loss: 0.12\n",
      "  Training epoch took: 0:04:32\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.25\n",
      "\n",
      "======== Epoch 8 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:20.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:42.\n",
      "  Batch   600  of    683.    Elapsed: 0:04:02.\n",
      "\n",
      "  Average training loss: 0.11\n",
      "  Training epoch took: 0:04:35\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.24\n",
      "\n",
      "======== Epoch 9 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:21.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:43.\n",
      "  Batch   600  of    683.    Elapsed: 0:04:06.\n",
      "\n",
      "  Average training loss: 0.11\n",
      "  Training epoch took: 0:04:39\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.25\n",
      "\n",
      "======== Epoch 10 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:25.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:53.\n",
      "  Batch   600  of    683.    Elapsed: 0:04:21.\n",
      "\n",
      "  Average training loss: 0.11\n",
      "  Training epoch took: 0:04:57\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.25\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▃▃▃▄▄▄▅▅▅▆▆▆▆▆▆▇▇▇███</td></tr><tr><td>train_batch_loss</td><td>██▄▃▂▆▅▃▂▄▂▁▁▂▂▃▁▂▁▂▁▁▁▁▅▂▁▁▂▂▁▁▁▆▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▃▃▂▂▂▁▁▁▁</td></tr><tr><td>val_f1</td><td>▁▇▇▇██████</td></tr><tr><td>val_loss</td><td>▇▁▁█▃▅▇▆▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_batch_loss</td><td>0.00148</td></tr><tr><td>train_loss</td><td>0.11217</td></tr><tr><td>val_f1</td><td>0.94986</td></tr><tr><td>val_loss</td><td>0.24533</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">balmy-sweep-3</strong> at: <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/qsnenpn7' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/qsnenpn7</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240301_002819-qsnenpn7\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: p4ld1h49 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.01\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\AI projects\\Project-BERTcomparison\\wandb\\run-20240301_011843-p4ld1h49</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/p4ld1h49' target=\"_blank\">eager-sweep-4</a></strong> to <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/p4ld1h49' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/p4ld1h49</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at michiyasunaga/BioLinkBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size = 8\n",
      "learning_rate = 5e-05\n",
      "\n",
      "======== Epoch 1 / 10 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of  1,365.    Elapsed: 0:00:48.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:34.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:21.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:03:09.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:57.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:44.\n",
      "\n",
      "  Average training loss: 0.36\n",
      "  Training epoch took: 0:05:23\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.93\n",
      "  Average validation loss: 0.26\n",
      "\n",
      "======== Epoch 2 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:47.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:33.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:16.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:02:58.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:40.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:23.\n",
      "\n",
      "  Average training loss: 0.23\n",
      "  Training epoch took: 0:04:58\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.21\n",
      "\n",
      "======== Epoch 3 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:43.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:25.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:08.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:02:51.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:34.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:17.\n",
      "\n",
      "  Average training loss: 0.20\n",
      "  Training epoch took: 0:04:53\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.94\n",
      "  Average validation loss: 0.31\n",
      "\n",
      "======== Epoch 4 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:45.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:31.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:16.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:03:01.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:46.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:31.\n",
      "\n",
      "  Average training loss: 0.17\n",
      "  Training epoch took: 0:05:08\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.94\n",
      "  Average validation loss: 0.27\n",
      "\n",
      "======== Epoch 5 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:45.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:30.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:15.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:03:01.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:46.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:31.\n",
      "\n",
      "  Average training loss: 0.14\n",
      "  Training epoch took: 0:05:06\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.94\n",
      "  Average validation loss: 0.33\n",
      "\n",
      "======== Epoch 6 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:44.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:26.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:09.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:02:52.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:35.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:17.\n",
      "\n",
      "  Average training loss: 0.13\n",
      "  Training epoch took: 0:04:52\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.27\n",
      "\n",
      "======== Epoch 7 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:42.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:25.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:08.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:02:51.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:35.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:18.\n",
      "\n",
      "  Average training loss: 0.12\n",
      "  Training epoch took: 0:04:54\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.25\n",
      "\n",
      "======== Epoch 8 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:46.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:33.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:20.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:03:05.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:48.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:32.\n",
      "\n",
      "  Average training loss: 0.09\n",
      "  Training epoch took: 0:05:08\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.27\n",
      "\n",
      "======== Epoch 9 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:44.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:27.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:11.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:02:54.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:36.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:20.\n",
      "\n",
      "  Average training loss: 0.08\n",
      "  Training epoch took: 0:04:55\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.31\n",
      "\n",
      "======== Epoch 10 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:43.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:25.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:08.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:02:51.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:35.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:18.\n",
      "\n",
      "  Average training loss: 0.06\n",
      "  Training epoch took: 0:04:53\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7393834e85b14f4e8ba58cfe2c5a21bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.021 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.052064…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▃▃▃▄▄▄▅▅▅▆▆▆▆▆▆▇▇▇███</td></tr><tr><td>train_batch_loss</td><td>▃▁▄▄▆▅▄▆▁▁▁▁▁▂▆▄▁▁▄▁▁▁▃▁▆▇▁▁▁█▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▅▄▄▃▃▂▂▁▁</td></tr><tr><td>val_f1</td><td>▁▅▁▄▃▆▇█▇▇</td></tr><tr><td>val_loss</td><td>▄▁▆▄█▅▃▄▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_batch_loss</td><td>0.00162</td></tr><tr><td>train_loss</td><td>0.0577</td></tr><tr><td>val_f1</td><td>0.9515</td></tr><tr><td>val_loss</td><td>0.32034</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">eager-sweep-4</strong> at: <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/p4ld1h49' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/p4ld1h49</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240301_011843-p4ld1h49\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: bvoutc26 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-06\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.01\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\AI projects\\Project-BERTcomparison\\wandb\\run-20240301_021252-bvoutc26</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/bvoutc26' target=\"_blank\">wobbly-sweep-5</a></strong> to <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/bvoutc26' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/bvoutc26</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at michiyasunaga/BioLinkBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size = 8\n",
      "learning_rate = 5e-06\n",
      "\n",
      "======== Epoch 1 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:43.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:26.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:09.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:02:52.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:35.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:18.\n",
      "\n",
      "  Average training loss: 0.32\n",
      "  Training epoch took: 0:04:54\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.93\n",
      "  Average validation loss: 0.29\n",
      "\n",
      "======== Epoch 2 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:43.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:27.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:10.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:02:53.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:36.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:19.\n",
      "\n",
      "  Average training loss: 0.22\n",
      "  Training epoch took: 0:04:54\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.94\n",
      "  Average validation loss: 0.22\n",
      "\n",
      "======== Epoch 3 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:43.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:26.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:09.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:02:53.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:36.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:19.\n",
      "\n",
      "  Average training loss: 0.19\n",
      "  Training epoch took: 0:04:54\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.94\n",
      "  Average validation loss: 0.29\n",
      "\n",
      "======== Epoch 4 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:45.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:28.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:11.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:02:54.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:37.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:20.\n",
      "\n",
      "  Average training loss: 0.16\n",
      "  Training epoch took: 0:04:56\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.25\n",
      "\n",
      "======== Epoch 5 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:44.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:27.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:09.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:02:52.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:35.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:17.\n",
      "\n",
      "  Average training loss: 0.14\n",
      "  Training epoch took: 0:04:51\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.25\n",
      "\n",
      "======== Epoch 6 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:42.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:24.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:06.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:02:48.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:30.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:12.\n",
      "\n",
      "  Average training loss: 0.13\n",
      "  Training epoch took: 0:04:47\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.26\n",
      "\n",
      "======== Epoch 7 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:42.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:24.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:06.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:02:48.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:30.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:12.\n",
      "\n",
      "  Average training loss: 0.11\n",
      "  Training epoch took: 0:04:46\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.96\n",
      "  Average validation loss: 0.24\n",
      "\n",
      "======== Epoch 8 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:42.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:24.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:06.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:02:48.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:30.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:12.\n",
      "\n",
      "  Average training loss: 0.11\n",
      "  Training epoch took: 0:04:47\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.26\n",
      "\n",
      "======== Epoch 9 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:42.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:24.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:06.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:02:48.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:30.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:13.\n",
      "\n",
      "  Average training loss: 0.09\n",
      "  Training epoch took: 0:04:47\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.26\n",
      "\n",
      "======== Epoch 10 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:42.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:24.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:06.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:02:48.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:31.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:13.\n",
      "\n",
      "  Average training loss: 0.10\n",
      "  Training epoch took: 0:04:47\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.27\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▃▃▃▄▄▄▅▅▅▆▆▆▆▆▆▇▇▇███</td></tr><tr><td>train_batch_loss</td><td>▄▂▃▃▁▁▂▁▁▁▂█▁▆▁▄▁▇▅▁▂▁▁▁▁▁▃▄▁▁▂▁▅▁▁▁▁▁▂▁</td></tr><tr><td>train_loss</td><td>█▅▄▃▃▂▂▁▁▁</td></tr><tr><td>val_f1</td><td>▁▅▄▅▇▇█▇▇▇</td></tr><tr><td>val_loss</td><td>█▁█▄▄▅▄▅▅▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_batch_loss</td><td>0.00093</td></tr><tr><td>train_loss</td><td>0.09601</td></tr><tr><td>val_f1</td><td>0.95395</td></tr><tr><td>val_loss</td><td>0.26794</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">wobbly-sweep-5</strong> at: <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/bvoutc26' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/bvoutc26</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240301_021252-bvoutc26\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: wmnda8oc with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.15\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-06\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.001\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\AI projects\\Project-BERTcomparison\\wandb\\run-20240301_030513-wmnda8oc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/wmnda8oc' target=\"_blank\">absurd-sweep-6</a></strong> to <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/wmnda8oc' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/wmnda8oc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at michiyasunaga/BioLinkBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size = 16\n",
      "learning_rate = 5e-06\n",
      "\n",
      "======== Epoch 1 / 10 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    683.    Elapsed: 0:01:19.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:38.\n",
      "  Batch   600  of    683.    Elapsed: 0:03:57.\n",
      "\n",
      "  Average training loss: 0.30\n",
      "  Training epoch took: 0:04:29\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.17\n",
      "\n",
      "======== Epoch 2 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:19.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:37.\n",
      "  Batch   600  of    683.    Elapsed: 0:03:56.\n",
      "\n",
      "  Average training loss: 0.17\n",
      "  Training epoch took: 0:04:29\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.18\n",
      "\n",
      "======== Epoch 3 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:19.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:38.\n",
      "  Batch   600  of    683.    Elapsed: 0:03:56.\n",
      "\n",
      "  Average training loss: 0.14\n",
      "  Training epoch took: 0:04:29\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.18\n",
      "\n",
      "======== Epoch 4 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:19.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:37.\n",
      "  Batch   600  of    683.    Elapsed: 0:03:56.\n",
      "\n",
      "  Average training loss: 0.12\n",
      "  Training epoch took: 0:04:28\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.22\n",
      "\n",
      "======== Epoch 5 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:19.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:38.\n",
      "  Batch   600  of    683.    Elapsed: 0:03:56.\n",
      "\n",
      "  Average training loss: 0.11\n",
      "  Training epoch took: 0:04:29\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.23\n",
      "\n",
      "======== Epoch 6 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:19.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:37.\n",
      "  Batch   600  of    683.    Elapsed: 0:03:56.\n",
      "\n",
      "  Average training loss: 0.10\n",
      "  Training epoch took: 0:04:29\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.22\n",
      "\n",
      "======== Epoch 7 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:19.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:38.\n",
      "  Batch   600  of    683.    Elapsed: 0:03:56.\n",
      "\n",
      "  Average training loss: 0.08\n",
      "  Training epoch took: 0:04:29\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.22\n",
      "\n",
      "======== Epoch 8 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:19.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:37.\n",
      "  Batch   600  of    683.    Elapsed: 0:03:56.\n",
      "\n",
      "  Average training loss: 0.07\n",
      "  Training epoch took: 0:04:29\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.26\n",
      "\n",
      "======== Epoch 9 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:19.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:38.\n",
      "  Batch   600  of    683.    Elapsed: 0:03:56.\n",
      "\n",
      "  Average training loss: 0.07\n",
      "  Training epoch took: 0:04:29\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.96\n",
      "  Average validation loss: 0.23\n",
      "\n",
      "======== Epoch 10 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:19.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:37.\n",
      "  Batch   600  of    683.    Elapsed: 0:03:56.\n",
      "\n",
      "  Average training loss: 0.07\n",
      "  Training epoch took: 0:04:29\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.96\n",
      "  Average validation loss: 0.23\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91654322f46246aab3ea34f3afe0e63d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▃▃▃▄▄▄▅▅▅▆▆▆▆▆▆▇▇▇███</td></tr><tr><td>train_batch_loss</td><td>▇▃▄▂▃▁▄▃▂█▁▆▆▃▄▁▁▁▁▄▁▅▇▁▅▁▁▇▁▁▁▄▄▁▁▁▁▁▁▄</td></tr><tr><td>train_loss</td><td>█▄▃▃▂▂▁▁▁▁</td></tr><tr><td>val_f1</td><td>▁▃▆▄▄▆▆▅██</td></tr><tr><td>val_loss</td><td>▁▂▂▅▅▅▅█▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_batch_loss</td><td>0.00216</td></tr><tr><td>train_loss</td><td>0.06761</td></tr><tr><td>val_f1</td><td>0.956</td></tr><tr><td>val_loss</td><td>0.23094</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">absurd-sweep-6</strong> at: <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/wmnda8oc' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/wmnda8oc</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240301_030513-wmnda8oc\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: kgz2f79i with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-06\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.01\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\AI projects\\Project-BERTcomparison\\wandb\\run-20240301_035349-kgz2f79i</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/kgz2f79i' target=\"_blank\">astral-sweep-7</a></strong> to <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/kgz2f79i' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/kgz2f79i</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at michiyasunaga/BioLinkBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size = 8\n",
      "learning_rate = 5e-06\n",
      "\n",
      "======== Epoch 1 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:42.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:24.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:06.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:02:47.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:29.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:11.\n",
      "\n",
      "  Average training loss: 0.41\n",
      "  Training epoch took: 0:04:46\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.89\n",
      "  Average validation loss: 0.44\n",
      "\n",
      "======== Epoch 2 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:42.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:24.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:06.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:02:48.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:30.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:11.\n",
      "\n",
      "  Average training loss: 0.26\n",
      "  Training epoch took: 0:04:46\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.94\n",
      "  Average validation loss: 0.29\n",
      "\n",
      "======== Epoch 3 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:42.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:24.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:06.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:02:47.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:29.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:11.\n",
      "\n",
      "  Average training loss: 0.24\n",
      "  Training epoch took: 0:04:45\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.94\n",
      "  Average validation loss: 0.24\n",
      "\n",
      "======== Epoch 4 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:42.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:24.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:06.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:02:47.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:29.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:11.\n",
      "\n",
      "  Average training loss: 0.21\n",
      "  Training epoch took: 0:04:45\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.94\n",
      "  Average validation loss: 0.28\n",
      "\n",
      "======== Epoch 5 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:42.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:24.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:06.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:02:47.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:29.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:11.\n",
      "\n",
      "  Average training loss: 0.19\n",
      "  Training epoch took: 0:04:46\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.94\n",
      "  Average validation loss: 0.30\n",
      "\n",
      "======== Epoch 6 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:42.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:24.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:06.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:02:47.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:29.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:11.\n",
      "\n",
      "  Average training loss: 0.18\n",
      "  Training epoch took: 0:04:45\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.27\n",
      "\n",
      "======== Epoch 7 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:42.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:24.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:06.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:02:47.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:29.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:11.\n",
      "\n",
      "  Average training loss: 0.17\n",
      "  Training epoch took: 0:04:46\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.28\n",
      "\n",
      "======== Epoch 8 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:42.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:24.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:06.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:02:48.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:30.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:11.\n",
      "\n",
      "  Average training loss: 0.16\n",
      "  Training epoch took: 0:04:46\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.26\n",
      "\n",
      "======== Epoch 9 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:42.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:24.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:05.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:02:47.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:29.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:11.\n",
      "\n",
      "  Average training loss: 0.14\n",
      "  Training epoch took: 0:04:45\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.94\n",
      "  Average validation loss: 0.29\n",
      "\n",
      "======== Epoch 10 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:42.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:24.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:05.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:02:47.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:29.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:11.\n",
      "\n",
      "  Average training loss: 0.14\n",
      "  Training epoch took: 0:04:45\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.27\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1601197b2bc4906a64978919720da33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▃▃▃▄▄▄▅▅▅▆▆▆▆▆▆▇▇▇███</td></tr><tr><td>train_batch_loss</td><td>▅▄▂▃▇▁▅▅▁▄▁▁▁█▁▁▁▁▁▁▅▁▁▅▁▁▁▁▁▁▁▁▂▁▁▁▁▅▁▁</td></tr><tr><td>train_loss</td><td>█▄▄▃▂▂▂▁▁▁</td></tr><tr><td>val_f1</td><td>▁▇▇▇▇███▇█</td></tr><tr><td>val_loss</td><td>█▃▁▂▃▂▂▂▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_batch_loss</td><td>0.00062</td></tr><tr><td>train_loss</td><td>0.14329</td></tr><tr><td>val_f1</td><td>0.94984</td></tr><tr><td>val_loss</td><td>0.27264</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">astral-sweep-7</strong> at: <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/kgz2f79i' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/kgz2f79i</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240301_035349-kgz2f79i\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: wpbgm87f with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.15\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-06\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.01\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\AI projects\\Project-BERTcomparison\\wandb\\run-20240301_044512-wpbgm87f</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/wpbgm87f' target=\"_blank\">unique-sweep-8</a></strong> to <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/wpbgm87f' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/wpbgm87f</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at michiyasunaga/BioLinkBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size = 8\n",
      "learning_rate = 5e-06\n",
      "\n",
      "======== Epoch 1 / 10 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of  1,365.    Elapsed: 0:00:42.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:24.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:06.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:02:48.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:30.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:11.\n",
      "\n",
      "  Average training loss: 0.31\n",
      "  Training epoch took: 0:04:46\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.94\n",
      "  Average validation loss: 0.24\n",
      "\n",
      "======== Epoch 2 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:42.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:24.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:06.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:02:47.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:29.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:11.\n",
      "\n",
      "  Average training loss: 0.19\n",
      "  Training epoch took: 0:04:45\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.22\n",
      "\n",
      "======== Epoch 3 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:42.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:24.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:06.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:02:47.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:29.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:11.\n",
      "\n",
      "  Average training loss: 0.15\n",
      "  Training epoch took: 0:04:45\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.26\n",
      "\n",
      "======== Epoch 4 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:42.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:24.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:06.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:02:47.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:29.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:11.\n",
      "\n",
      "  Average training loss: 0.12\n",
      "  Training epoch took: 0:04:45\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.25\n",
      "\n",
      "======== Epoch 5 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:42.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:24.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:06.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:02:47.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:29.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:11.\n",
      "\n",
      "  Average training loss: 0.11\n",
      "  Training epoch took: 0:04:45\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.96\n",
      "  Average validation loss: 0.24\n",
      "\n",
      "======== Epoch 6 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:42.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:24.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:05.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:02:47.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:29.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:11.\n",
      "\n",
      "  Average training loss: 0.08\n",
      "  Training epoch took: 0:04:45\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.26\n",
      "\n",
      "======== Epoch 7 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:42.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:24.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:05.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:02:47.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:29.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:11.\n",
      "\n",
      "  Average training loss: 0.07\n",
      "  Training epoch took: 0:04:45\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.96\n",
      "  Average validation loss: 0.27\n",
      "\n",
      "======== Epoch 8 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:42.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:24.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:06.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:02:47.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:29.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:11.\n",
      "\n",
      "  Average training loss: 0.06\n",
      "  Training epoch took: 0:04:45\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.96\n",
      "  Average validation loss: 0.27\n",
      "\n",
      "======== Epoch 9 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:42.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:24.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:06.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:02:48.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:29.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:11.\n",
      "\n",
      "  Average training loss: 0.06\n",
      "  Training epoch took: 0:04:45\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.29\n",
      "\n",
      "======== Epoch 10 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:42.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:24.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:06.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:02:47.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:29.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:11.\n",
      "\n",
      "  Average training loss: 0.05\n",
      "  Training epoch took: 0:04:45\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.96\n",
      "  Average validation loss: 0.28\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▃▃▃▄▄▄▅▅▅▆▆▆▆▆▆▇▇▇███</td></tr><tr><td>train_batch_loss</td><td>▅█▁▄▂█▁▁▁▁█▁▁▁▁▁▁▁▄▁▁▁▆▁▁▂▁▁▇▁▁▁▁█▁▁▁▂▁▁</td></tr><tr><td>train_loss</td><td>█▅▄▃▃▂▁▁▁▁</td></tr><tr><td>val_f1</td><td>▁▃▃▆▇▆▇█▆▇</td></tr><tr><td>val_loss</td><td>▃▁▆▄▃▆▇▆█▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_batch_loss</td><td>0.00025</td></tr><tr><td>train_loss</td><td>0.04874</td></tr><tr><td>val_f1</td><td>0.95518</td></tr><tr><td>val_loss</td><td>0.28044</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">unique-sweep-8</strong> at: <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/wpbgm87f' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/wpbgm87f</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240301_044512-wpbgm87f\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 37hhoavc with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.15\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.01\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\AI projects\\Project-BERTcomparison\\wandb\\run-20240301_053637-37hhoavc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/37hhoavc' target=\"_blank\">dark-sweep-9</a></strong> to <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/37hhoavc' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/37hhoavc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at michiyasunaga/BioLinkBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size = 8\n",
      "learning_rate = 5e-05\n",
      "\n",
      "======== Epoch 1 / 10 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of  1,365.    Elapsed: 0:00:42.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:23.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:05.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:02:47.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:29.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:11.\n",
      "\n",
      "  Average training loss: 0.35\n",
      "  Training epoch took: 0:04:45\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.93\n",
      "  Average validation loss: 0.30\n",
      "\n",
      "======== Epoch 2 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:42.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:24.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:05.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:02:47.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:29.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:11.\n",
      "\n",
      "  Average training loss: 0.21\n",
      "  Training epoch took: 0:04:45\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.94\n",
      "  Average validation loss: 0.27\n",
      "\n",
      "======== Epoch 3 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:42.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:24.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:06.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:02:47.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:29.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:11.\n",
      "\n",
      "  Average training loss: 0.14\n",
      "  Training epoch took: 0:04:45\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.94\n",
      "  Average validation loss: 0.31\n",
      "\n",
      "======== Epoch 4 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:42.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:24.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:06.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:02:47.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:29.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:11.\n",
      "\n",
      "  Average training loss: 0.10\n",
      "  Training epoch took: 0:04:45\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.31\n",
      "\n",
      "======== Epoch 5 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:42.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:24.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:06.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:02:48.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:29.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:11.\n",
      "\n",
      "  Average training loss: 0.09\n",
      "  Training epoch took: 0:04:46\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.29\n",
      "\n",
      "======== Epoch 6 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:42.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:24.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:06.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:02:47.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:29.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:11.\n",
      "\n",
      "  Average training loss: 0.06\n",
      "  Training epoch took: 0:04:45\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.96\n",
      "  Average validation loss: 0.27\n",
      "\n",
      "======== Epoch 7 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:42.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:24.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:06.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:02:47.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:29.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:11.\n",
      "\n",
      "  Average training loss: 0.05\n",
      "  Training epoch took: 0:04:45\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.96\n",
      "  Average validation loss: 0.29\n",
      "\n",
      "======== Epoch 8 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:42.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:24.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:06.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:02:47.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:29.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:11.\n",
      "\n",
      "  Average training loss: 0.04\n",
      "  Training epoch took: 0:04:45\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.96\n",
      "  Average validation loss: 0.29\n",
      "\n",
      "======== Epoch 9 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:42.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:24.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:06.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:02:47.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:29.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:11.\n",
      "\n",
      "  Average training loss: 0.03\n",
      "  Training epoch took: 0:04:45\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.96\n",
      "  Average validation loss: 0.29\n",
      "\n",
      "======== Epoch 10 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:42.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:24.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:05.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:02:47.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:29.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:11.\n",
      "\n",
      "  Average training loss: 0.02\n",
      "  Training epoch took: 0:04:45\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.96\n",
      "  Average validation loss: 0.29\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2372a661a16b466c97e429ef5067b68a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▃▃▃▄▄▄▅▅▅▆▆▆▆▆▆▇▇▇███</td></tr><tr><td>train_batch_loss</td><td>▁▂▁▁▁▁▄▁▁▁▄▂▅▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▅▄▃▂▂▂▁▁▁</td></tr><tr><td>val_f1</td><td>▁▄▄▅▇█████</td></tr><tr><td>val_loss</td><td>▇▂██▅▁▅▅▅▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_batch_loss</td><td>0.00059</td></tr><tr><td>train_loss</td><td>0.021</td></tr><tr><td>val_f1</td><td>0.95601</td></tr><tr><td>val_loss</td><td>0.29308</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dark-sweep-9</strong> at: <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/37hhoavc' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/37hhoavc</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240301_053637-37hhoavc\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: fv4e86ua with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.01\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\AI projects\\Project-BERTcomparison\\wandb\\run-20240301_062807-fv4e86ua</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/fv4e86ua' target=\"_blank\">fluent-sweep-10</a></strong> to <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/fv4e86ua' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/fv4e86ua</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at michiyasunaga/BioLinkBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size = 8\n",
      "learning_rate = 5e-05\n",
      "\n",
      "======== Epoch 1 / 10 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of  1,365.    Elapsed: 0:00:42.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:23.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:05.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:02:47.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:29.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:11.\n",
      "\n",
      "  Average training loss: 0.34\n",
      "  Training epoch took: 0:04:46\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.94\n",
      "  Average validation loss: 0.26\n",
      "\n",
      "======== Epoch 2 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:42.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:24.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:05.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:02:47.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:29.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:11.\n",
      "\n",
      "  Average training loss: 0.24\n",
      "  Training epoch took: 0:04:45\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.93\n",
      "  Average validation loss: 0.32\n",
      "\n",
      "======== Epoch 3 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:42.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:24.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:06.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:02:48.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:29.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:11.\n",
      "\n",
      "  Average training loss: 0.20\n",
      "  Training epoch took: 0:04:45\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.94\n",
      "  Average validation loss: 0.33\n",
      "\n",
      "======== Epoch 4 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:42.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:24.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:06.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:02:47.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:29.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:11.\n",
      "\n",
      "  Average training loss: 0.14\n",
      "  Training epoch took: 0:04:46\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.31\n",
      "\n",
      "======== Epoch 5 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:42.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:24.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:05.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:02:47.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:29.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:11.\n",
      "\n",
      "  Average training loss: 0.11\n",
      "  Training epoch took: 0:04:45\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.94\n",
      "  Average validation loss: 0.32\n",
      "\n",
      "======== Epoch 6 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:42.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:24.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:06.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:02:48.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:30.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:11.\n",
      "\n",
      "  Average training loss: 0.09\n",
      "  Training epoch took: 0:04:46\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.28\n",
      "\n",
      "======== Epoch 7 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:42.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:24.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:06.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:02:47.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:29.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:11.\n",
      "\n",
      "  Average training loss: 0.07\n",
      "  Training epoch took: 0:04:46\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.30\n",
      "\n",
      "======== Epoch 8 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:42.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:24.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:05.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:02:47.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:29.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:11.\n",
      "\n",
      "  Average training loss: 0.06\n",
      "  Training epoch took: 0:04:45\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.33\n",
      "\n",
      "======== Epoch 9 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:42.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:24.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:05.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:02:47.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:29.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:11.\n",
      "\n",
      "  Average training loss: 0.03\n",
      "  Training epoch took: 0:04:45\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.36\n",
      "\n",
      "======== Epoch 10 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:42.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:24.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:06.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:02:47.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:29.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:11.\n",
      "\n",
      "  Average training loss: 0.03\n",
      "  Training epoch took: 0:04:45\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.35\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04db5ac698fb490984aecfc57f89c568",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▃▃▃▄▄▄▅▅▅▆▆▆▆▆▆▇▇▇███</td></tr><tr><td>train_batch_loss</td><td>▃▁▁▁▁▄▄▃▁▁▅▁▁▁▄▅▁▁▁▁█▅▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█</td></tr><tr><td>train_loss</td><td>█▆▅▄▃▂▂▂▁▁</td></tr><tr><td>val_f1</td><td>▂▁▁▇▄▇████</td></tr><tr><td>val_loss</td><td>▁▅▆▅▅▃▄▆█▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_batch_loss</td><td>0.00013</td></tr><tr><td>train_loss</td><td>0.03316</td></tr><tr><td>val_f1</td><td>0.95233</td></tr><tr><td>val_loss</td><td>0.34805</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fluent-sweep-10</strong> at: <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/fv4e86ua' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/fv4e86ua</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240301_062807-fv4e86ua\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 6rb849wi with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.15\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.01\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\AI projects\\Project-BERTcomparison\\wandb\\run-20240301_071930-6rb849wi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/6rb849wi' target=\"_blank\">exalted-sweep-11</a></strong> to <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/6rb849wi' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/6rb849wi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at michiyasunaga/BioLinkBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size = 16\n",
      "learning_rate = 5e-05\n",
      "\n",
      "======== Epoch 1 / 10 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    683.    Elapsed: 0:01:19.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:37.\n",
      "  Batch   600  of    683.    Elapsed: 0:03:56.\n",
      "\n",
      "  Average training loss: 0.39\n",
      "  Training epoch took: 0:04:28\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.86\n",
      "  Average validation loss: 0.36\n",
      "\n",
      "======== Epoch 2 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:19.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:37.\n",
      "  Batch   600  of    683.    Elapsed: 0:03:56.\n",
      "\n",
      "  Average training loss: 0.26\n",
      "  Training epoch took: 0:04:28\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.94\n",
      "  Average validation loss: 0.23\n",
      "\n",
      "======== Epoch 3 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:19.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:37.\n",
      "  Batch   600  of    683.    Elapsed: 0:03:56.\n",
      "\n",
      "  Average training loss: 0.21\n",
      "  Training epoch took: 0:04:28\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.92\n",
      "  Average validation loss: 0.27\n",
      "\n",
      "======== Epoch 4 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:19.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:37.\n",
      "  Batch   600  of    683.    Elapsed: 0:03:56.\n",
      "\n",
      "  Average training loss: 0.22\n",
      "  Training epoch took: 0:04:28\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.94\n",
      "  Average validation loss: 0.26\n",
      "\n",
      "======== Epoch 5 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:19.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:37.\n",
      "  Batch   600  of    683.    Elapsed: 0:03:56.\n",
      "\n",
      "  Average training loss: 0.19\n",
      "  Training epoch took: 0:04:28\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.94\n",
      "  Average validation loss: 0.21\n",
      "\n",
      "======== Epoch 6 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:19.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:37.\n",
      "  Batch   600  of    683.    Elapsed: 0:03:56.\n",
      "\n",
      "  Average training loss: 0.16\n",
      "  Training epoch took: 0:04:28\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.94\n",
      "  Average validation loss: 0.27\n",
      "\n",
      "======== Epoch 7 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:19.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:37.\n",
      "  Batch   600  of    683.    Elapsed: 0:03:56.\n",
      "\n",
      "  Average training loss: 0.13\n",
      "  Training epoch took: 0:04:28\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.21\n",
      "\n",
      "======== Epoch 8 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:19.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:37.\n",
      "  Batch   600  of    683.    Elapsed: 0:03:56.\n",
      "\n",
      "  Average training loss: 0.10\n",
      "  Training epoch took: 0:04:28\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.24\n",
      "\n",
      "======== Epoch 9 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:19.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:38.\n",
      "  Batch   600  of    683.    Elapsed: 0:03:56.\n",
      "\n",
      "  Average training loss: 0.08\n",
      "  Training epoch took: 0:04:29\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.23\n",
      "\n",
      "======== Epoch 10 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:19.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:37.\n",
      "  Batch   600  of    683.    Elapsed: 0:03:56.\n",
      "\n",
      "  Average training loss: 0.08\n",
      "  Training epoch took: 0:04:28\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.24\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▃▃▃▄▄▄▅▅▅▆▆▆▆▆▆▇▇▇███</td></tr><tr><td>train_batch_loss</td><td>▆▄▃▂▁▁▃▆▃▃▁▁▃▁▁▃▁▃▃▃▅▃▂▁▁▁▁▃▃▁█▄▁▁▁▁▃▁▁▃</td></tr><tr><td>train_loss</td><td>█▅▄▄▄▃▂▂▁▁</td></tr><tr><td>val_f1</td><td>▁▇▆▇▇▇████</td></tr><tr><td>val_loss</td><td>█▂▄▄▁▄▁▂▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_batch_loss</td><td>0.01033</td></tr><tr><td>train_loss</td><td>0.07734</td></tr><tr><td>val_f1</td><td>0.95111</td></tr><tr><td>val_loss</td><td>0.2412</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">exalted-sweep-11</strong> at: <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/6rb849wi' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/6rb849wi</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240301_071930-6rb849wi\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: tlqwwtxr with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-06\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.01\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\AI projects\\Project-BERTcomparison\\wandb\\run-20240301_080759-tlqwwtxr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/tlqwwtxr' target=\"_blank\">deep-sweep-12</a></strong> to <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/tlqwwtxr' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/tlqwwtxr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at michiyasunaga/BioLinkBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size = 16\n",
      "learning_rate = 5e-06\n",
      "\n",
      "======== Epoch 1 / 10 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    683.    Elapsed: 0:01:18.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:37.\n",
      "  Batch   600  of    683.    Elapsed: 0:03:56.\n",
      "\n",
      "  Average training loss: 0.36\n",
      "  Training epoch took: 0:04:29\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.91\n",
      "  Average validation loss: 0.24\n",
      "\n",
      "======== Epoch 2 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:19.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:37.\n",
      "  Batch   600  of    683.    Elapsed: 0:03:56.\n",
      "\n",
      "  Average training loss: 0.19\n",
      "  Training epoch took: 0:04:29\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.94\n",
      "  Average validation loss: 0.22\n",
      "\n",
      "======== Epoch 3 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:19.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:37.\n",
      "  Batch   600  of    683.    Elapsed: 0:03:56.\n",
      "\n",
      "  Average training loss: 0.16\n",
      "  Training epoch took: 0:04:29\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.19\n",
      "\n",
      "======== Epoch 4 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:19.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:37.\n",
      "  Batch   600  of    683.    Elapsed: 0:03:56.\n",
      "\n",
      "  Average training loss: 0.15\n",
      "  Training epoch took: 0:04:29\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.21\n",
      "\n",
      "======== Epoch 5 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:19.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:37.\n",
      "  Batch   600  of    683.    Elapsed: 0:03:56.\n",
      "\n",
      "  Average training loss: 0.14\n",
      "  Training epoch took: 0:04:28\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.22\n",
      "\n",
      "======== Epoch 6 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:19.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:38.\n",
      "  Batch   600  of    683.    Elapsed: 0:03:56.\n",
      "\n",
      "  Average training loss: 0.13\n",
      "  Training epoch took: 0:04:29\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.22\n",
      "\n",
      "======== Epoch 7 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:19.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:37.\n",
      "  Batch   600  of    683.    Elapsed: 0:03:56.\n",
      "\n",
      "  Average training loss: 0.12\n",
      "  Training epoch took: 0:04:28\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.24\n",
      "\n",
      "======== Epoch 8 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:19.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:38.\n",
      "  Batch   600  of    683.    Elapsed: 0:03:56.\n",
      "\n",
      "  Average training loss: 0.11\n",
      "  Training epoch took: 0:04:29\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.96\n",
      "  Average validation loss: 0.22\n",
      "\n",
      "======== Epoch 9 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:19.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:38.\n",
      "  Batch   600  of    683.    Elapsed: 0:03:56.\n",
      "\n",
      "  Average training loss: 0.11\n",
      "  Training epoch took: 0:04:29\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.23\n",
      "\n",
      "======== Epoch 10 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:19.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:37.\n",
      "  Batch   600  of    683.    Elapsed: 0:03:56.\n",
      "\n",
      "  Average training loss: 0.09\n",
      "  Training epoch took: 0:04:29\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.23\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▃▃▃▄▄▄▅▅▅▆▆▆▆▆▆▇▇▇███</td></tr><tr><td>train_batch_loss</td><td>▇▄▄▃▂▄▃▁▁▁▁▄▄▁▅▆▆▁▁▁▁▁▃▁▃▁██▁▁▁▃▁▃▄▅▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▂▂▁▁▁</td></tr><tr><td>val_f1</td><td>▁▅▇▇▆▇▇█▇▇</td></tr><tr><td>val_loss</td><td>█▅▁▄▅▆▇▅▇▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_batch_loss</td><td>0.00648</td></tr><tr><td>train_loss</td><td>0.09113</td></tr><tr><td>val_f1</td><td>0.95231</td></tr><tr><td>val_loss</td><td>0.23005</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">deep-sweep-12</strong> at: <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/tlqwwtxr' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/tlqwwtxr</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240301_080759-tlqwwtxr\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: v3lljeke with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.15\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-06\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.001\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\AI projects\\Project-BERTcomparison\\wandb\\run-20240301_085632-v3lljeke</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/v3lljeke' target=\"_blank\">faithful-sweep-13</a></strong> to <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/v3lljeke' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/v3lljeke</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at michiyasunaga/BioLinkBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size = 16\n",
      "learning_rate = 5e-06\n",
      "\n",
      "======== Epoch 1 / 10 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    683.    Elapsed: 0:01:19.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:38.\n",
      "  Batch   600  of    683.    Elapsed: 0:03:56.\n",
      "\n",
      "  Average training loss: 0.32\n",
      "  Training epoch took: 0:04:29\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.94\n",
      "  Average validation loss: 0.16\n",
      "\n",
      "======== Epoch 2 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:19.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:37.\n",
      "  Batch   600  of    683.    Elapsed: 0:03:56.\n",
      "\n",
      "  Average training loss: 0.17\n",
      "  Training epoch took: 0:04:29\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.94\n",
      "  Average validation loss: 0.21\n",
      "\n",
      "======== Epoch 3 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:19.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:37.\n",
      "  Batch   600  of    683.    Elapsed: 0:03:56.\n",
      "\n",
      "  Average training loss: 0.14\n",
      "  Training epoch took: 0:04:29\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.19\n",
      "\n",
      "======== Epoch 4 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:19.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:37.\n",
      "  Batch   600  of    683.    Elapsed: 0:03:56.\n",
      "\n",
      "  Average training loss: 0.12\n",
      "  Training epoch took: 0:04:28\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.96\n",
      "  Average validation loss: 0.19\n",
      "\n",
      "======== Epoch 5 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:19.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:37.\n",
      "  Batch   600  of    683.    Elapsed: 0:03:56.\n",
      "\n",
      "  Average training loss: 0.11\n",
      "  Training epoch took: 0:04:28\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.96\n",
      "  Average validation loss: 0.18\n",
      "\n",
      "======== Epoch 6 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:19.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:37.\n",
      "  Batch   600  of    683.    Elapsed: 0:03:56.\n",
      "\n",
      "  Average training loss: 0.09\n",
      "  Training epoch took: 0:04:29\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.22\n",
      "\n",
      "======== Epoch 7 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:19.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:37.\n",
      "  Batch   600  of    683.    Elapsed: 0:03:56.\n",
      "\n",
      "  Average training loss: 0.09\n",
      "  Training epoch took: 0:04:28\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.24\n",
      "\n",
      "======== Epoch 8 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:19.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:38.\n",
      "  Batch   600  of    683.    Elapsed: 0:03:57.\n",
      "\n",
      "  Average training loss: 0.07\n",
      "  Training epoch took: 0:04:29\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.96\n",
      "  Average validation loss: 0.22\n",
      "\n",
      "======== Epoch 9 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:19.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:37.\n",
      "  Batch   600  of    683.    Elapsed: 0:03:56.\n",
      "\n",
      "  Average training loss: 0.07\n",
      "  Training epoch took: 0:04:29\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.96\n",
      "  Average validation loss: 0.21\n",
      "\n",
      "======== Epoch 10 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:19.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:38.\n",
      "  Batch   600  of    683.    Elapsed: 0:03:56.\n",
      "\n",
      "  Average training loss: 0.07\n",
      "  Training epoch took: 0:04:29\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.96\n",
      "  Average validation loss: 0.22\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dce20125dbfe42cab9d524dc1085754a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▃▃▃▄▄▄▅▅▅▆▆▆▆▆▆▇▇▇███</td></tr><tr><td>train_batch_loss</td><td>█▅▆▁▃▁█▆▃▁▁▁▂▁▁▁▁▄▁▁▁▁▃▁▁▁▁▆▁▁▁▄▁▁▂▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▄▃▃▂▂▂▁▁▁</td></tr><tr><td>val_f1</td><td>▁▁▅▇█▆▅▇██</td></tr><tr><td>val_loss</td><td>▁▅▄▃▃▆█▆▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_batch_loss</td><td>0.00096</td></tr><tr><td>train_loss</td><td>0.06661</td></tr><tr><td>val_f1</td><td>0.95845</td></tr><tr><td>val_loss</td><td>0.21781</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">faithful-sweep-13</strong> at: <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/v3lljeke' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/v3lljeke</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240301_085632-v3lljeke\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 8066we9v with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.15\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.001\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\AI projects\\Project-BERTcomparison\\wandb\\run-20240301_094508-8066we9v</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/8066we9v' target=\"_blank\">icy-sweep-14</a></strong> to <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/8066we9v' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/8066we9v</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at michiyasunaga/BioLinkBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size = 8\n",
      "learning_rate = 5e-05\n",
      "\n",
      "======== Epoch 1 / 10 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of  1,365.    Elapsed: 0:00:42.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:24.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:06.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:02:48.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:30.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:12.\n",
      "\n",
      "  Average training loss: 0.35\n",
      "  Training epoch took: 0:04:46\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.92\n",
      "  Average validation loss: 0.32\n",
      "\n",
      "======== Epoch 2 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:42.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:24.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:06.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:02:47.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:29.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:11.\n",
      "\n",
      "  Average training loss: 0.22\n",
      "  Training epoch took: 0:04:46\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.94\n",
      "  Average validation loss: 0.25\n",
      "\n",
      "======== Epoch 3 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:42.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:24.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:05.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:02:47.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:29.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:11.\n",
      "\n",
      "  Average training loss: 0.16\n",
      "  Training epoch took: 0:04:45\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.92\n",
      "  Average validation loss: 0.35\n",
      "\n",
      "======== Epoch 4 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:42.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:24.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:06.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:02:48.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:30.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:11.\n",
      "\n",
      "  Average training loss: 0.10\n",
      "  Training epoch took: 0:04:46\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.28\n",
      "\n",
      "======== Epoch 5 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:42.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:24.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:06.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:02:47.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:29.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:11.\n",
      "\n",
      "  Average training loss: 0.08\n",
      "  Training epoch took: 0:04:46\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.27\n",
      "\n",
      "======== Epoch 6 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:42.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:24.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:06.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:02:47.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:29.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:11.\n",
      "\n",
      "  Average training loss: 0.05\n",
      "  Training epoch took: 0:04:46\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.31\n",
      "\n",
      "======== Epoch 7 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:42.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:24.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:06.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:02:47.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:29.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:11.\n",
      "\n",
      "  Average training loss: 0.05\n",
      "  Training epoch took: 0:04:46\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.96\n",
      "  Average validation loss: 0.28\n",
      "\n",
      "======== Epoch 8 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:42.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:24.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:06.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:02:47.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:29.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:11.\n",
      "\n",
      "  Average training loss: 0.04\n",
      "  Training epoch took: 0:04:45\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.29\n",
      "\n",
      "======== Epoch 9 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:42.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:24.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:06.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:02:50.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:35.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:19.\n",
      "\n",
      "  Average training loss: 0.03\n",
      "  Training epoch took: 0:04:55\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.34\n",
      "\n",
      "======== Epoch 10 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:43.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:27.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:10.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:02:53.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:36.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:19.\n",
      "\n",
      "  Average training loss: 0.03\n",
      "  Training epoch took: 0:04:54\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.29\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9557667baae5434f97a30b0cfa1872e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▃▃▃▄▄▄▅▅▅▆▆▆▆▆▆▇▇▇███</td></tr><tr><td>train_batch_loss</td><td>▂▄▅▁▁▁▁▁▃▁▁▁▁▁▇▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▅▄▃▂▂▁▁▁▁</td></tr><tr><td>val_f1</td><td>▁▅▂█▇▇██▇█</td></tr><tr><td>val_loss</td><td>▆▁█▃▃▅▃▄▇▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_batch_loss</td><td>0.00012</td></tr><tr><td>train_loss</td><td>0.02786</td></tr><tr><td>val_f1</td><td>0.95355</td></tr><tr><td>val_loss</td><td>0.2949</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">icy-sweep-14</strong> at: <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/8066we9v' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/8066we9v</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240301_094508-8066we9v\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 34pvz645 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.15\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.001\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\AI projects\\Project-BERTcomparison\\wandb\\run-20240301_103655-34pvz645</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/34pvz645' target=\"_blank\">sparkling-sweep-15</a></strong> to <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/34pvz645' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/34pvz645</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at michiyasunaga/BioLinkBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size = 16\n",
      "learning_rate = 5e-05\n",
      "\n",
      "======== Epoch 1 / 10 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    683.    Elapsed: 0:01:20.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:41.\n",
      "  Batch   600  of    683.    Elapsed: 0:04:02.\n",
      "\n",
      "  Average training loss: 0.25\n",
      "  Training epoch took: 0:04:35\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.94\n",
      "  Average validation loss: 0.25\n",
      "\n",
      "======== Epoch 2 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:20.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:39.\n",
      "  Batch   600  of    683.    Elapsed: 0:03:58.\n",
      "\n",
      "  Average training loss: 0.15\n",
      "  Training epoch took: 0:04:30\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.19\n",
      "\n",
      "======== Epoch 3 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:19.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:38.\n",
      "  Batch   600  of    683.    Elapsed: 0:03:57.\n",
      "\n",
      "  Average training loss: 0.09\n",
      "  Training epoch took: 0:04:29\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.94\n",
      "  Average validation loss: 0.28\n",
      "\n",
      "======== Epoch 4 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:19.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:38.\n",
      "  Batch   600  of    683.    Elapsed: 0:03:58.\n",
      "\n",
      "  Average training loss: 0.07\n",
      "  Training epoch took: 0:04:30\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.22\n",
      "\n",
      "======== Epoch 5 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:19.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:38.\n",
      "  Batch   600  of    683.    Elapsed: 0:03:57.\n",
      "\n",
      "  Average training loss: 0.04\n",
      "  Training epoch took: 0:04:29\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.96\n",
      "  Average validation loss: 0.25\n",
      "\n",
      "======== Epoch 6 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:19.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:38.\n",
      "  Batch   600  of    683.    Elapsed: 0:03:57.\n",
      "\n",
      "  Average training loss: 0.03\n",
      "  Training epoch took: 0:04:30\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.96\n",
      "  Average validation loss: 0.32\n",
      "\n",
      "======== Epoch 7 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:19.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:38.\n",
      "  Batch   600  of    683.    Elapsed: 0:03:57.\n",
      "\n",
      "  Average training loss: 0.02\n",
      "  Training epoch took: 0:04:30\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.96\n",
      "  Average validation loss: 0.29\n",
      "\n",
      "======== Epoch 8 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:19.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:38.\n",
      "  Batch   600  of    683.    Elapsed: 0:03:57.\n",
      "\n",
      "  Average training loss: 0.01\n",
      "  Training epoch took: 0:04:30\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.96\n",
      "  Average validation loss: 0.29\n",
      "\n",
      "======== Epoch 9 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:19.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:38.\n",
      "  Batch   600  of    683.    Elapsed: 0:03:57.\n",
      "\n",
      "  Average training loss: 0.01\n",
      "  Training epoch took: 0:04:30\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.96\n",
      "  Average validation loss: 0.31\n",
      "\n",
      "======== Epoch 10 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:19.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:38.\n",
      "  Batch   600  of    683.    Elapsed: 0:03:57.\n",
      "\n",
      "  Average training loss: 0.00\n",
      "  Training epoch took: 0:04:30\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.96\n",
      "  Average validation loss: 0.31\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cde1b7bca1b462ca4262de33b2b6830",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▃▃▃▄▄▄▅▅▅▆▆▆▆▆▆▇▇▇███</td></tr><tr><td>train_batch_loss</td><td>▄▇▁▆▄█▁▂▁▁▁▆▆▁▄▁▁▁▁▁▁▁▁▁▁▁██▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▅▄▃▂▂▁▁▁▁</td></tr><tr><td>val_f1</td><td>▁▅▁▅▇▆▆█▇█</td></tr><tr><td>val_loss</td><td>▄▁▆▂▄█▆▆██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_batch_loss</td><td>9e-05</td></tr><tr><td>train_loss</td><td>0.0029</td></tr><tr><td>val_f1</td><td>0.9613</td></tr><tr><td>val_loss</td><td>0.31355</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">sparkling-sweep-15</strong> at: <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/34pvz645' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/34pvz645</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240301_103655-34pvz645\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: fx2h6ctj with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-06\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.01\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\AI projects\\Project-BERTcomparison\\wandb\\run-20240301_112546-fx2h6ctj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/fx2h6ctj' target=\"_blank\">vocal-sweep-16</a></strong> to <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/fx2h6ctj' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/fx2h6ctj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at michiyasunaga/BioLinkBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size = 16\n",
      "learning_rate = 5e-06\n",
      "\n",
      "======== Epoch 1 / 10 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    683.    Elapsed: 0:01:19.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:38.\n",
      "  Batch   600  of    683.    Elapsed: 0:03:57.\n",
      "\n",
      "  Average training loss: 0.38\n",
      "  Training epoch took: 0:04:29\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.93\n",
      "  Average validation loss: 0.18\n",
      "\n",
      "======== Epoch 2 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:19.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:38.\n",
      "  Batch   600  of    683.    Elapsed: 0:03:56.\n",
      "\n",
      "  Average training loss: 0.19\n",
      "  Training epoch took: 0:04:29\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.94\n",
      "  Average validation loss: 0.20\n",
      "\n",
      "======== Epoch 3 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:19.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:38.\n",
      "  Batch   600  of    683.    Elapsed: 0:03:56.\n",
      "\n",
      "  Average training loss: 0.16\n",
      "  Training epoch took: 0:04:29\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.20\n",
      "\n",
      "======== Epoch 4 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:19.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:38.\n",
      "  Batch   600  of    683.    Elapsed: 0:03:56.\n",
      "\n",
      "  Average training loss: 0.15\n",
      "  Training epoch took: 0:04:29\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.94\n",
      "  Average validation loss: 0.22\n",
      "\n",
      "======== Epoch 5 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:19.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:38.\n",
      "  Batch   600  of    683.    Elapsed: 0:03:56.\n",
      "\n",
      "  Average training loss: 0.13\n",
      "  Training epoch took: 0:04:29\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.20\n",
      "\n",
      "======== Epoch 6 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:19.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:38.\n",
      "  Batch   600  of    683.    Elapsed: 0:03:56.\n",
      "\n",
      "  Average training loss: 0.11\n",
      "  Training epoch took: 0:04:29\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.23\n",
      "\n",
      "======== Epoch 7 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:22.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:44.\n",
      "  Batch   600  of    683.    Elapsed: 0:04:03.\n",
      "\n",
      "  Average training loss: 0.11\n",
      "  Training epoch took: 0:04:36\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.22\n",
      "\n",
      "======== Epoch 8 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:19.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:39.\n",
      "  Batch   600  of    683.    Elapsed: 0:03:58.\n",
      "\n",
      "  Average training loss: 0.11\n",
      "  Training epoch took: 0:04:30\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.23\n",
      "\n",
      "======== Epoch 9 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:19.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:38.\n",
      "  Batch   600  of    683.    Elapsed: 0:03:58.\n",
      "\n",
      "  Average training loss: 0.11\n",
      "  Training epoch took: 0:04:32\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.23\n",
      "\n",
      "======== Epoch 10 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:21.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:40.\n",
      "  Batch   600  of    683.    Elapsed: 0:04:00.\n",
      "\n",
      "  Average training loss: 0.11\n",
      "  Training epoch took: 0:04:32\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.22\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90b1a35a88494f53940043eb3a81474b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.020 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.056250…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▃▃▃▄▄▄▅▅▅▆▆▆▆▆▆▇▇▇███</td></tr><tr><td>train_batch_loss</td><td>█▅▂▂▆▁▄▁▁▁▃▄▂▁▁▁▂▂▁▄▁▇▄▇▁▁▁▁▁▁▁▆▁▁▁▁▁▃▁▁</td></tr><tr><td>train_loss</td><td>█▃▂▂▂▁▁▁▁▁</td></tr><tr><td>val_f1</td><td>▁▄▇▄▇▇▇▇▇█</td></tr><tr><td>val_loss</td><td>▁▃▄▇▄▇▇█▇▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_batch_loss</td><td>3.97037</td></tr><tr><td>train_loss</td><td>0.10822</td></tr><tr><td>val_f1</td><td>0.95436</td></tr><tr><td>val_loss</td><td>0.21828</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">vocal-sweep-16</strong> at: <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/fx2h6ctj' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/fx2h6ctj</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240301_112546-fx2h6ctj\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 27qzy921 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.15\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.001\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\AI projects\\Project-BERTcomparison\\wandb\\run-20240301_121440-27qzy921</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/27qzy921' target=\"_blank\">fragrant-sweep-17</a></strong> to <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/27qzy921' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/27qzy921</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at michiyasunaga/BioLinkBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size = 16\n",
      "learning_rate = 5e-05\n",
      "\n",
      "======== Epoch 1 / 10 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    683.    Elapsed: 0:01:19.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:39.\n",
      "  Batch   600  of    683.    Elapsed: 0:03:58.\n",
      "\n",
      "  Average training loss: 0.25\n",
      "  Training epoch took: 0:04:31\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.17\n",
      "\n",
      "======== Epoch 2 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:19.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:39.\n",
      "  Batch   600  of    683.    Elapsed: 0:03:58.\n",
      "\n",
      "  Average training loss: 0.14\n",
      "  Training epoch took: 0:04:30\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.22\n",
      "\n",
      "======== Epoch 3 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:26.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:55.\n",
      "  Batch   600  of    683.    Elapsed: 0:04:23.\n",
      "\n",
      "  Average training loss: 0.11\n",
      "  Training epoch took: 0:04:58\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.23\n",
      "\n",
      "======== Epoch 4 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:27.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:51.\n",
      "  Batch   600  of    683.    Elapsed: 0:04:17.\n",
      "\n",
      "  Average training loss: 0.06\n",
      "  Training epoch took: 0:04:52\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.96\n",
      "  Average validation loss: 0.22\n",
      "\n",
      "======== Epoch 5 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:24.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:49.\n",
      "  Batch   600  of    683.    Elapsed: 0:04:14.\n",
      "\n",
      "  Average training loss: 0.05\n",
      "  Training epoch took: 0:04:50\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.96\n",
      "  Average validation loss: 0.27\n",
      "\n",
      "======== Epoch 6 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:25.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:50.\n",
      "  Batch   600  of    683.    Elapsed: 0:04:14.\n",
      "\n",
      "  Average training loss: 0.03\n",
      "  Training epoch took: 0:04:49\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.96\n",
      "  Average validation loss: 0.26\n",
      "\n",
      "======== Epoch 7 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:23.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:53.\n",
      "  Batch   600  of    683.    Elapsed: 0:04:20.\n",
      "\n",
      "  Average training loss: 0.02\n",
      "  Training epoch took: 0:04:57\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.37\n",
      "\n",
      "======== Epoch 8 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:26.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:47.\n",
      "  Batch   600  of    683.    Elapsed: 0:04:10.\n",
      "\n",
      "  Average training loss: 0.01\n",
      "  Training epoch took: 0:04:44\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.96\n",
      "  Average validation loss: 0.30\n",
      "\n",
      "======== Epoch 9 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:22.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:43.\n",
      "  Batch   600  of    683.    Elapsed: 0:04:06.\n",
      "\n",
      "  Average training loss: 0.00\n",
      "  Training epoch took: 0:04:40\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.96\n",
      "  Average validation loss: 0.35\n",
      "\n",
      "======== Epoch 10 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:22.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:46.\n",
      "  Batch   600  of    683.    Elapsed: 0:04:10.\n",
      "\n",
      "  Average training loss: 0.01\n",
      "  Training epoch took: 0:04:45\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.96\n",
      "  Average validation loss: 0.37\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▃▃▃▄▄▄▅▅▅▆▆▆▆▆▆▇▇▇███</td></tr><tr><td>train_batch_loss</td><td>▂▃▂▃▁▁▁▃█▃▁▁▁▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▅▄▃▂▂▁▁▁▁</td></tr><tr><td>val_f1</td><td>▄▁▃█▆▇▆▇▇▇</td></tr><tr><td>val_loss</td><td>▁▃▃▃▄▄█▆▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_batch_loss</td><td>5e-05</td></tr><tr><td>train_loss</td><td>0.00528</td></tr><tr><td>val_f1</td><td>0.95683</td></tr><tr><td>val_loss</td><td>0.37191</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fragrant-sweep-17</strong> at: <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/27qzy921' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/27qzy921</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240301_121440-27qzy921\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: awklqvzz with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.15\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-06\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.01\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\AI projects\\Project-BERTcomparison\\wandb\\run-20240301_130616-awklqvzz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/awklqvzz' target=\"_blank\">true-sweep-18</a></strong> to <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/awklqvzz' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/awklqvzz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at michiyasunaga/BioLinkBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size = 8\n",
      "learning_rate = 5e-06\n",
      "\n",
      "======== Epoch 1 / 10 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of  1,365.    Elapsed: 0:00:45.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:30.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:14.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:02:57.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:41.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:24.\n",
      "\n",
      "  Average training loss: 0.31\n",
      "  Training epoch took: 0:05:00\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.24\n",
      "\n",
      "======== Epoch 2 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:48.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:33.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:17.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:03:03.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:50.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:37.\n",
      "\n",
      "  Average training loss: 0.20\n",
      "  Training epoch took: 0:05:14\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.22\n",
      "\n",
      "======== Epoch 3 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:47.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:34.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:21.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:03:12.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:04:05.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:50.\n",
      "\n",
      "  Average training loss: 0.16\n",
      "  Training epoch took: 0:05:28\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.22\n",
      "\n",
      "======== Epoch 4 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:44.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:28.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:14.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:03:00.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:46.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:32.\n",
      "\n",
      "  Average training loss: 0.13\n",
      "  Training epoch took: 0:05:10\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.96\n",
      "  Average validation loss: 0.22\n",
      "\n",
      "======== Epoch 5 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:46.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:33.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:19.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:03:05.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:49.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:34.\n",
      "\n",
      "  Average training loss: 0.11\n",
      "  Training epoch took: 0:05:09\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.25\n",
      "\n",
      "======== Epoch 6 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:45.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:30.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:15.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:03:01.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:45.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:31.\n",
      "\n",
      "  Average training loss: 0.09\n",
      "  Training epoch took: 0:05:10\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.94\n",
      "  Average validation loss: 0.30\n",
      "\n",
      "======== Epoch 7 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:47.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:32.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:18.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:03:02.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:48.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:34.\n",
      "\n",
      "  Average training loss: 0.07\n",
      "  Training epoch took: 0:05:13\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.96\n",
      "  Average validation loss: 0.25\n",
      "\n",
      "======== Epoch 8 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:46.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:32.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:18.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:03:03.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:50.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:35.\n",
      "\n",
      "  Average training loss: 0.07\n",
      "  Training epoch took: 0:05:12\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.96\n",
      "  Average validation loss: 0.26\n",
      "\n",
      "======== Epoch 9 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:47.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:33.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:19.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:03:05.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:51.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:38.\n",
      "\n",
      "  Average training loss: 0.06\n",
      "  Training epoch took: 0:05:16\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.96\n",
      "  Average validation loss: 0.28\n",
      "\n",
      "======== Epoch 10 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:45.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:29.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:12.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:02:56.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:40.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:25.\n",
      "\n",
      "  Average training loss: 0.05\n",
      "  Training epoch took: 0:05:02\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.96\n",
      "  Average validation loss: 0.28\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▃▃▃▄▄▄▅▅▅▆▆▆▆▆▆▇▇▇███</td></tr><tr><td>train_batch_loss</td><td>▃▃▄▁▅▁▁▁█▁▅▁▂▅▁▁▁▃▁▁▁▁▁▅▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▅▄▃▃▂▂▁▁▁</td></tr><tr><td>val_f1</td><td>▂▄▅▇▆▁██▆▆</td></tr><tr><td>val_loss</td><td>▃▂▁▂▄█▄▄▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_batch_loss</td><td>0.01409</td></tr><tr><td>train_loss</td><td>0.04943</td></tr><tr><td>val_f1</td><td>0.956</td></tr><tr><td>val_loss</td><td>0.27794</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">true-sweep-18</strong> at: <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/awklqvzz' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/awklqvzz</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240301_130616-awklqvzz\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: jti0nj4u with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-06\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.001\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\AI projects\\Project-BERTcomparison\\wandb\\run-20240301_140228-jti0nj4u</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/jti0nj4u' target=\"_blank\">comic-sweep-19</a></strong> to <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/jti0nj4u' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/jti0nj4u</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at michiyasunaga/BioLinkBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size = 16\n",
      "learning_rate = 5e-06\n",
      "\n",
      "======== Epoch 1 / 10 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    683.    Elapsed: 0:01:21.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:57.\n",
      "  Batch   600  of    683.    Elapsed: 0:04:32.\n",
      "\n",
      "  Average training loss: 0.34\n",
      "  Training epoch took: 0:05:09\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.93\n",
      "  Average validation loss: 0.23\n",
      "\n",
      "======== Epoch 2 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:26.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:48.\n",
      "  Batch   600  of    683.    Elapsed: 0:04:10.\n",
      "\n",
      "  Average training loss: 0.19\n",
      "  Training epoch took: 0:04:45\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.19\n",
      "\n",
      "======== Epoch 3 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:24.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:47.\n",
      "  Batch   600  of    683.    Elapsed: 0:04:15.\n",
      "\n",
      "  Average training loss: 0.16\n",
      "  Training epoch took: 0:04:53\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.19\n",
      "\n",
      "======== Epoch 4 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:33.\n",
      "  Batch   400  of    683.    Elapsed: 0:03:04.\n",
      "  Batch   600  of    683.    Elapsed: 0:04:34.\n",
      "\n",
      "  Average training loss: 0.15\n",
      "  Training epoch took: 0:05:10\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.22\n",
      "\n",
      "======== Epoch 5 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:28.\n",
      "  Batch   400  of    683.    Elapsed: 0:03:03.\n",
      "  Batch   600  of    683.    Elapsed: 0:04:38.\n",
      "\n",
      "  Average training loss: 0.13\n",
      "  Training epoch took: 0:05:18\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.23\n",
      "\n",
      "======== Epoch 6 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:36.\n",
      "  Batch   400  of    683.    Elapsed: 0:03:12.\n",
      "  Batch   600  of    683.    Elapsed: 0:04:45.\n",
      "\n",
      "  Average training loss: 0.11\n",
      "  Training epoch took: 0:05:23\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.25\n",
      "\n",
      "======== Epoch 7 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:29.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:52.\n",
      "  Batch   600  of    683.    Elapsed: 0:04:14.\n",
      "\n",
      "  Average training loss: 0.12\n",
      "  Training epoch took: 0:04:48\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.24\n",
      "\n",
      "======== Epoch 8 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:25.\n",
      "  Batch   400  of    683.    Elapsed: 0:03:01.\n",
      "  Batch   600  of    683.    Elapsed: 0:04:37.\n",
      "\n",
      "  Average training loss: 0.11\n",
      "  Training epoch took: 0:05:16\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.25\n",
      "\n",
      "======== Epoch 9 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:36.\n",
      "  Batch   400  of    683.    Elapsed: 0:03:10.\n",
      "  Batch   600  of    683.    Elapsed: 0:04:43.\n",
      "\n",
      "  Average training loss: 0.10\n",
      "  Training epoch took: 0:05:19\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.24\n",
      "\n",
      "======== Epoch 10 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:29.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:55.\n",
      "  Batch   600  of    683.    Elapsed: 0:04:24.\n",
      "\n",
      "  Average training loss: 0.10\n",
      "  Training epoch took: 0:04:59\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.24\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "750cedb3aac6438cb60ea9993fee2b45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.016 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.069710…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▃▃▃▄▄▄▅▅▅▆▆▆▆▆▆▇▇▇███</td></tr><tr><td>train_batch_loss</td><td>▇▇▄▂▁▅▂▁▄▁▅▃▁▁▁▃▂▁▄▂▃▁▁▂█▂▁▁▁▂▁▁▄▁▃▅▅▆▁▂</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▁▁▁▁▁</td></tr><tr><td>val_f1</td><td>▁▅▇▆▆▆▆▆██</td></tr><tr><td>val_loss</td><td>▅▁▁▅▆█▆█▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_batch_loss</td><td>0.00057</td></tr><tr><td>train_loss</td><td>0.10275</td></tr><tr><td>val_f1</td><td>0.95436</td></tr><tr><td>val_loss</td><td>0.24104</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">comic-sweep-19</strong> at: <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/jti0nj4u' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/jti0nj4u</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240301_140228-jti0nj4u\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: nwnuu30w with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.01\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\AI projects\\Project-BERTcomparison\\wandb\\run-20240301_145749-nwnuu30w</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/nwnuu30w' target=\"_blank\">divine-sweep-20</a></strong> to <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/nwnuu30w' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/nwnuu30w</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at michiyasunaga/BioLinkBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size = 16\n",
      "learning_rate = 5e-05\n",
      "\n",
      "======== Epoch 1 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:26.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:53.\n",
      "  Batch   600  of    683.    Elapsed: 0:04:18.\n",
      "\n",
      "  Average training loss: 0.31\n",
      "  Training epoch took: 0:04:54\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.93\n",
      "  Average validation loss: 0.24\n",
      "\n",
      "======== Epoch 2 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:26.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:51.\n",
      "  Batch   600  of    683.    Elapsed: 0:04:10.\n",
      "\n",
      "  Average training loss: 0.20\n",
      "  Training epoch took: 0:04:43\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.20\n",
      "\n",
      "======== Epoch 3 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:18.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:37.\n",
      "  Batch   600  of    683.    Elapsed: 0:03:55.\n",
      "\n",
      "  Average training loss: 0.16\n",
      "  Training epoch took: 0:04:27\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.93\n",
      "  Average validation loss: 0.25\n",
      "\n",
      "======== Epoch 4 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:18.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:37.\n",
      "  Batch   600  of    683.    Elapsed: 0:03:55.\n",
      "\n",
      "  Average training loss: 0.12\n",
      "  Training epoch took: 0:04:28\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.22\n",
      "\n",
      "======== Epoch 5 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:20.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:43.\n",
      "  Batch   600  of    683.    Elapsed: 0:04:06.\n",
      "\n",
      "  Average training loss: 0.11\n",
      "  Training epoch took: 0:04:40\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.23\n",
      "\n",
      "======== Epoch 6 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:20.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:39.\n",
      "  Batch   600  of    683.    Elapsed: 0:03:59.\n",
      "\n",
      "  Average training loss: 0.08\n",
      "  Training epoch took: 0:04:32\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.22\n",
      "\n",
      "======== Epoch 7 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:21.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:41.\n",
      "  Batch   600  of    683.    Elapsed: 0:04:02.\n",
      "\n",
      "  Average training loss: 0.06\n",
      "  Training epoch took: 0:04:35\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.26\n",
      "\n",
      "======== Epoch 8 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:20.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:39.\n",
      "  Batch   600  of    683.    Elapsed: 0:03:58.\n",
      "\n",
      "  Average training loss: 0.04\n",
      "  Training epoch took: 0:04:31\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.96\n",
      "  Average validation loss: 0.30\n",
      "\n",
      "======== Epoch 9 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:20.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:39.\n",
      "  Batch   600  of    683.    Elapsed: 0:04:00.\n",
      "\n",
      "  Average training loss: 0.04\n",
      "  Training epoch took: 0:04:34\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.96\n",
      "  Average validation loss: 0.26\n",
      "\n",
      "======== Epoch 10 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:23.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:46.\n",
      "  Batch   600  of    683.    Elapsed: 0:04:09.\n",
      "\n",
      "  Average training loss: 0.03\n",
      "  Training epoch took: 0:04:43\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.96\n",
      "  Average validation loss: 0.29\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ce9809d139f4c9cb8518375d9ca35fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▃▃▃▄▄▄▅▅▅▆▆▆▆▆▆▇▇▇███</td></tr><tr><td>train_batch_loss</td><td>▃▆▂▄▁█▁▅▁▁▄▁▁▁▁▁▁▅▁▃▁▁▁▁▂▅▁▁▁▁▁▂▁▁▅▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▅▄▃▃▂▂▁▁▁</td></tr><tr><td>val_f1</td><td>▁▆▂▆▆▇▇███</td></tr><tr><td>val_loss</td><td>▄▁▄▂▃▂▅█▅▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_batch_loss</td><td>0.00065</td></tr><tr><td>train_loss</td><td>0.02786</td></tr><tr><td>val_f1</td><td>0.95518</td></tr><tr><td>val_loss</td><td>0.28671</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">divine-sweep-20</strong> at: <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/nwnuu30w' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/nwnuu30w</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240301_145749-nwnuu30w\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: v615gny2 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-06\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.001\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\AI projects\\Project-BERTcomparison\\wandb\\run-20240301_154749-v615gny2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/v615gny2' target=\"_blank\">distinctive-sweep-21</a></strong> to <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/v615gny2' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/v615gny2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at michiyasunaga/BioLinkBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size = 16\n",
      "learning_rate = 5e-06\n",
      "\n",
      "======== Epoch 1 / 10 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    683.    Elapsed: 0:01:22.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:46.\n",
      "  Batch   600  of    683.    Elapsed: 0:04:09.\n",
      "\n",
      "  Average training loss: 0.34\n",
      "  Training epoch took: 0:04:43\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.94\n",
      "  Average validation loss: 0.19\n",
      "\n",
      "======== Epoch 2 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:20.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:43.\n",
      "  Batch   600  of    683.    Elapsed: 0:04:04.\n",
      "\n",
      "  Average training loss: 0.19\n",
      "  Training epoch took: 0:04:38\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.94\n",
      "  Average validation loss: 0.21\n",
      "\n",
      "======== Epoch 3 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:23.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:46.\n",
      "  Batch   600  of    683.    Elapsed: 0:04:09.\n",
      "\n",
      "  Average training loss: 0.16\n",
      "  Training epoch took: 0:04:43\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.94\n",
      "  Average validation loss: 0.21\n",
      "\n",
      "======== Epoch 4 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:23.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:46.\n",
      "  Batch   600  of    683.    Elapsed: 0:04:09.\n",
      "\n",
      "  Average training loss: 0.15\n",
      "  Training epoch took: 0:04:43\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.94\n",
      "  Average validation loss: 0.23\n",
      "\n",
      "======== Epoch 5 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:21.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:41.\n",
      "  Batch   600  of    683.    Elapsed: 0:04:03.\n",
      "\n",
      "  Average training loss: 0.14\n",
      "  Training epoch took: 0:04:37\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.94\n",
      "  Average validation loss: 0.26\n",
      "\n",
      "======== Epoch 6 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:19.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:38.\n",
      "  Batch   600  of    683.    Elapsed: 0:03:57.\n",
      "\n",
      "  Average training loss: 0.12\n",
      "  Training epoch took: 0:04:30\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.24\n",
      "\n",
      "======== Epoch 7 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:21.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:44.\n",
      "  Batch   600  of    683.    Elapsed: 0:04:04.\n",
      "\n",
      "  Average training loss: 0.11\n",
      "  Training epoch took: 0:04:37\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.25\n",
      "\n",
      "======== Epoch 8 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:23.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:47.\n",
      "  Batch   600  of    683.    Elapsed: 0:04:11.\n",
      "\n",
      "  Average training loss: 0.11\n",
      "  Training epoch took: 0:04:45\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.24\n",
      "\n",
      "======== Epoch 9 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:23.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:46.\n",
      "  Batch   600  of    683.    Elapsed: 0:04:08.\n",
      "\n",
      "  Average training loss: 0.10\n",
      "  Training epoch took: 0:04:42\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.24\n",
      "\n",
      "======== Epoch 10 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:21.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:44.\n",
      "  Batch   600  of    683.    Elapsed: 0:04:06.\n",
      "\n",
      "  Average training loss: 0.10\n",
      "  Training epoch took: 0:04:39\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.24\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▃▃▃▄▄▄▅▅▅▆▆▆▆▆▆▇▇▇███</td></tr><tr><td>train_batch_loss</td><td>██▂▆▂▂▁▁▂▁▄▅▁▁▅▄▂▃▁▅▁▁▃▁▄▁▁▁▁▁▁▄▂▁▁▂▅▄▁▅</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▂▂▁▁▁</td></tr><tr><td>val_f1</td><td>▁▁▄▅▄▆▇███</td></tr><tr><td>val_loss</td><td>▁▃▃▅█▆▇▆▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_batch_loss</td><td>0.00096</td></tr><tr><td>train_loss</td><td>0.09505</td></tr><tr><td>val_f1</td><td>0.95068</td></tr><tr><td>val_loss</td><td>0.23805</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">distinctive-sweep-21</strong> at: <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/v615gny2' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/v615gny2</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240301_154749-v615gny2\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: h2ce572z with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.15\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-06\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.001\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\AI projects\\Project-BERTcomparison\\wandb\\run-20240301_163823-h2ce572z</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/h2ce572z' target=\"_blank\">brisk-sweep-22</a></strong> to <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/h2ce572z' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/h2ce572z</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at michiyasunaga/BioLinkBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size = 16\n",
      "learning_rate = 5e-06\n",
      "\n",
      "======== Epoch 1 / 10 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    683.    Elapsed: 0:01:20.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:42.\n",
      "  Batch   600  of    683.    Elapsed: 0:04:03.\n",
      "\n",
      "  Average training loss: 0.36\n",
      "  Training epoch took: 0:04:36\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.93\n",
      "  Average validation loss: 0.22\n",
      "\n",
      "======== Epoch 2 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:19.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:39.\n",
      "  Batch   600  of    683.    Elapsed: 0:04:01.\n",
      "\n",
      "  Average training loss: 0.18\n",
      "  Training epoch took: 0:04:33\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.16\n",
      "\n",
      "======== Epoch 3 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:23.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:44.\n",
      "  Batch   600  of    683.    Elapsed: 0:04:04.\n",
      "\n",
      "  Average training loss: 0.15\n",
      "  Training epoch took: 0:04:37\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.22\n",
      "\n",
      "======== Epoch 4 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:19.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:40.\n",
      "  Batch   600  of    683.    Elapsed: 0:04:00.\n",
      "\n",
      "  Average training loss: 0.13\n",
      "  Training epoch took: 0:04:33\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.19\n",
      "\n",
      "======== Epoch 5 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:22.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:42.\n",
      "  Batch   600  of    683.    Elapsed: 0:04:02.\n",
      "\n",
      "  Average training loss: 0.11\n",
      "  Training epoch took: 0:04:35\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.21\n",
      "\n",
      "======== Epoch 6 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:19.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:38.\n",
      "  Batch   600  of    683.    Elapsed: 0:03:57.\n",
      "\n",
      "  Average training loss: 0.10\n",
      "  Training epoch took: 0:04:29\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.23\n",
      "\n",
      "======== Epoch 7 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:19.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:38.\n",
      "  Batch   600  of    683.    Elapsed: 0:03:58.\n",
      "\n",
      "  Average training loss: 0.09\n",
      "  Training epoch took: 0:04:31\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.25\n",
      "\n",
      "======== Epoch 8 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:20.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:39.\n",
      "  Batch   600  of    683.    Elapsed: 0:03:58.\n",
      "\n",
      "  Average training loss: 0.08\n",
      "  Training epoch took: 0:04:30\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.22\n",
      "\n",
      "======== Epoch 9 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:19.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:39.\n",
      "  Batch   600  of    683.    Elapsed: 0:03:59.\n",
      "\n",
      "  Average training loss: 0.08\n",
      "  Training epoch took: 0:04:32\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.24\n",
      "\n",
      "======== Epoch 10 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:20.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:39.\n",
      "  Batch   600  of    683.    Elapsed: 0:03:59.\n",
      "\n",
      "  Average training loss: 0.07\n",
      "  Training epoch took: 0:04:32\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.24\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1af8569ac52045f4885605c9964d231e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▃▃▃▄▄▄▅▅▅▆▆▆▆▆▆▇▇▇███</td></tr><tr><td>train_batch_loss</td><td>█▇▃▃▁▁▁▃▁▁▄▁▁▃▂▁▃▂▄▅▁▁▁▁▁▂▁▁▁▁▁▁▁▁▄▁▁▁▅▂</td></tr><tr><td>train_loss</td><td>█▄▃▃▂▂▂▁▁▁</td></tr><tr><td>val_f1</td><td>▁▇▆▇█▇▇███</td></tr><tr><td>val_loss</td><td>▆▁▅▃▅▇█▅▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_batch_loss</td><td>0.00149</td></tr><tr><td>train_loss</td><td>0.06888</td></tr><tr><td>val_f1</td><td>0.95231</td></tr><tr><td>val_loss</td><td>0.23877</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">brisk-sweep-22</strong> at: <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/h2ce572z' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/h2ce572z</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240301_163823-h2ce572z\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: yoxig7g8 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.01\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\AI projects\\Project-BERTcomparison\\wandb\\run-20240301_172740-yoxig7g8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/yoxig7g8' target=\"_blank\">silver-sweep-23</a></strong> to <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/yoxig7g8' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/yoxig7g8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at michiyasunaga/BioLinkBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size = 8\n",
      "learning_rate = 5e-05\n",
      "\n",
      "======== Epoch 1 / 10 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of  1,365.    Elapsed: 0:00:42.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:24.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:06.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:02:48.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:30.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:13.\n",
      "\n",
      "  Average training loss: 0.34\n",
      "  Training epoch took: 0:04:47\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.93\n",
      "  Average validation loss: 0.31\n",
      "\n",
      "======== Epoch 2 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:43.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:25.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:07.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:02:49.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:31.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:14.\n",
      "\n",
      "  Average training loss: 0.24\n",
      "  Training epoch took: 0:04:49\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.24\n",
      "\n",
      "======== Epoch 3 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:42.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:25.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:07.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:02:49.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:32.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:15.\n",
      "\n",
      "  Average training loss: 0.19\n",
      "  Training epoch took: 0:04:51\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.94\n",
      "  Average validation loss: 0.27\n",
      "\n",
      "======== Epoch 4 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:42.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:25.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:07.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:02:50.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:34.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:18.\n",
      "\n",
      "  Average training loss: 0.15\n",
      "  Training epoch took: 0:04:53\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.94\n",
      "  Average validation loss: 0.33\n",
      "\n",
      "======== Epoch 5 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:43.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:26.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:08.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:02:50.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:32.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:14.\n",
      "\n",
      "  Average training loss: 0.13\n",
      "  Training epoch took: 0:04:48\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.29\n",
      "\n",
      "======== Epoch 6 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:42.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:25.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:07.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:02:50.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:32.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:14.\n",
      "\n",
      "  Average training loss: 0.09\n",
      "  Training epoch took: 0:04:48\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.96\n",
      "  Average validation loss: 0.27\n",
      "\n",
      "======== Epoch 7 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:43.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:26.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:09.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:02:51.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:34.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:17.\n",
      "\n",
      "  Average training loss: 0.08\n",
      "  Training epoch took: 0:04:52\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.36\n",
      "\n",
      "======== Epoch 8 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:44.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:26.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:08.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:02:52.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:34.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:17.\n",
      "\n",
      "  Average training loss: 0.05\n",
      "  Training epoch took: 0:04:52\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.37\n",
      "\n",
      "======== Epoch 9 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:43.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:27.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:09.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:02:52.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:36.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:18.\n",
      "\n",
      "  Average training loss: 0.04\n",
      "  Training epoch took: 0:04:53\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.33\n",
      "\n",
      "======== Epoch 10 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of  1,365.    Elapsed: 0:00:43.\n",
      "  Batch   400  of  1,365.    Elapsed: 0:01:25.\n",
      "  Batch   600  of  1,365.    Elapsed: 0:02:07.\n",
      "  Batch   800  of  1,365.    Elapsed: 0:02:49.\n",
      "  Batch 1,000  of  1,365.    Elapsed: 0:03:31.\n",
      "  Batch 1,200  of  1,365.    Elapsed: 0:04:13.\n",
      "\n",
      "  Average training loss: 0.03\n",
      "  Training epoch took: 0:04:48\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.96\n",
      "  Average validation loss: 0.36\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12dcf35541a44817938777556ec0aafd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▃▃▃▄▄▄▅▅▅▆▆▆▆▆▆▇▇▇███</td></tr><tr><td>train_batch_loss</td><td>▁▁█▄▁▄▁▁▁▂▁▁▁▄▅▁▆▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅▁▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▆▅▄▃▂▂▂▁▁</td></tr><tr><td>val_f1</td><td>▁▆▃▄▇█▅▇▇█</td></tr><tr><td>val_loss</td><td>▅▁▃▆▄▂▇█▆▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_batch_loss</td><td>0.0001</td></tr><tr><td>train_loss</td><td>0.02681</td></tr><tr><td>val_f1</td><td>0.95558</td></tr><tr><td>val_loss</td><td>0.35542</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">silver-sweep-23</strong> at: <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/yoxig7g8' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/yoxig7g8</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240301_172740-yoxig7g8\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: fhyhd9o6 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.001\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\AI projects\\Project-BERTcomparison\\wandb\\run-20240301_181954-fhyhd9o6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/fhyhd9o6' target=\"_blank\">fearless-sweep-24</a></strong> to <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/fhyhd9o6' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/fhyhd9o6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at michiyasunaga/BioLinkBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size = 16\n",
      "learning_rate = 5e-05\n",
      "\n",
      "======== Epoch 1 / 10 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    683.    Elapsed: 0:01:19.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:37.\n",
      "  Batch   600  of    683.    Elapsed: 0:03:57.\n",
      "\n",
      "  Average training loss: 0.32\n",
      "  Training epoch took: 0:04:31\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.91\n",
      "  Average validation loss: 0.33\n",
      "\n",
      "======== Epoch 2 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:20.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:39.\n",
      "  Batch   600  of    683.    Elapsed: 0:03:58.\n",
      "\n",
      "  Average training loss: 0.19\n",
      "  Training epoch took: 0:04:31\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.94\n",
      "  Average validation loss: 0.29\n",
      "\n",
      "======== Epoch 3 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:20.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:42.\n",
      "  Batch   600  of    683.    Elapsed: 0:04:02.\n",
      "\n",
      "  Average training loss: 0.15\n",
      "  Training epoch took: 0:04:35\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.21\n",
      "\n",
      "======== Epoch 4 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:20.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:39.\n",
      "  Batch   600  of    683.    Elapsed: 0:03:58.\n",
      "\n",
      "  Average training loss: 0.13\n",
      "  Training epoch took: 0:04:30\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.25\n",
      "\n",
      "======== Epoch 5 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:18.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:37.\n",
      "  Batch   600  of    683.    Elapsed: 0:03:55.\n",
      "\n",
      "  Average training loss: 0.10\n",
      "  Training epoch took: 0:04:27\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.26\n",
      "\n",
      "======== Epoch 6 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:18.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:37.\n",
      "  Batch   600  of    683.    Elapsed: 0:03:55.\n",
      "\n",
      "  Average training loss: 0.08\n",
      "  Training epoch took: 0:04:27\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.28\n",
      "\n",
      "======== Epoch 7 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:19.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:37.\n",
      "  Batch   600  of    683.    Elapsed: 0:03:55.\n",
      "\n",
      "  Average training loss: 0.06\n",
      "  Training epoch took: 0:04:27\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.27\n",
      "\n",
      "======== Epoch 8 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:18.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:37.\n",
      "  Batch   600  of    683.    Elapsed: 0:03:55.\n",
      "\n",
      "  Average training loss: 0.04\n",
      "  Training epoch took: 0:04:27\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.31\n",
      "\n",
      "======== Epoch 9 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:18.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:37.\n",
      "  Batch   600  of    683.    Elapsed: 0:03:55.\n",
      "\n",
      "  Average training loss: 0.03\n",
      "  Training epoch took: 0:04:27\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.32\n",
      "\n",
      "======== Epoch 10 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:19.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:37.\n",
      "  Batch   600  of    683.    Elapsed: 0:03:55.\n",
      "\n",
      "  Average training loss: 0.03\n",
      "  Training epoch took: 0:04:28\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.30\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85e25ddb392b4510a66b892a5e85e33b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.020 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.055547…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▃▃▃▄▄▄▅▅▅▆▆▆▆▆▆▇▇▇███</td></tr><tr><td>train_batch_loss</td><td>▇▂▆▅▇▅█▄▁▁▅▁█▁█▆▅▁▄▁▅▁▄▁▃▇▁▁▁▁▁▁▁▆▁▆▁▇▁▄</td></tr><tr><td>train_loss</td><td>█▅▄▃▃▂▂▁▁▁</td></tr><tr><td>val_f1</td><td>▁▅▇▆▇▆█▇██</td></tr><tr><td>val_loss</td><td>█▆▁▃▄▅▄▇▇▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_batch_loss</td><td>0.00031</td></tr><tr><td>train_loss</td><td>0.03111</td></tr><tr><td>val_f1</td><td>0.95478</td></tr><tr><td>val_loss</td><td>0.30417</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fearless-sweep-24</strong> at: <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/fhyhd9o6' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/fhyhd9o6</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240301_181954-fhyhd9o6\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: wtilrjmh with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.15\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.01\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\AI projects\\Project-BERTcomparison\\wandb\\run-20240301_190833-wtilrjmh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/wtilrjmh' target=\"_blank\">frosty-sweep-25</a></strong> to <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/wtilrjmh' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/wtilrjmh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at michiyasunaga/BioLinkBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size = 16\n",
      "learning_rate = 5e-05\n",
      "\n",
      "======== Epoch 1 / 10 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    683.    Elapsed: 0:01:18.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:38.\n",
      "  Batch   600  of    683.    Elapsed: 0:03:59.\n",
      "\n",
      "  Average training loss: 0.27\n",
      "  Training epoch took: 0:04:33\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.94\n",
      "  Average validation loss: 0.21\n",
      "\n",
      "======== Epoch 2 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:20.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:39.\n",
      "  Batch   600  of    683.    Elapsed: 0:03:59.\n",
      "\n",
      "  Average training loss: 0.17\n",
      "  Training epoch took: 0:04:33\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.93\n",
      "  Average validation loss: 0.27\n",
      "\n",
      "======== Epoch 3 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:20.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:41.\n",
      "  Batch   600  of    683.    Elapsed: 0:04:03.\n",
      "\n",
      "  Average training loss: 0.11\n",
      "  Training epoch took: 0:04:36\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.23\n",
      "\n",
      "======== Epoch 4 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:21.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:42.\n",
      "  Batch   600  of    683.    Elapsed: 0:04:04.\n",
      "\n",
      "  Average training loss: 0.08\n",
      "  Training epoch took: 0:04:37\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.22\n",
      "\n",
      "======== Epoch 5 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:21.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:43.\n",
      "  Batch   600  of    683.    Elapsed: 0:04:04.\n",
      "\n",
      "  Average training loss: 0.07\n",
      "  Training epoch took: 0:04:38\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.96\n",
      "  Average validation loss: 0.21\n",
      "\n",
      "======== Epoch 6 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:22.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:44.\n",
      "  Batch   600  of    683.    Elapsed: 0:04:06.\n",
      "\n",
      "  Average training loss: 0.05\n",
      "  Training epoch took: 0:04:39\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.28\n",
      "\n",
      "======== Epoch 7 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:21.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:43.\n",
      "  Batch   600  of    683.    Elapsed: 0:04:04.\n",
      "\n",
      "  Average training loss: 0.03\n",
      "  Training epoch took: 0:04:37\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.96\n",
      "  Average validation loss: 0.27\n",
      "\n",
      "======== Epoch 8 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:21.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:40.\n",
      "  Batch   600  of    683.    Elapsed: 0:04:00.\n",
      "\n",
      "  Average training loss: 0.03\n",
      "  Training epoch took: 0:04:34\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.96\n",
      "  Average validation loss: 0.27\n",
      "\n",
      "======== Epoch 9 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:24.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:49.\n",
      "  Batch   600  of    683.    Elapsed: 0:04:13.\n",
      "\n",
      "  Average training loss: 0.02\n",
      "  Training epoch took: 0:04:48\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.96\n",
      "  Average validation loss: 0.28\n",
      "\n",
      "======== Epoch 10 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:22.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:42.\n",
      "  Batch   600  of    683.    Elapsed: 0:04:04.\n",
      "\n",
      "  Average training loss: 0.01\n",
      "  Training epoch took: 0:04:38\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.96\n",
      "  Average validation loss: 0.29\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "198237e1ed9141a1bad1e19142edc3cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▃▃▃▄▄▄▅▅▅▆▆▆▆▆▆▇▇▇███</td></tr><tr><td>train_batch_loss</td><td>▅▄▅█▁█▁▂▁▃▅▃▁▁▁▁▁▁▁▁▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▅▄▃▃▂▁▁▁▁</td></tr><tr><td>val_f1</td><td>▄▁▇▇█▇▇▇██</td></tr><tr><td>val_loss</td><td>▁▆▃▂▂▇▇▆██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_batch_loss</td><td>0.0001</td></tr><tr><td>train_loss</td><td>0.01207</td></tr><tr><td>val_f1</td><td>0.95683</td></tr><tr><td>val_loss</td><td>0.288</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">frosty-sweep-25</strong> at: <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/wtilrjmh' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/wtilrjmh</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240301_190833-wtilrjmh\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: bqlwlrct with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.15\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-06\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.01\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\AI projects\\Project-BERTcomparison\\wandb\\run-20240301_195843-bqlwlrct</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/bqlwlrct' target=\"_blank\">comfy-sweep-26</a></strong> to <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/sweeps/h06m7k9y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/bqlwlrct' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/bqlwlrct</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at michiyasunaga/BioLinkBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size = 16\n",
      "learning_rate = 5e-06\n",
      "\n",
      "======== Epoch 1 / 10 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    683.    Elapsed: 0:01:24.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:45.\n",
      "  Batch   600  of    683.    Elapsed: 0:04:06.\n",
      "\n",
      "  Average training loss: 0.32\n",
      "  Training epoch took: 0:04:41\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.93\n",
      "  Average validation loss: 0.20\n",
      "\n",
      "======== Epoch 2 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:21.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:41.\n",
      "  Batch   600  of    683.    Elapsed: 0:04:02.\n",
      "\n",
      "  Average training loss: 0.17\n",
      "  Training epoch took: 0:04:35\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.19\n",
      "\n",
      "======== Epoch 3 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:21.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:43.\n",
      "  Batch   600  of    683.    Elapsed: 0:04:04.\n",
      "\n",
      "  Average training loss: 0.14\n",
      "  Training epoch took: 0:04:38\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.21\n",
      "\n",
      "======== Epoch 4 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:21.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:41.\n",
      "  Batch   600  of    683.    Elapsed: 0:04:02.\n",
      "\n",
      "  Average training loss: 0.12\n",
      "  Training epoch took: 0:04:35\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.22\n",
      "\n",
      "======== Epoch 5 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:20.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:43.\n",
      "  Batch   600  of    683.    Elapsed: 0:04:07.\n",
      "\n",
      "  Average training loss: 0.11\n",
      "  Training epoch took: 0:04:41\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.22\n",
      "\n",
      "======== Epoch 6 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:21.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:41.\n",
      "  Batch   600  of    683.    Elapsed: 0:04:01.\n",
      "\n",
      "  Average training loss: 0.10\n",
      "  Training epoch took: 0:04:33\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.21\n",
      "\n",
      "======== Epoch 7 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:21.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:42.\n",
      "  Batch   600  of    683.    Elapsed: 0:04:02.\n",
      "\n",
      "  Average training loss: 0.09\n",
      "  Training epoch took: 0:04:37\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.24\n",
      "\n",
      "======== Epoch 8 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:20.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:39.\n",
      "  Batch   600  of    683.    Elapsed: 0:04:02.\n",
      "\n",
      "  Average training loss: 0.07\n",
      "  Training epoch took: 0:04:34\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.25\n",
      "\n",
      "======== Epoch 9 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:20.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:39.\n",
      "  Batch   600  of    683.    Elapsed: 0:04:04.\n",
      "\n",
      "  Average training loss: 0.07\n",
      "  Training epoch took: 0:04:37\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.96\n",
      "  Average validation loss: 0.23\n",
      "\n",
      "======== Epoch 10 / 10 ========\n",
      "Training...\n",
      "  Batch   200  of    683.    Elapsed: 0:01:20.\n",
      "  Batch   400  of    683.    Elapsed: 0:02:41.\n",
      "  Batch   600  of    683.    Elapsed: 0:04:02.\n",
      "\n",
      "  Average training loss: 0.07\n",
      "  Training epoch took: 0:04:36\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.95\n",
      "  Average validation loss: 0.23\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▃▃▃▄▄▄▅▅▅▆▆▆▆▆▆▇▇▇███</td></tr><tr><td>train_batch_loss</td><td>██▆▁▄▅█▅▂▁▄▁▃▁▁▄▁▄▁▄▅▁▅▁▄▁▁▁▄▂▄▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▂▁▁▁▁</td></tr><tr><td>val_f1</td><td>▁▆▆▅▇▇▇▇██</td></tr><tr><td>val_loss</td><td>▂▁▄▄▄▄▆█▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_batch_loss</td><td>0.00073</td></tr><tr><td>train_loss</td><td>0.07295</td></tr><tr><td>val_f1</td><td>0.95436</td></tr><tr><td>val_loss</td><td>0.23467</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">comfy-sweep-26</strong> at: <a href='https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/bqlwlrct' target=\"_blank\">https://wandb.ai/jon-l-collins/michiyasunagaBioLinkBERT-base_2024-02-29/runs/bqlwlrct</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240301_195843-bqlwlrct\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Exiting.\n"
     ]
    }
   ],
   "source": [
    "wandb.agent(sweep_id, train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bestpytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
