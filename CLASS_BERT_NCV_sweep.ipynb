{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file variables\n",
    "\n",
    "#pretrained_bert = 'cimm-kzn/endr-bert'\n",
    "pretrained_bert = 'allenai/scibert_scivocab_uncased'\n",
    "#pretrained_bert = 'dmis-lab/biobert-base-cased-v1.2'\n",
    "#pretrained_bert = 'bert-large-uncased'\n",
    "#pretrained_bert = 'microsoft/BiomedNLP-BiomedBERT-large-uncased-abstract'\n",
    "#pretrained_bert = 'microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext'\n",
    "#pretrained_bert = 'dmis-lab/biobert-v1.1'\n",
    "#pretrained_bert = 'bert-base-uncased'\n",
    "#pretrained_bert = 'SpanBERT/spanbert-base-cased\n",
    "#pretrained_bert = 'google/pegasus-xsum'\n",
    "#pretrained_bert = 'facebook/bart-base'\n",
    "#pretrained_bert = 'emilyalsentzer/Bio_ClinicalBERT'\n",
    "#pretrained_bert = 'allenai/scibert_scivocab cased'\n",
    "\n",
    "\n",
    "#TRAINING FILE\n",
    "use_huggingface = True\n",
    "training_file = \"**training**.csv\"\n",
    "# training_file = \"**training**.csv\"\n",
    "column1_AEtext = \"text\" #\"Example\"\n",
    "column2_AEflag = \"label\" #\"AE Flag\"\n",
    "train_test_split_percent = 0.8\n",
    "balance_multiplier = 1.0\n",
    "#batch_size = 3\n",
    "balance_me = True\n",
    "cross_val_number = 5\n",
    "weight_decay = 0.01\n",
    "#balance_me = False\n",
    "\n",
    "#TEST FILE\n",
    "test_file = \"**test**.csv\"\n",
    "# test_file = \"**test**.csv\"\n",
    "test_column1_AEtext = \"Example\"\n",
    "test_column2_AEflag = \"AE Flag\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = 'BERTcomparison.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find BERTcomparison.ipynb.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjon-l-collins\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'bayes'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = {\n",
    "        'name': 'val_loss',\n",
    "        'goal': 'minimize'\n",
    "}\n",
    "sweep_config['metric'] = metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-02-02 21:17:49.860864\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "print(str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: vpaxo9s7\n",
      "Sweep URL: https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02/sweeps/vpaxo9s7\n"
     ]
    }
   ],
   "source": [
    "parameters_dict = {\n",
    "    'learning_rate': {\n",
    "        'values': [5e-6]\n",
    "    },\n",
    "    'batch_size': {\n",
    "        'values': [12]\n",
    "        },\n",
    "    \n",
    "    'dropout': {\n",
    "          'values': [0.1,0.4,0.5]\n",
    "        },\n",
    "    \n",
    "    'epochs': {\n",
    "        'values': [14]\n",
    "    }   \n",
    " \n",
    "}\n",
    "sweep_config['parameters'] = parameters_dict\n",
    "pretrained_bert2 = pretrained_bert.replace('/','')\n",
    "sweep_id = wandb.sweep(sweep_config, project=f\"{pretrained_bert2}_{str(datetime.datetime.now())[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "#from tqdm.notebook import tqdm\n",
    "#from tqdm import tqdm\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, BertConfig, AutoTokenizer, AutoModelForSequenceClassification,AutoConfig\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "use_huggingface = True\n",
    "if use_huggingface == True:\n",
    "    dataset = load_dataset(\"ade_corpus_v2\",\"Ade_corpus_v2_classification\")\n",
    "else:\n",
    "    dataset = pd.read_csv(training_file, encoding='unicode_escape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_huggingface == True:\n",
    "    #convert to pandas dataframe\n",
    "    df = pd.DataFrame(dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    16695\n",
       "1     6821\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[column2_AEflag].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_AEs = df[column2_AEflag].value_counts()[1]\n",
    "balance_number = int(num_AEs * balance_multiplier)\n",
    "\n",
    "# num_AEs = 500\n",
    "# balance_number = int(num_AEs * balance_multiplier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_AEs = 6821\n",
      "balance_number = 6821\n",
      "balance_multiplier = 1.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"num_AEs = {num_AEs}\\nbalance_number = {balance_number}\\nbalance_multiplier = {balance_multiplier}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(balance_me == True):\n",
    "    d_NoAE = df[df[column2_AEflag]==0]\n",
    "    shuffle_d_NoAE = d_NoAE.sample(frac=1,random_state=42)[:balance_number]\n",
    "    d_AE = df[df[column2_AEflag]==1]\n",
    "    #d_AE = df[df[column2_AEflag]==1][:500]\n",
    "    df = pd.concat([shuffle_d_NoAE,d_AE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    6821\n",
       "1    6821\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[column2_AEflag].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\sklearn\\utils\\validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\sklearn\\utils\\validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_data,test = train_test_split(df, test_size=1-train_test_split_percent,stratify=df[column2_AEflag], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10913, 2), (2729, 2))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, random_split\n",
    "import random\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "def process_data(df_train,df_test):\n",
    "    #tokenizer = BertTokenizer.from_pretrained(pretrained_bert, do_lower_case=True)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(pretrained_bert, add_prefix_space=True, use_fast=False)\n",
    "    encoded_data_train = tokenizer.batch_encode_plus(df_train[column1_AEtext].values,\n",
    "                                                    add_special_tokens=True,\n",
    "                                                    return_attention_mask=True,\n",
    "                                                    pad_to_max_length=True,\n",
    "                                                    max_length=512,\n",
    "                                                    return_tensors='pt')\n",
    "    encoded_data_val = tokenizer.batch_encode_plus(df_test[column1_AEtext].values,\n",
    "                                                    add_special_tokens=True,\n",
    "                                                    return_attention_mask=True,\n",
    "                                                    pad_to_max_length=True,\n",
    "                                                    max_length=512,\n",
    "                                                    return_tensors='pt')\n",
    "    input_ids_train = encoded_data_train['input_ids']\n",
    "    attention_masks_train = encoded_data_train['attention_mask']\n",
    "    #labels = torch.tensor(dataset['AE Flag'].replace(label_dict).values)\n",
    "    labels_train = torch.tensor(df_train[column2_AEflag].values)    \n",
    "    input_ids_val = encoded_data_val['input_ids']\n",
    "    attention_masks_val = encoded_data_val['attention_mask']\n",
    "    #labels = torch.tensor(dataset['AE Flag'].replace(label_dict).values)\n",
    "    labels_val = torch.tensor(df_test[column2_AEflag].values)\n",
    "    dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n",
    "    dataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)\n",
    "    batch_size = wandb.config.batch_size\n",
    "    dataloader_train = DataLoader(dataset_train, sampler=RandomSampler(dataset_train), batch_size=batch_size)\n",
    "    dataloader_validation = DataLoader(dataset_val, sampler = SequentialSampler(dataset_val), batch_size=batch_size)\n",
    "    return dataloader_train,dataloader_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ret_model():\n",
    "    \n",
    "    configuration = AutoConfig.from_pretrained(pretrained_bert)\n",
    "    configuration.hidden_dropout_prob = wandb.config.dropout\n",
    "    configuration.attention_probs_dropout_prob = wandb.config.dropout\n",
    "    configuration.num_labels = 2\n",
    "    configuration.output_attentions = False\n",
    "    configuration.output_hidden_states = False\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(pretrained_bert,config=configuration)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "def ret_optim(model):\n",
    "    print(f\"learning_rate = {wandb.config.learning_rate}\")\n",
    "    optimizer = AdamW(model.parameters(),\n",
    "                      lr = wandb.config.learning_rate, \n",
    "                      eps = 1e-8,weight_decay=weight_decay)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ret_scheduler(optimizer, dataloader_train):\n",
    "    epochs = wandb.config.epochs\n",
    "    total_steps = len(dataloader_train) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                                num_warmup_steps = 0, \n",
    "                                                num_training_steps = total_steps)\n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(elapsed):\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def f1_score_func(preds,labels):\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return f1_score(labels_flat, preds_flat, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(text,tokenizer,device,model):\n",
    "    encoded_input = tokenizer(text, return_tensors=\"pt\",truncation=True,max_length=512,padding=True)\n",
    "    encoded_input.to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(**encoded_input)\n",
    "    res = output.logits[0].detach().to('cpu').numpy()\n",
    "    return np.argmax(res)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classificationreporter(model,model_name):\n",
    "    #tokenizer = BertTokenizer.from_pretrained(model_name, do_lower_case=True)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, add_prefix_space=True, use_fast=False)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    results=[]\n",
    "    for text in test[column1_AEtext].values:\n",
    "        results.append(classify(text,tokenizer,device,model))\n",
    "    ground_truth=test[column2_AEflag].values\n",
    "    conf = confusion_matrix(ground_truth, results)\n",
    "    ac=accuracy_score(ground_truth, results)\n",
    "    #print(conf)\n",
    "    print('Accuracy Score on test data:',ac)\n",
    "    return conf,ground_truth,results,ac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classificationreporterval(model,model_name,val_df):\n",
    "    #tokenizer = BertTokenizer.from_pretrained(model_name, do_lower_case=True)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, add_prefix_space=True, use_fast=False)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    results=[]\n",
    "    for text in val_df[column1_AEtext].values:\n",
    "        results.append(classify(text,tokenizer,device,model))\n",
    "    ground_truth=val_df[column2_AEflag].values\n",
    "    conf = confusion_matrix(ground_truth, results)\n",
    "    ac=accuracy_score(ground_truth, results)\n",
    "    #print(conf)\n",
    "    print('Accuracy Score on validation data:',ac)\n",
    "    return conf,ground_truth,results,ac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score,f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "def train(model,train_df,val_df):\n",
    "    epochs = wandb.config.epochs\n",
    "    dataloader_train, dataloader_validation = process_data(train_df,val_df)\n",
    "    total_val_loss=[]\n",
    "    total_val_f1=[]\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    for epoch_i in range(0, epochs):\n",
    "        print(\"\")\n",
    "        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "        print('Training...')\n",
    "        optimizer = ret_optim(model)\n",
    "        scheduler = ret_scheduler(optimizer, dataloader_train)\n",
    "        training_stats = []\n",
    "        t0 = time.time()\n",
    "        total_train_loss = 0\n",
    "        model.train()   \n",
    "        for step, batch in enumerate(dataloader_train):\n",
    "            if step % 200 == 0 and not step == 0:\n",
    "                elapsed = format_time(time.time() - t0)\n",
    "                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(dataloader_train), elapsed))\n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "            model.zero_grad()\n",
    "            outputs = model(b_input_ids,\n",
    "                            token_type_ids=None,\n",
    "                            attention_mask=b_input_mask,\n",
    "                            labels=b_labels)\n",
    "            loss = outputs[0]\n",
    "            total_train_loss += loss.item()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        avg_train_loss = total_train_loss / len(dataloader_train)\n",
    "        training_time = format_time(time.time() - t0)\n",
    "        print(\"\")\n",
    "        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "        print(\"  Training epoch took: {:}\".format(training_time))\n",
    "        print(\"\")\n",
    "        model.eval()\n",
    "        total_eval_loss = 0\n",
    "        predictions, true_vals = [], []\n",
    "        #nb_eval_steps = 0\n",
    "        for batch in dataloader_validation:\n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(b_input_ids,\n",
    "                                token_type_ids=None,\n",
    "                                attention_mask=b_input_mask,\n",
    "                                labels=b_labels)\n",
    "                loss=outputs[0]\n",
    "                logits=outputs[1]\n",
    "            total_eval_loss += loss.item()\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "            predictions.append(logits)\n",
    "            true_vals.append(label_ids)\n",
    "\n",
    "        predictions = np.concatenate(predictions, axis=0) #added\n",
    "        true_vals = np.concatenate(true_vals, axis=0) #added\n",
    "\n",
    "        val_f1 = f1_score_func(predictions, true_vals)\n",
    "        total_val_f1.append(val_f1) #added\n",
    "        avg_val_loss = total_eval_loss / len(dataloader_validation)\n",
    "        total_val_loss.append(avg_val_loss)\n",
    "    conf_val,ground_truth_val,results_val,accuracy_score_val=classificationreporterval(model,pretrained_bert, val_df)\n",
    "    recall_val = recall_score(ground_truth_val, results_val, pos_label=1)\n",
    "    f1_val = f1_score(ground_truth_val, results_val, pos_label=1)\n",
    "    print('Recall value for validation data is :',recall_val)\n",
    "    print('F1 value for validation data is :',f1_val)\n",
    "    print('Confusion Matrix :')\n",
    "    print(conf_val)\n",
    "    print('Report on val data: ')\n",
    "    print(classification_report(ground_truth_val, results_val))\n",
    "    print(\"Running cumulative Classification Report on test data...\") #print at end of each fold\n",
    "    conf,ground_truth,results,accuracy_score=classificationreporter(model,model_name=pretrained_bert)  \n",
    "    recall = recall_score(ground_truth, results, pos_label=1)\n",
    "    f1 = f1_score(ground_truth, results, pos_label=1)\n",
    "    #recall_test.append(recall)\n",
    "    #f1_test.append(f1)\n",
    "    print('Recall value for test data is :',recall)\n",
    "    print('F1 value for test data is :',f1)\n",
    "    print('Confusion Matrix :')  \n",
    "    print(conf)\n",
    "    print('Report on test data: ')\n",
    "    print(classification_report(ground_truth, results))                           \n",
    "    return total_val_loss[-1],recall_val,f1_val,accuracy_score_val,conf_val,recall,f1,conf,accuracy_score\n",
    "    #val_loss,recall_val,f1_val,accuracy_score_val,conf_val,recall,f1_test,conf,accuracy_score=train(model,train_df,val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "def train_folds():\n",
    "    kf = KFold(n_splits=cross_val_number, random_state=42, shuffle=True)\n",
    "    wandb.init()\n",
    "    val=[]\n",
    "    f1=[]\n",
    "    rec_val=[]\n",
    "    f1_v=[]\n",
    "    acc_score_val=[]\n",
    "    rec=[]\n",
    "    confusion_val=[]\n",
    "    confusion_test=[]\n",
    "    acc_score=[]\n",
    "    for fold,(train_index, val_index) in enumerate(kf.split(train_data)):\n",
    "        print('======== Fold {:} / {:} ========'.format(fold+1,cross_val_number))\n",
    "        train_df = train_data.iloc[train_index]\n",
    "        val_df = train_data.iloc[val_index]\n",
    "        model=ret_model()\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        model.to(device)\n",
    "        val_loss,recall_val,f1_val,accuracy_score_val,conf_val,recall,f1_test,conf,accuracy_score=train(model,train_df,val_df)\n",
    "        val.append(val_loss)\n",
    "        f1.append(f1_test)\n",
    "        rec_val.append(recall_val)\n",
    "        f1_v.append(f1_val)\n",
    "        acc_score_val.append(accuracy_score_val)\n",
    "        rec.append(recall)\n",
    "        confusion_val.append(np.array(conf_val))\n",
    "        confusion_test.append(np.array(conf))\n",
    "        acc_score.append(accuracy_score)\n",
    "    \n",
    "    f1_avg=sum(f1_v)/len(f1_v)\n",
    "    val_avg=sum(val)/len(val)\n",
    "    avg_acc_val=sum(acc_score_val)/len(acc_score_val)\n",
    "    avg_rec_val=sum(rec_val)/len(rec_val)\n",
    "    avg_acc=sum(acc_score)/len(acc_score)\n",
    "    avg_recall=sum(rec)/len(rec)\n",
    "    f1_test=sum(f1)/len(f1)\n",
    "    average_conf_val=np.mean(confusion_val,axis=0)\n",
    "    average_conf_test=np.mean(confusion_test,axis=0)\n",
    "    print('Average Confusion Matrix for validation data:')\n",
    "    print(average_conf_val)   \n",
    "    print('Average Confusion Matrix for test data:')\n",
    "    print(average_conf_test)    \n",
    "    wandb.log({\"val_loss\":val_avg})\n",
    "    wandb.log({\"recall_validation\": avg_rec_val})\n",
    "    wandb.log({\"f1_validation\": f1_avg})\n",
    "    wandb.log({\"recall_test\": avg_recall})\n",
    "    wandb.log({\"f1_test\": f1_test})\n",
    "    wandb.log({\"Accuracy on test data\": avg_acc})\n",
    "    wandb.log({\"Accuracy on validation data\": avg_acc_val})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 70fri1y6 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 12\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-06\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find BERTcomparison.ipynb.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\AI projects\\Project-BERTcomparison\\wandb\\run-20240202_211758-70fri1y6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02/runs/70fri1y6' target=\"_blank\">toasty-sweep-1</a></strong> to <a href='https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02/sweeps/vpaxo9s7' target=\"_blank\">https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02/sweeps/vpaxo9s7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02' target=\"_blank\">https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02/sweeps/vpaxo9s7' target=\"_blank\">https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02/sweeps/vpaxo9s7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02/runs/70fri1y6' target=\"_blank\">https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02/runs/70fri1y6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Fold 1 / 5 ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:01:13.\n",
      "  Batch   400  of    728.    Elapsed: 0:02:13.\n",
      "  Batch   600  of    728.    Elapsed: 0:03:12.\n",
      "\n",
      "  Average training loss: 0.56\n",
      "  Training epoch took: 0:03:50\n",
      "\n",
      "\n",
      "======== Epoch 2 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.37\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 3 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.30\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 4 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.27\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 5 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.26\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 6 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.24\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 7 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.23\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 8 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:58.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.23\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 9 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.22\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 10 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.21\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 11 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:58.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.21\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 12 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:58.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:55.\n",
      "\n",
      "  Average training loss: 0.18\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 13 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.20\n",
      "  Training epoch took: 0:03:34\n",
      "\n",
      "\n",
      "======== Epoch 14 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.18\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "Accuracy Score on validation data: 0.9253321117727897\n",
      "Recall value for validation data is : 0.9813780260707635\n",
      "F1 value for validation data is : 0.9282254513430207\n",
      "Confusion Matrix :\n",
      "[[ 966  143]\n",
      " [  20 1054]]\n",
      "Report on val data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.87      0.92      1109\n",
      "           1       0.88      0.98      0.93      1074\n",
      "\n",
      "    accuracy                           0.93      2183\n",
      "   macro avg       0.93      0.93      0.93      2183\n",
      "weighted avg       0.93      0.93      0.93      2183\n",
      "\n",
      "Running cumulative Classification Report on test data...\n",
      "Accuracy Score on test data: 0.9391718578233785\n",
      "Recall value for test data is : 0.9831378299120235\n",
      "F1 value for test data is : 0.9417134831460675\n",
      "Confusion Matrix :\n",
      "[[1222  143]\n",
      " [  23 1341]]\n",
      "Report on test data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.90      0.94      1365\n",
      "           1       0.90      0.98      0.94      1364\n",
      "\n",
      "    accuracy                           0.94      2729\n",
      "   macro avg       0.94      0.94      0.94      2729\n",
      "weighted avg       0.94      0.94      0.94      2729\n",
      "\n",
      "======== Fold 2 / 5 ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:58.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:55.\n",
      "\n",
      "  Average training loss: 0.54\n",
      "  Training epoch took: 0:03:32\n",
      "\n",
      "\n",
      "======== Epoch 2 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:58.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:55.\n",
      "\n",
      "  Average training loss: 0.35\n",
      "  Training epoch took: 0:03:32\n",
      "\n",
      "\n",
      "======== Epoch 3 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.30\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 4 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.27\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 5 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.25\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 6 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.25\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 7 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.24\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 8 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.23\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 9 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.21\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 10 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.21\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 11 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.20\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 12 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.20\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 13 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.18\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 14 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.18\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "Accuracy Score on validation data: 0.9147961520842877\n",
      "Recall value for validation data is : 0.9802158273381295\n",
      "F1 value for validation data is : 0.9213863060016906\n",
      "Confusion Matrix :\n",
      "[[ 907  164]\n",
      " [  22 1090]]\n",
      "Report on val data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.85      0.91      1071\n",
      "           1       0.87      0.98      0.92      1112\n",
      "\n",
      "    accuracy                           0.91      2183\n",
      "   macro avg       0.92      0.91      0.91      2183\n",
      "weighted avg       0.92      0.91      0.91      2183\n",
      "\n",
      "Running cumulative Classification Report on test data...\n",
      "Accuracy Score on test data: 0.9153536093807255\n",
      "Recall value for test data is : 0.9838709677419355\n",
      "F1 value for test data is : 0.9207547169811321\n",
      "Confusion Matrix :\n",
      "[[1156  209]\n",
      " [  22 1342]]\n",
      "Report on test data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.85      0.91      1365\n",
      "           1       0.87      0.98      0.92      1364\n",
      "\n",
      "    accuracy                           0.92      2729\n",
      "   macro avg       0.92      0.92      0.91      2729\n",
      "weighted avg       0.92      0.92      0.91      2729\n",
      "\n",
      "======== Fold 3 / 5 ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.55\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 2 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.36\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 3 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:55.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:54.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:52.\n",
      "\n",
      "  Average training loss: 0.30\n",
      "  Training epoch took: 0:03:30\n",
      "\n",
      "\n",
      "======== Epoch 4 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.28\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 5 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:58.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.25\n",
      "  Training epoch took: 0:03:34\n",
      "\n",
      "\n",
      "======== Epoch 6 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:58.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.25\n",
      "  Training epoch took: 0:03:34\n",
      "\n",
      "\n",
      "======== Epoch 7 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.24\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 8 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.23\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 9 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.22\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 10 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:58.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.22\n",
      "  Training epoch took: 0:03:34\n",
      "\n",
      "\n",
      "======== Epoch 11 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:58.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:55.\n",
      "\n",
      "  Average training loss: 0.21\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 12 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.20\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 13 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.19\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 14 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.18\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "Accuracy Score on validation data: 0.9262482821804856\n",
      "Recall value for validation data is : 0.9822429906542056\n",
      "F1 value for validation data is : 0.9288555015466196\n",
      "Confusion Matrix :\n",
      "[[ 971  142]\n",
      " [  19 1051]]\n",
      "Report on val data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.87      0.92      1113\n",
      "           1       0.88      0.98      0.93      1070\n",
      "\n",
      "    accuracy                           0.93      2183\n",
      "   macro avg       0.93      0.93      0.93      2183\n",
      "weighted avg       0.93      0.93      0.93      2183\n",
      "\n",
      "Running cumulative Classification Report on test data...\n",
      "Accuracy Score on test data: 0.9281788200806156\n",
      "Recall value for test data is : 0.9816715542521994\n",
      "F1 value for test data is : 0.9318023660403617\n",
      "Confusion Matrix :\n",
      "[[1194  171]\n",
      " [  25 1339]]\n",
      "Report on test data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.87      0.92      1365\n",
      "           1       0.89      0.98      0.93      1364\n",
      "\n",
      "    accuracy                           0.93      2729\n",
      "   macro avg       0.93      0.93      0.93      2729\n",
      "weighted avg       0.93      0.93      0.93      2729\n",
      "\n",
      "======== Fold 4 / 5 ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.54\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 2 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.36\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 3 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.31\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 4 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.27\n",
      "  Training epoch took: 0:03:34\n",
      "\n",
      "\n",
      "======== Epoch 5 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:58.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.25\n",
      "  Training epoch took: 0:03:34\n",
      "\n",
      "\n",
      "======== Epoch 6 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.25\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 7 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:58.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:55.\n",
      "\n",
      "  Average training loss: 0.24\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 8 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.23\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 9 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.23\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 10 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.22\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 11 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:58.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.21\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 12 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.20\n",
      "  Training epoch took: 0:03:34\n",
      "\n",
      "\n",
      "======== Epoch 13 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.19\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 14 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.18\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "Accuracy Score on validation data: 0.9431714023831348\n",
      "Recall value for validation data is : 0.9795008912655971\n",
      "F1 value for validation data is : 0.9465977605512489\n",
      "Confusion Matrix :\n",
      "[[ 959  101]\n",
      " [  23 1099]]\n",
      "Report on val data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.90      0.94      1060\n",
      "           1       0.92      0.98      0.95      1122\n",
      "\n",
      "    accuracy                           0.94      2182\n",
      "   macro avg       0.95      0.94      0.94      2182\n",
      "weighted avg       0.95      0.94      0.94      2182\n",
      "\n",
      "Running cumulative Classification Report on test data...\n",
      "Accuracy Score on test data: 0.9435690729204836\n",
      "Recall value for test data is : 0.9706744868035191\n",
      "F1 value for test data is : 0.9450392576730907\n",
      "Confusion Matrix :\n",
      "[[1251  114]\n",
      " [  40 1324]]\n",
      "Report on test data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.92      0.94      1365\n",
      "           1       0.92      0.97      0.95      1364\n",
      "\n",
      "    accuracy                           0.94      2729\n",
      "   macro avg       0.94      0.94      0.94      2729\n",
      "weighted avg       0.94      0.94      0.94      2729\n",
      "\n",
      "======== Fold 5 / 5 ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:58.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:55.\n",
      "\n",
      "  Average training loss: 0.59\n",
      "  Training epoch took: 0:03:32\n",
      "\n",
      "\n",
      "======== Epoch 2 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:58.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:56.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:55.\n",
      "\n",
      "  Average training loss: 0.38\n",
      "  Training epoch took: 0:03:32\n",
      "\n",
      "\n",
      "======== Epoch 3 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.31\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 4 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.26\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 5 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.25\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 6 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.26\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 7 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.23\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 8 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:58.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.24\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 9 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.22\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 10 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.21\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 11 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.20\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 12 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:58.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:55.\n",
      "\n",
      "  Average training loss: 0.20\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 13 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:58.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:55.\n",
      "\n",
      "  Average training loss: 0.19\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 14 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.18\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "Accuracy Score on validation data: 0.9450045829514208\n",
      "Recall value for validation data is : 0.9703429101019463\n",
      "F1 value for validation data is : 0.94579945799458\n",
      "Confusion Matrix :\n",
      "[[1015   88]\n",
      " [  32 1047]]\n",
      "Report on val data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.92      0.94      1103\n",
      "           1       0.92      0.97      0.95      1079\n",
      "\n",
      "    accuracy                           0.95      2182\n",
      "   macro avg       0.95      0.95      0.94      2182\n",
      "weighted avg       0.95      0.95      0.94      2182\n",
      "\n",
      "Running cumulative Classification Report on test data...\n",
      "Accuracy Score on test data: 0.9406375961890803\n",
      "Recall value for test data is : 0.9530791788856305\n",
      "F1 value for test data is : 0.941346850108617\n",
      "Confusion Matrix :\n",
      "[[1267   98]\n",
      " [  64 1300]]\n",
      "Report on test data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.93      0.94      1365\n",
      "           1       0.93      0.95      0.94      1364\n",
      "\n",
      "    accuracy                           0.94      2729\n",
      "   macro avg       0.94      0.94      0.94      2729\n",
      "weighted avg       0.94      0.94      0.94      2729\n",
      "\n",
      "Average Confusion Matrix for validation data:\n",
      "[[ 963.6  127.6]\n",
      " [  23.2 1068.2]]\n",
      "Average Confusion Matrix for test data:\n",
      "[[1218.   147. ]\n",
      " [  34.8 1329.2]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy on test data</td><td></td></tr><tr><td>Accuracy on validation data</td><td></td></tr><tr><td>f1_test</td><td></td></tr><tr><td>f1_validation</td><td></td></tr><tr><td>recall_test</td><td></td></tr><tr><td>recall_validation</td><td></td></tr><tr><td>val_loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy on test data</td><td>0.93338</td></tr><tr><td>Accuracy on validation data</td><td>0.93091</td></tr><tr><td>f1_test</td><td>0.93613</td></tr><tr><td>f1_validation</td><td>0.93417</td></tr><tr><td>recall_test</td><td>0.97449</td></tr><tr><td>recall_validation</td><td>0.97874</td></tr><tr><td>val_loss</td><td>0.32729</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">toasty-sweep-1</strong> at: <a href='https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02/runs/70fri1y6' target=\"_blank\">https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02/runs/70fri1y6</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240202_211758-70fri1y6\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 4yapyme8 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 12\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-06\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find BERTcomparison.ipynb.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\AI projects\\Project-BERTcomparison\\wandb\\run-20240203_015447-4yapyme8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02/runs/4yapyme8' target=\"_blank\">amber-sweep-2</a></strong> to <a href='https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02/sweeps/vpaxo9s7' target=\"_blank\">https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02/sweeps/vpaxo9s7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02' target=\"_blank\">https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02/sweeps/vpaxo9s7' target=\"_blank\">https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02/sweeps/vpaxo9s7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02/runs/4yapyme8' target=\"_blank\">https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02/runs/4yapyme8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Fold 1 / 5 ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:58.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:55.\n",
      "\n",
      "  Average training loss: 0.54\n",
      "  Training epoch took: 0:03:32\n",
      "\n",
      "\n",
      "======== Epoch 2 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.34\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 3 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.29\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 4 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.27\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 5 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.25\n",
      "  Training epoch took: 0:03:34\n",
      "\n",
      "\n",
      "======== Epoch 6 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.24\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 7 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.23\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 8 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.22\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 9 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:58.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.22\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 10 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.21\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 11 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:58.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.21\n",
      "  Training epoch took: 0:03:34\n",
      "\n",
      "\n",
      "======== Epoch 12 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.20\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 13 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:58.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.18\n",
      "  Training epoch took: 0:03:34\n",
      "\n",
      "\n",
      "======== Epoch 14 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.18\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "Accuracy Score on validation data: 0.9303710490151168\n",
      "Recall value for validation data is : 0.9636871508379888\n",
      "F1 value for validation data is : 0.9315931593159316\n",
      "Confusion Matrix :\n",
      "[[ 996  113]\n",
      " [  39 1035]]\n",
      "Report on val data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.90      0.93      1109\n",
      "           1       0.90      0.96      0.93      1074\n",
      "\n",
      "    accuracy                           0.93      2183\n",
      "   macro avg       0.93      0.93      0.93      2183\n",
      "weighted avg       0.93      0.93      0.93      2183\n",
      "\n",
      "Running cumulative Classification Report on test data...\n",
      "Accuracy Score on test data: 0.9402711615976548\n",
      "Recall value for test data is : 0.968475073313783\n",
      "F1 value for test data is : 0.9418894830659537\n",
      "Confusion Matrix :\n",
      "[[1245  120]\n",
      " [  43 1321]]\n",
      "Report on test data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.91      0.94      1365\n",
      "           1       0.92      0.97      0.94      1364\n",
      "\n",
      "    accuracy                           0.94      2729\n",
      "   macro avg       0.94      0.94      0.94      2729\n",
      "weighted avg       0.94      0.94      0.94      2729\n",
      "\n",
      "======== Fold 2 / 5 ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:58.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.57\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 2 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.36\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 3 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:55.\n",
      "\n",
      "  Average training loss: 0.30\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 4 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.27\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 5 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:58.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.26\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 6 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.23\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 7 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.24\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 8 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.22\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 9 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.22\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 10 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:58.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.22\n",
      "  Training epoch took: 0:03:34\n",
      "\n",
      "\n",
      "======== Epoch 11 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.20\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 12 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.19\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 13 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.18\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 14 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.19\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "Accuracy Score on validation data: 0.9404489234997709\n",
      "Recall value for validation data is : 0.9622302158273381\n",
      "F1 value for validation data is : 0.9427312775330396\n",
      "Confusion Matrix :\n",
      "[[ 983   88]\n",
      " [  42 1070]]\n",
      "Report on val data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.92      0.94      1071\n",
      "           1       0.92      0.96      0.94      1112\n",
      "\n",
      "    accuracy                           0.94      2183\n",
      "   macro avg       0.94      0.94      0.94      2183\n",
      "weighted avg       0.94      0.94      0.94      2183\n",
      "\n",
      "Running cumulative Classification Report on test data...\n",
      "Accuracy Score on test data: 0.9475998534261634\n",
      "Recall value for test data is : 0.9721407624633431\n",
      "F1 value for test data is : 0.9488372093023256\n",
      "Confusion Matrix :\n",
      "[[1260  105]\n",
      " [  38 1326]]\n",
      "Report on test data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.92      0.95      1365\n",
      "           1       0.93      0.97      0.95      1364\n",
      "\n",
      "    accuracy                           0.95      2729\n",
      "   macro avg       0.95      0.95      0.95      2729\n",
      "weighted avg       0.95      0.95      0.95      2729\n",
      "\n",
      "======== Fold 3 / 5 ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:58.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:55.\n",
      "\n",
      "  Average training loss: 0.57\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 2 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.36\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 3 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.30\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 4 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.26\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 5 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.26\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 6 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.24\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 7 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:58.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.23\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 8 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.23\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 9 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:58.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.22\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 10 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.20\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 11 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.20\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 12 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.20\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 13 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.18\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 14 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:58.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.17\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "Accuracy Score on validation data: 0.9372423270728355\n",
      "Recall value for validation data is : 0.9728971962616823\n",
      "F1 value for validation data is : 0.9382604776926544\n",
      "Confusion Matrix :\n",
      "[[1005  108]\n",
      " [  29 1041]]\n",
      "Report on val data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.90      0.94      1113\n",
      "           1       0.91      0.97      0.94      1070\n",
      "\n",
      "    accuracy                           0.94      2183\n",
      "   macro avg       0.94      0.94      0.94      2183\n",
      "weighted avg       0.94      0.94      0.94      2183\n",
      "\n",
      "Running cumulative Classification Report on test data...\n",
      "Accuracy Score on test data: 0.9406375961890803\n",
      "Recall value for test data is : 0.9706744868035191\n",
      "F1 value for test data is : 0.9423487544483986\n",
      "Confusion Matrix :\n",
      "[[1243  122]\n",
      " [  40 1324]]\n",
      "Report on test data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.91      0.94      1365\n",
      "           1       0.92      0.97      0.94      1364\n",
      "\n",
      "    accuracy                           0.94      2729\n",
      "   macro avg       0.94      0.94      0.94      2729\n",
      "weighted avg       0.94      0.94      0.94      2729\n",
      "\n",
      "======== Fold 4 / 5 ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:58.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.56\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 2 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.36\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 3 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.30\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 4 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.26\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 5 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.27\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 6 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:58.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:55.\n",
      "\n",
      "  Average training loss: 0.25\n",
      "  Training epoch took: 0:03:32\n",
      "\n",
      "\n",
      "======== Epoch 7 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:58.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:55.\n",
      "\n",
      "  Average training loss: 0.23\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 8 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.22\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 9 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.23\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 10 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.21\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 11 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.21\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 12 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.20\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 13 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:58.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.19\n",
      "  Training epoch took: 0:03:34\n",
      "\n",
      "\n",
      "======== Epoch 14 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.17\n",
      "  Training epoch took: 0:03:34\n",
      "\n",
      "Accuracy Score on validation data: 0.9427131072410633\n",
      "Recall value for validation data is : 0.9759358288770054\n",
      "F1 value for validation data is : 0.9460043196544276\n",
      "Confusion Matrix :\n",
      "[[ 962   98]\n",
      " [  27 1095]]\n",
      "Report on val data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.91      0.94      1060\n",
      "           1       0.92      0.98      0.95      1122\n",
      "\n",
      "    accuracy                           0.94      2182\n",
      "   macro avg       0.95      0.94      0.94      2182\n",
      "weighted avg       0.94      0.94      0.94      2182\n",
      "\n",
      "Running cumulative Classification Report on test data...\n",
      "Accuracy Score on test data: 0.9413704653719311\n",
      "Recall value for test data is : 0.9750733137829912\n",
      "F1 value for test data is : 0.9432624113475178\n",
      "Confusion Matrix :\n",
      "[[1239  126]\n",
      " [  34 1330]]\n",
      "Report on test data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.91      0.94      1365\n",
      "           1       0.91      0.98      0.94      1364\n",
      "\n",
      "    accuracy                           0.94      2729\n",
      "   macro avg       0.94      0.94      0.94      2729\n",
      "weighted avg       0.94      0.94      0.94      2729\n",
      "\n",
      "======== Fold 5 / 5 ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.54\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 2 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.36\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 3 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.30\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 4 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.27\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 5 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.26\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 6 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.25\n",
      "  Training epoch took: 0:03:34\n",
      "\n",
      "\n",
      "======== Epoch 7 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.24\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 8 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:58.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.24\n",
      "  Training epoch took: 0:03:34\n",
      "\n",
      "\n",
      "======== Epoch 9 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.22\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 10 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.21\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 11 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.21\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 12 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.21\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 13 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.19\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 14 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.19\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "Accuracy Score on validation data: 0.9376718606782768\n",
      "Recall value for validation data is : 0.9833178869323448\n",
      "F1 value for validation data is : 0.9397697077059345\n",
      "Confusion Matrix :\n",
      "[[ 985  118]\n",
      " [  18 1061]]\n",
      "Report on val data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.89      0.94      1103\n",
      "           1       0.90      0.98      0.94      1079\n",
      "\n",
      "    accuracy                           0.94      2182\n",
      "   macro avg       0.94      0.94      0.94      2182\n",
      "weighted avg       0.94      0.94      0.94      2182\n",
      "\n",
      "Running cumulative Classification Report on test data...\n",
      "Accuracy Score on test data: 0.9380725540491023\n",
      "Recall value for test data is : 0.9824046920821115\n",
      "F1 value for test data is : 0.9406809406809407\n",
      "Confusion Matrix :\n",
      "[[1220  145]\n",
      " [  24 1340]]\n",
      "Report on test data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.89      0.94      1365\n",
      "           1       0.90      0.98      0.94      1364\n",
      "\n",
      "    accuracy                           0.94      2729\n",
      "   macro avg       0.94      0.94      0.94      2729\n",
      "weighted avg       0.94      0.94      0.94      2729\n",
      "\n",
      "Average Confusion Matrix for validation data:\n",
      "[[ 986.2  105. ]\n",
      " [  31.  1060.4]]\n",
      "Average Confusion Matrix for test data:\n",
      "[[1241.4  123.6]\n",
      " [  35.8 1328.2]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy on test data</td><td></td></tr><tr><td>Accuracy on validation data</td><td></td></tr><tr><td>f1_test</td><td></td></tr><tr><td>f1_validation</td><td></td></tr><tr><td>recall_test</td><td></td></tr><tr><td>recall_validation</td><td></td></tr><tr><td>val_loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy on test data</td><td>0.94159</td></tr><tr><td>Accuracy on validation data</td><td>0.93769</td></tr><tr><td>f1_test</td><td>0.9434</td></tr><tr><td>f1_validation</td><td>0.93967</td></tr><tr><td>recall_test</td><td>0.97375</td></tr><tr><td>recall_validation</td><td>0.97161</td></tr><tr><td>val_loss</td><td>0.30258</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">amber-sweep-2</strong> at: <a href='https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02/runs/4yapyme8' target=\"_blank\">https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02/runs/4yapyme8</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240203_015447-4yapyme8\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ytrkj5go with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 12\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-06\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find BERTcomparison.ipynb.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\AI projects\\Project-BERTcomparison\\wandb\\run-20240203_063128-ytrkj5go</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02/runs/ytrkj5go' target=\"_blank\">flowing-sweep-3</a></strong> to <a href='https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02/sweeps/vpaxo9s7' target=\"_blank\">https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02/sweeps/vpaxo9s7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02' target=\"_blank\">https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02/sweeps/vpaxo9s7' target=\"_blank\">https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02/sweeps/vpaxo9s7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02/runs/ytrkj5go' target=\"_blank\">https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02/runs/ytrkj5go</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Fold 1 / 5 ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:58.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:55.\n",
      "\n",
      "  Average training loss: 0.29\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 2 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:58.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.17\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 3 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:55.\n",
      "\n",
      "  Average training loss: 0.12\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 4 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.09\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 5 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:58.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.05\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 6 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:58.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.05\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 7 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.03\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 8 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.02\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 9 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.02\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 10 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:58.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:55.\n",
      "\n",
      "  Average training loss: 0.02\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 11 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.02\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 12 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.01\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 13 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.01\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 14 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.01\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "Accuracy Score on validation data: 0.9482363719651855\n",
      "Recall value for validation data is : 0.9683426443202979\n",
      "F1 value for validation data is : 0.9484724122207022\n",
      "Confusion Matrix :\n",
      "[[1030   79]\n",
      " [  34 1040]]\n",
      "Report on val data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.93      0.95      1109\n",
      "           1       0.93      0.97      0.95      1074\n",
      "\n",
      "    accuracy                           0.95      2183\n",
      "   macro avg       0.95      0.95      0.95      2183\n",
      "weighted avg       0.95      0.95      0.95      2183\n",
      "\n",
      "Running cumulative Classification Report on test data...\n",
      "Accuracy Score on test data: 0.9571271528032246\n",
      "Recall value for test data is : 0.9780058651026393\n",
      "F1 value for test data is : 0.9579892280071813\n",
      "Confusion Matrix :\n",
      "[[1278   87]\n",
      " [  30 1334]]\n",
      "Report on test data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.94      0.96      1365\n",
      "           1       0.94      0.98      0.96      1364\n",
      "\n",
      "    accuracy                           0.96      2729\n",
      "   macro avg       0.96      0.96      0.96      2729\n",
      "weighted avg       0.96      0.96      0.96      2729\n",
      "\n",
      "======== Fold 2 / 5 ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:58.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.29\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 2 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.17\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 3 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.12\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 4 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.08\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 5 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.05\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 6 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.03\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 7 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:58.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.03\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 8 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:58.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:55.\n",
      "\n",
      "  Average training loss: 0.02\n",
      "  Training epoch took: 0:03:32\n",
      "\n",
      "\n",
      "======== Epoch 9 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:58.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.02\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 10 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.02\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 11 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.02\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 12 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:58.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.02\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 13 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:58.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.01\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 14 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.01\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "Accuracy Score on validation data: 0.9496106275767293\n",
      "Recall value for validation data is : 0.9460431654676259\n",
      "F1 value for validation data is : 0.950316169828365\n",
      "Confusion Matrix :\n",
      "[[1021   50]\n",
      " [  60 1052]]\n",
      "Report on val data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.95      0.95      1071\n",
      "           1       0.95      0.95      0.95      1112\n",
      "\n",
      "    accuracy                           0.95      2183\n",
      "   macro avg       0.95      0.95      0.95      2183\n",
      "weighted avg       0.95      0.95      0.95      2183\n",
      "\n",
      "Running cumulative Classification Report on test data...\n",
      "Accuracy Score on test data: 0.9519970685232686\n",
      "Recall value for test data is : 0.9450146627565983\n",
      "F1 value for test data is : 0.9516426725729051\n",
      "Confusion Matrix :\n",
      "[[1309   56]\n",
      " [  75 1289]]\n",
      "Report on test data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.96      0.95      1365\n",
      "           1       0.96      0.95      0.95      1364\n",
      "\n",
      "    accuracy                           0.95      2729\n",
      "   macro avg       0.95      0.95      0.95      2729\n",
      "weighted avg       0.95      0.95      0.95      2729\n",
      "\n",
      "======== Fold 3 / 5 ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.30\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 2 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.18\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 3 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:58.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.12\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 4 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.08\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 5 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:58.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:55.\n",
      "\n",
      "  Average training loss: 0.06\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 6 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.04\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 7 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:58.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:55.\n",
      "\n",
      "  Average training loss: 0.03\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 8 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.02\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 9 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.02\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 10 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:55.\n",
      "\n",
      "  Average training loss: 0.01\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 11 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:58.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:56.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:54.\n",
      "\n",
      "  Average training loss: 0.01\n",
      "  Training epoch took: 0:03:31\n",
      "\n",
      "\n",
      "======== Epoch 12 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:58.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:55.\n",
      "\n",
      "  Average training loss: 0.01\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 13 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.01\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 14 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:58.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:55.\n",
      "\n",
      "  Average training loss: 0.02\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "Accuracy Score on validation data: 0.951442968392121\n",
      "Recall value for validation data is : 0.9757009345794393\n",
      "F1 value for validation data is : 0.951686417502279\n",
      "Confusion Matrix :\n",
      "[[1033   80]\n",
      " [  26 1044]]\n",
      "Report on val data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.93      0.95      1113\n",
      "           1       0.93      0.98      0.95      1070\n",
      "\n",
      "    accuracy                           0.95      2183\n",
      "   macro avg       0.95      0.95      0.95      2183\n",
      "weighted avg       0.95      0.95      0.95      2183\n",
      "\n",
      "Running cumulative Classification Report on test data...\n",
      "Accuracy Score on test data: 0.9563942836203737\n",
      "Recall value for test data is : 0.9765395894428153\n",
      "F1 value for test data is : 0.9572403880704277\n",
      "Confusion Matrix :\n",
      "[[1278   87]\n",
      " [  32 1332]]\n",
      "Report on test data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.94      0.96      1365\n",
      "           1       0.94      0.98      0.96      1364\n",
      "\n",
      "    accuracy                           0.96      2729\n",
      "   macro avg       0.96      0.96      0.96      2729\n",
      "weighted avg       0.96      0.96      0.96      2729\n",
      "\n",
      "======== Fold 4 / 5 ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:58.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.31\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 2 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.18\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 3 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:58.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.13\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 4 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.08\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 5 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.06\n",
      "  Training epoch took: 0:03:34\n",
      "\n",
      "\n",
      "======== Epoch 6 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.04\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 7 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.03\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 8 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.02\n",
      "  Training epoch took: 0:03:34\n",
      "\n",
      "\n",
      "======== Epoch 9 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.02\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 10 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:58.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.02\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 11 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:58.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:55.\n",
      "\n",
      "  Average training loss: 0.02\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 12 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.02\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 13 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.01\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 14 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.01\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "Accuracy Score on validation data: 0.9523373052245646\n",
      "Recall value for validation data is : 0.9759358288770054\n",
      "F1 value for validation data is : 0.9546643417611159\n",
      "Confusion Matrix :\n",
      "[[ 983   77]\n",
      " [  27 1095]]\n",
      "Report on val data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.93      0.95      1060\n",
      "           1       0.93      0.98      0.95      1122\n",
      "\n",
      "    accuracy                           0.95      2182\n",
      "   macro avg       0.95      0.95      0.95      2182\n",
      "weighted avg       0.95      0.95      0.95      2182\n",
      "\n",
      "Running cumulative Classification Report on test data...\n",
      "Accuracy Score on test data: 0.9501648955661415\n",
      "Recall value for test data is : 0.9728739002932552\n",
      "F1 value for test data is : 0.9512544802867383\n",
      "Confusion Matrix :\n",
      "[[1266   99]\n",
      " [  37 1327]]\n",
      "Report on test data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.93      0.95      1365\n",
      "           1       0.93      0.97      0.95      1364\n",
      "\n",
      "    accuracy                           0.95      2729\n",
      "   macro avg       0.95      0.95      0.95      2729\n",
      "weighted avg       0.95      0.95      0.95      2729\n",
      "\n",
      "======== Fold 5 / 5 ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:58.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.30\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 2 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.17\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 3 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.13\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 4 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.09\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 5 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.05\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 6 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.04\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 7 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.03\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 8 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:55.\n",
      "\n",
      "  Average training loss: 0.02\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 9 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:58.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:55.\n",
      "\n",
      "  Average training loss: 0.02\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 10 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.01\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 11 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:58.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:55.\n",
      "\n",
      "  Average training loss: 0.02\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 12 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.01\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 13 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.01\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 14 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.02\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "Accuracy Score on validation data: 0.9537121906507791\n",
      "Recall value for validation data is : 0.9777571825764597\n",
      "F1 value for validation data is : 0.9543193125282677\n",
      "Confusion Matrix :\n",
      "[[1026   77]\n",
      " [  24 1055]]\n",
      "Report on val data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.93      0.95      1103\n",
      "           1       0.93      0.98      0.95      1079\n",
      "\n",
      "    accuracy                           0.95      2182\n",
      "   macro avg       0.95      0.95      0.95      2182\n",
      "weighted avg       0.95      0.95      0.95      2182\n",
      "\n",
      "Running cumulative Classification Report on test data...\n",
      "Accuracy Score on test data: 0.9516306339318432\n",
      "Recall value for test data is : 0.9611436950146628\n",
      "F1 value for test data is : 0.9520697167755993\n",
      "Confusion Matrix :\n",
      "[[1286   79]\n",
      " [  53 1311]]\n",
      "Report on test data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.94      0.95      1365\n",
      "           1       0.94      0.96      0.95      1364\n",
      "\n",
      "    accuracy                           0.95      2729\n",
      "   macro avg       0.95      0.95      0.95      2729\n",
      "weighted avg       0.95      0.95      0.95      2729\n",
      "\n",
      "Average Confusion Matrix for validation data:\n",
      "[[1018.6   72.6]\n",
      " [  34.2 1057.2]]\n",
      "Average Confusion Matrix for test data:\n",
      "[[1283.4   81.6]\n",
      " [  45.4 1318.6]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8671dbe6bf2547dbacc93545e6663bfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy on test data</td><td></td></tr><tr><td>Accuracy on validation data</td><td></td></tr><tr><td>f1_test</td><td></td></tr><tr><td>f1_validation</td><td></td></tr><tr><td>recall_test</td><td></td></tr><tr><td>recall_validation</td><td></td></tr><tr><td>val_loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy on test data</td><td>0.95346</td></tr><tr><td>Accuracy on validation data</td><td>0.95107</td></tr><tr><td>f1_test</td><td>0.95404</td></tr><tr><td>f1_validation</td><td>0.95189</td></tr><tr><td>recall_test</td><td>0.96672</td></tr><tr><td>recall_validation</td><td>0.96876</td></tr><tr><td>val_loss</td><td>0.45989</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">flowing-sweep-3</strong> at: <a href='https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02/runs/ytrkj5go' target=\"_blank\">https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02/runs/ytrkj5go</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240203_063128-ytrkj5go\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: c4vrx9qv with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 12\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-06\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find BERTcomparison.ipynb.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\AI projects\\Project-BERTcomparison\\wandb\\run-20240203_110757-c4vrx9qv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02/runs/c4vrx9qv' target=\"_blank\">zany-sweep-4</a></strong> to <a href='https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02/sweeps/vpaxo9s7' target=\"_blank\">https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02/sweeps/vpaxo9s7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02' target=\"_blank\">https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02/sweeps/vpaxo9s7' target=\"_blank\">https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02/sweeps/vpaxo9s7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02/runs/c4vrx9qv' target=\"_blank\">https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02/runs/c4vrx9qv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Fold 1 / 5 ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:58.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.69\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 2 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:58.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.57\n",
      "  Training epoch took: 0:03:34\n",
      "\n",
      "\n",
      "======== Epoch 3 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:58.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:55.\n",
      "\n",
      "  Average training loss: 0.50\n",
      "  Training epoch took: 0:03:32\n",
      "\n",
      "\n",
      "======== Epoch 4 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:58.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:55.\n",
      "\n",
      "  Average training loss: 0.45\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 5 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:58.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.41\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 6 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:55.\n",
      "\n",
      "  Average training loss: 0.37\n",
      "  Training epoch took: 0:03:32\n",
      "\n",
      "\n",
      "======== Epoch 7 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:58.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:55.\n",
      "\n",
      "  Average training loss: 0.35\n",
      "  Training epoch took: 0:03:32\n",
      "\n",
      "\n",
      "======== Epoch 8 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.34\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 9 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:58.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:55.\n",
      "\n",
      "  Average training loss: 0.31\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 10 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.30\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 11 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.30\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 12 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.29\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 13 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:55.\n",
      "\n",
      "  Average training loss: 0.28\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 14 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.28\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "Accuracy Score on validation data: 0.8740265689418232\n",
      "Recall value for validation data is : 0.9776536312849162\n",
      "F1 value for validation data is : 0.8842105263157894\n",
      "Confusion Matrix :\n",
      "[[ 858  251]\n",
      " [  24 1050]]\n",
      "Report on val data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.77      0.86      1109\n",
      "           1       0.81      0.98      0.88      1074\n",
      "\n",
      "    accuracy                           0.87      2183\n",
      "   macro avg       0.89      0.88      0.87      2183\n",
      "weighted avg       0.89      0.87      0.87      2183\n",
      "\n",
      "Running cumulative Classification Report on test data...\n",
      "Accuracy Score on test data: 0.8966654452180286\n",
      "Recall value for test data is : 0.9809384164222874\n",
      "F1 value for test data is : 0.9046653144016228\n",
      "Confusion Matrix :\n",
      "[[1109  256]\n",
      " [  26 1338]]\n",
      "Report on test data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.81      0.89      1365\n",
      "           1       0.84      0.98      0.90      1364\n",
      "\n",
      "    accuracy                           0.90      2729\n",
      "   macro avg       0.91      0.90      0.90      2729\n",
      "weighted avg       0.91      0.90      0.90      2729\n",
      "\n",
      "======== Fold 2 / 5 ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.70\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 2 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:58.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.62\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 3 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:58.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:55.\n",
      "\n",
      "  Average training loss: 0.52\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 4 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.47\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 5 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.42\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 6 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:56.\n",
      "\n",
      "  Average training loss: 0.38\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 7 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:03:00.\n",
      "\n",
      "  Average training loss: 0.37\n",
      "  Training epoch took: 0:03:37\n",
      "\n",
      "\n",
      "======== Epoch 8 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:01:00.\n",
      "  Batch   400  of    728.    Elapsed: 0:02:00.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:58.\n",
      "\n",
      "  Average training loss: 0.35\n",
      "  Training epoch took: 0:03:34\n",
      "\n",
      "\n",
      "======== Epoch 9 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:54.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:50.\n",
      "\n",
      "  Average training loss: 0.33\n",
      "  Training epoch took: 0:03:26\n",
      "\n",
      "\n",
      "======== Epoch 10 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:54.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:51.\n",
      "\n",
      "  Average training loss: 0.31\n",
      "  Training epoch took: 0:03:27\n",
      "\n",
      "\n",
      "======== Epoch 11 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:50.\n",
      "\n",
      "  Average training loss: 0.30\n",
      "  Training epoch took: 0:03:26\n",
      "\n",
      "\n",
      "======== Epoch 12 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:50.\n",
      "\n",
      "  Average training loss: 0.28\n",
      "  Training epoch took: 0:03:26\n",
      "\n",
      "\n",
      "======== Epoch 13 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:54.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:50.\n",
      "\n",
      "  Average training loss: 0.29\n",
      "  Training epoch took: 0:03:26\n",
      "\n",
      "\n",
      "======== Epoch 14 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:54.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:50.\n",
      "\n",
      "  Average training loss: 0.28\n",
      "  Training epoch took: 0:03:26\n",
      "\n",
      "Accuracy Score on validation data: 0.9038021071919377\n",
      "Recall value for validation data is : 0.9820143884892086\n",
      "F1 value for validation data is : 0.912280701754386\n",
      "Confusion Matrix :\n",
      "[[ 881  190]\n",
      " [  20 1092]]\n",
      "Report on val data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.82      0.89      1071\n",
      "           1       0.85      0.98      0.91      1112\n",
      "\n",
      "    accuracy                           0.90      2183\n",
      "   macro avg       0.91      0.90      0.90      2183\n",
      "weighted avg       0.91      0.90      0.90      2183\n",
      "\n",
      "Running cumulative Classification Report on test data...\n",
      "Accuracy Score on test data: 0.9032612678636863\n",
      "Recall value for test data is : 0.9882697947214076\n",
      "F1 value for test data is : 0.910810810810811\n",
      "Confusion Matrix :\n",
      "[[1117  248]\n",
      " [  16 1348]]\n",
      "Report on test data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.82      0.89      1365\n",
      "           1       0.84      0.99      0.91      1364\n",
      "\n",
      "    accuracy                           0.90      2729\n",
      "   macro avg       0.92      0.90      0.90      2729\n",
      "weighted avg       0.92      0.90      0.90      2729\n",
      "\n",
      "======== Fold 3 / 5 ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:54.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:51.\n",
      "\n",
      "  Average training loss: 0.71\n",
      "  Training epoch took: 0:03:27\n",
      "\n",
      "\n",
      "======== Epoch 2 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:54.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:50.\n",
      "\n",
      "  Average training loss: 0.66\n",
      "  Training epoch took: 0:03:27\n",
      "\n",
      "\n",
      "======== Epoch 3 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:52.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.54\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 4 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.49\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 5 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.44\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 6 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.41\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 7 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.38\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 8 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.35\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 9 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.34\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 10 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.32\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 11 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.31\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 12 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.30\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 13 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.28\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 14 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.28\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "Accuracy Score on validation data: 0.918460833715071\n",
      "Recall value for validation data is : 0.9616822429906542\n",
      "F1 value for validation data is : 0.9203935599284435\n",
      "Confusion Matrix :\n",
      "[[ 976  137]\n",
      " [  41 1029]]\n",
      "Report on val data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.88      0.92      1113\n",
      "           1       0.88      0.96      0.92      1070\n",
      "\n",
      "    accuracy                           0.92      2183\n",
      "   macro avg       0.92      0.92      0.92      2183\n",
      "weighted avg       0.92      0.92      0.92      2183\n",
      "\n",
      "Running cumulative Classification Report on test data...\n",
      "Accuracy Score on test data: 0.923415170392085\n",
      "Recall value for test data is : 0.9655425219941349\n",
      "F1 value for test data is : 0.9264861062258178\n",
      "Confusion Matrix :\n",
      "[[1203  162]\n",
      " [  47 1317]]\n",
      "Report on test data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.88      0.92      1365\n",
      "           1       0.89      0.97      0.93      1364\n",
      "\n",
      "    accuracy                           0.92      2729\n",
      "   macro avg       0.93      0.92      0.92      2729\n",
      "weighted avg       0.93      0.92      0.92      2729\n",
      "\n",
      "======== Fold 4 / 5 ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:50.\n",
      "\n",
      "  Average training loss: 0.70\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 2 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.62\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 3 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.53\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 4 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.47\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 5 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.43\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 6 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.39\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 7 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.37\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 8 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.35\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 9 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.33\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 10 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.32\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 11 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.30\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 12 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.30\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 13 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.29\n",
      "  Training epoch took: 0:03:26\n",
      "\n",
      "\n",
      "======== Epoch 14 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.28\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "Accuracy Score on validation data: 0.8932172318973419\n",
      "Recall value for validation data is : 0.9901960784313726\n",
      "F1 value for validation data is : 0.9050916496945011\n",
      "Confusion Matrix :\n",
      "[[ 838  222]\n",
      " [  11 1111]]\n",
      "Report on val data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.79      0.88      1060\n",
      "           1       0.83      0.99      0.91      1122\n",
      "\n",
      "    accuracy                           0.89      2182\n",
      "   macro avg       0.91      0.89      0.89      2182\n",
      "weighted avg       0.91      0.89      0.89      2182\n",
      "\n",
      "Running cumulative Classification Report on test data...\n",
      "Accuracy Score on test data: 0.8933675338951997\n",
      "Recall value for test data is : 0.9860703812316716\n",
      "F1 value for test data is : 0.9023817510902382\n",
      "Confusion Matrix :\n",
      "[[1093  272]\n",
      " [  19 1345]]\n",
      "Report on test data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.80      0.88      1365\n",
      "           1       0.83      0.99      0.90      1364\n",
      "\n",
      "    accuracy                           0.89      2729\n",
      "   macro avg       0.91      0.89      0.89      2729\n",
      "weighted avg       0.91      0.89      0.89      2729\n",
      "\n",
      "======== Fold 5 / 5 ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.70\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 2 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.61\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 3 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.52\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 4 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.47\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 5 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.44\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 6 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.41\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 7 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.37\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 8 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.35\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 9 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.33\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 10 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.32\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 11 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.31\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 12 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.29\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 13 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.28\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 14 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.28\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "Accuracy Score on validation data: 0.8854262144821264\n",
      "Recall value for validation data is : 0.9833178869323448\n",
      "F1 value for validation data is : 0.8946037099494099\n",
      "Confusion Matrix :\n",
      "[[ 871  232]\n",
      " [  18 1061]]\n",
      "Report on val data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.79      0.87      1103\n",
      "           1       0.82      0.98      0.89      1079\n",
      "\n",
      "    accuracy                           0.89      2182\n",
      "   macro avg       0.90      0.89      0.88      2182\n",
      "weighted avg       0.90      0.89      0.88      2182\n",
      "\n",
      "Running cumulative Classification Report on test data...\n",
      "Accuracy Score on test data: 0.8977647489923049\n",
      "Recall value for test data is : 0.9846041055718475\n",
      "F1 value for test data is : 0.9059021922428331\n",
      "Confusion Matrix :\n",
      "[[1107  258]\n",
      " [  21 1343]]\n",
      "Report on test data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.81      0.89      1365\n",
      "           1       0.84      0.98      0.91      1364\n",
      "\n",
      "    accuracy                           0.90      2729\n",
      "   macro avg       0.91      0.90      0.90      2729\n",
      "weighted avg       0.91      0.90      0.90      2729\n",
      "\n",
      "Average Confusion Matrix for validation data:\n",
      "[[ 884.8  206.4]\n",
      " [  22.8 1068.6]]\n",
      "Average Confusion Matrix for test data:\n",
      "[[1125.8  239.2]\n",
      " [  25.8 1338.2]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy on test data</td><td></td></tr><tr><td>Accuracy on validation data</td><td></td></tr><tr><td>f1_test</td><td></td></tr><tr><td>f1_validation</td><td></td></tr><tr><td>recall_test</td><td></td></tr><tr><td>recall_validation</td><td></td></tr><tr><td>val_loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy on test data</td><td>0.90289</td></tr><tr><td>Accuracy on validation data</td><td>0.89499</td></tr><tr><td>f1_test</td><td>0.91005</td></tr><tr><td>f1_validation</td><td>0.90332</td></tr><tr><td>recall_test</td><td>0.98109</td></tr><tr><td>recall_validation</td><td>0.97897</td></tr><tr><td>val_loss</td><td>0.35107</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">zany-sweep-4</strong> at: <a href='https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02/runs/c4vrx9qv' target=\"_blank\">https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02/runs/c4vrx9qv</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240203_110757-c4vrx9qv\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: in6xo22f with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 12\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-06\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find BERTcomparison.ipynb.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\AI projects\\Project-BERTcomparison\\wandb\\run-20240203_153537-in6xo22f</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02/runs/in6xo22f' target=\"_blank\">skilled-sweep-5</a></strong> to <a href='https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02/sweeps/vpaxo9s7' target=\"_blank\">https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02/sweeps/vpaxo9s7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02' target=\"_blank\">https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02/sweeps/vpaxo9s7' target=\"_blank\">https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02/sweeps/vpaxo9s7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02/runs/in6xo22f' target=\"_blank\">https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02/runs/in6xo22f</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Fold 1 / 5 ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.31\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 2 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:50.\n",
      "\n",
      "  Average training loss: 0.17\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 3 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.12\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 4 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.09\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 5 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.05\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 6 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.04\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 7 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.02\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 8 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:50.\n",
      "\n",
      "  Average training loss: 0.02\n",
      "  Training epoch took: 0:03:26\n",
      "\n",
      "\n",
      "======== Epoch 9 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.01\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 10 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.02\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 11 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.01\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 12 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.01\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 13 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.01\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 14 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.01\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "Accuracy Score on validation data: 0.9496106275767293\n",
      "Recall value for validation data is : 0.952513966480447\n",
      "F1 value for validation data is : 0.9489795918367346\n",
      "Confusion Matrix :\n",
      "[[1050   59]\n",
      " [  51 1023]]\n",
      "Report on val data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.95      0.95      1109\n",
      "           1       0.95      0.95      0.95      1074\n",
      "\n",
      "    accuracy                           0.95      2183\n",
      "   macro avg       0.95      0.95      0.95      2183\n",
      "weighted avg       0.95      0.95      0.95      2183\n",
      "\n",
      "Running cumulative Classification Report on test data...\n",
      "Accuracy Score on test data: 0.9593257603517772\n",
      "Recall value for test data is : 0.9626099706744868\n",
      "F1 value for test data is : 0.9594446474241871\n",
      "Confusion Matrix :\n",
      "[[1305   60]\n",
      " [  51 1313]]\n",
      "Report on test data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96      1365\n",
      "           1       0.96      0.96      0.96      1364\n",
      "\n",
      "    accuracy                           0.96      2729\n",
      "   macro avg       0.96      0.96      0.96      2729\n",
      "weighted avg       0.96      0.96      0.96      2729\n",
      "\n",
      "======== Fold 2 / 5 ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.31\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 2 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:50.\n",
      "\n",
      "  Average training loss: 0.17\n",
      "  Training epoch took: 0:03:26\n",
      "\n",
      "\n",
      "======== Epoch 3 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.14\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 4 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.08\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 5 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.06\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 6 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.04\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 7 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.03\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 8 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.02\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 9 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.02\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 10 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.02\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 11 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.01\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 12 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.01\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 13 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.01\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 14 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.01\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "Accuracy Score on validation data: 0.9519010535959689\n",
      "Recall value for validation data is : 0.9550359712230215\n",
      "F1 value for validation data is : 0.9528936742934051\n",
      "Confusion Matrix :\n",
      "[[1016   55]\n",
      " [  50 1062]]\n",
      "Report on val data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.95      0.95      1071\n",
      "           1       0.95      0.96      0.95      1112\n",
      "\n",
      "    accuracy                           0.95      2183\n",
      "   macro avg       0.95      0.95      0.95      2183\n",
      "weighted avg       0.95      0.95      0.95      2183\n",
      "\n",
      "Running cumulative Classification Report on test data...\n",
      "Accuracy Score on test data: 0.952363503114694\n",
      "Recall value for test data is : 0.9516129032258065\n",
      "F1 value for test data is : 0.9523110785033014\n",
      "Confusion Matrix :\n",
      "[[1301   64]\n",
      " [  66 1298]]\n",
      "Report on test data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.95      0.95      1365\n",
      "           1       0.95      0.95      0.95      1364\n",
      "\n",
      "    accuracy                           0.95      2729\n",
      "   macro avg       0.95      0.95      0.95      2729\n",
      "weighted avg       0.95      0.95      0.95      2729\n",
      "\n",
      "======== Fold 3 / 5 ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.31\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 2 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.18\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 3 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.13\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 4 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.10\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 5 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.06\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 6 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.04\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 7 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.03\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 8 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.02\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 9 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.01\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 10 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.03\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 11 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.01\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 12 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.02\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 13 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.02\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 14 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.01\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "Accuracy Score on validation data: 0.9523591387998168\n",
      "Recall value for validation data is : 0.9700934579439252\n",
      "F1 value for validation data is : 0.9522935779816514\n",
      "Confusion Matrix :\n",
      "[[1041   72]\n",
      " [  32 1038]]\n",
      "Report on val data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.94      0.95      1113\n",
      "           1       0.94      0.97      0.95      1070\n",
      "\n",
      "    accuracy                           0.95      2183\n",
      "   macro avg       0.95      0.95      0.95      2183\n",
      "weighted avg       0.95      0.95      0.95      2183\n",
      "\n",
      "Running cumulative Classification Report on test data...\n",
      "Accuracy Score on test data: 0.9530963722975448\n",
      "Recall value for test data is : 0.9743401759530792\n",
      "F1 value for test data is : 0.9540559942569994\n",
      "Confusion Matrix :\n",
      "[[1272   93]\n",
      " [  35 1329]]\n",
      "Report on test data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.93      0.95      1365\n",
      "           1       0.93      0.97      0.95      1364\n",
      "\n",
      "    accuracy                           0.95      2729\n",
      "   macro avg       0.95      0.95      0.95      2729\n",
      "weighted avg       0.95      0.95      0.95      2729\n",
      "\n",
      "======== Fold 4 / 5 ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.30\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 2 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.17\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 3 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.12\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 4 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.09\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 5 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.06\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 6 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.04\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 7 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.03\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 8 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.02\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 9 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.02\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 10 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.02\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 11 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.02\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 12 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.01\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 13 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.01\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 14 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.01\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "Accuracy Score on validation data: 0.9518790100824931\n",
      "Recall value for validation data is : 0.9795008912655971\n",
      "F1 value for validation data is : 0.9544072948328268\n",
      "Confusion Matrix :\n",
      "[[ 978   82]\n",
      " [  23 1099]]\n",
      "Report on val data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.92      0.95      1060\n",
      "           1       0.93      0.98      0.95      1122\n",
      "\n",
      "    accuracy                           0.95      2182\n",
      "   macro avg       0.95      0.95      0.95      2182\n",
      "weighted avg       0.95      0.95      0.95      2182\n",
      "\n",
      "Running cumulative Classification Report on test data...\n",
      "Accuracy Score on test data: 0.9494320263832906\n",
      "Recall value for test data is : 0.9802052785923754\n",
      "F1 value for test data is : 0.9509246088193457\n",
      "Confusion Matrix :\n",
      "[[1254  111]\n",
      " [  27 1337]]\n",
      "Report on test data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.92      0.95      1365\n",
      "           1       0.92      0.98      0.95      1364\n",
      "\n",
      "    accuracy                           0.95      2729\n",
      "   macro avg       0.95      0.95      0.95      2729\n",
      "weighted avg       0.95      0.95      0.95      2729\n",
      "\n",
      "======== Fold 5 / 5 ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.31\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 2 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:50.\n",
      "\n",
      "  Average training loss: 0.18\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 3 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:50.\n",
      "\n",
      "  Average training loss: 0.13\n",
      "  Training epoch took: 0:03:26\n",
      "\n",
      "\n",
      "======== Epoch 4 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.09\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 5 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.06\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 6 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:50.\n",
      "\n",
      "  Average training loss: 0.04\n",
      "  Training epoch took: 0:03:26\n",
      "\n",
      "\n",
      "======== Epoch 7 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.03\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 8 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.02\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 9 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.02\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 10 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.02\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 11 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.02\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 12 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.02\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 13 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:50.\n",
      "\n",
      "  Average training loss: 0.02\n",
      "  Training epoch took: 0:03:26\n",
      "\n",
      "\n",
      "======== Epoch 14 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.01\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "Accuracy Score on validation data: 0.9541704857928506\n",
      "Recall value for validation data is : 0.9814643188137164\n",
      "F1 value for validation data is : 0.9549143372407575\n",
      "Confusion Matrix :\n",
      "[[1023   80]\n",
      " [  20 1059]]\n",
      "Report on val data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.93      0.95      1103\n",
      "           1       0.93      0.98      0.95      1079\n",
      "\n",
      "    accuracy                           0.95      2182\n",
      "   macro avg       0.96      0.95      0.95      2182\n",
      "weighted avg       0.96      0.95      0.95      2182\n",
      "\n",
      "Running cumulative Classification Report on test data...\n",
      "Accuracy Score on test data: 0.949798460974716\n",
      "Recall value for test data is : 0.969208211143695\n",
      "F1 value for test data is : 0.9507371449119023\n",
      "Confusion Matrix :\n",
      "[[1270   95]\n",
      " [  42 1322]]\n",
      "Report on test data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.93      0.95      1365\n",
      "           1       0.93      0.97      0.95      1364\n",
      "\n",
      "    accuracy                           0.95      2729\n",
      "   macro avg       0.95      0.95      0.95      2729\n",
      "weighted avg       0.95      0.95      0.95      2729\n",
      "\n",
      "Average Confusion Matrix for validation data:\n",
      "[[1021.6   69.6]\n",
      " [  35.2 1056.2]]\n",
      "Average Confusion Matrix for test data:\n",
      "[[1280.4   84.6]\n",
      " [  44.2 1319.8]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy on test data</td><td></td></tr><tr><td>Accuracy on validation data</td><td></td></tr><tr><td>f1_test</td><td></td></tr><tr><td>f1_validation</td><td></td></tr><tr><td>recall_test</td><td></td></tr><tr><td>recall_validation</td><td></td></tr><tr><td>val_loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy on test data</td><td>0.9528</td></tr><tr><td>Accuracy on validation data</td><td>0.95198</td></tr><tr><td>f1_test</td><td>0.95349</td></tr><tr><td>f1_validation</td><td>0.9527</td></tr><tr><td>recall_test</td><td>0.9676</td></tr><tr><td>recall_validation</td><td>0.96772</td></tr><tr><td>val_loss</td><td>0.47006</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">skilled-sweep-5</strong> at: <a href='https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02/runs/in6xo22f' target=\"_blank\">https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02/runs/in6xo22f</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240203_153537-in6xo22f\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: b7oly0d6 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 12\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-06\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find BERTcomparison.ipynb.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\AI projects\\Project-BERTcomparison\\wandb\\run-20240203_195911-b7oly0d6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02/runs/b7oly0d6' target=\"_blank\">divine-sweep-6</a></strong> to <a href='https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02/sweeps/vpaxo9s7' target=\"_blank\">https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02/sweeps/vpaxo9s7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02' target=\"_blank\">https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02/sweeps/vpaxo9s7' target=\"_blank\">https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02/sweeps/vpaxo9s7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02/runs/b7oly0d6' target=\"_blank\">https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02/runs/b7oly0d6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Fold 1 / 5 ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.57\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 2 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.37\n",
      "  Training epoch took: 0:03:26\n",
      "\n",
      "\n",
      "======== Epoch 3 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.31\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 4 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:50.\n",
      "\n",
      "  Average training loss: 0.28\n",
      "  Training epoch took: 0:03:26\n",
      "\n",
      "\n",
      "======== Epoch 5 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.27\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 6 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.25\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 7 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.24\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 8 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.23\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 9 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:50.\n",
      "\n",
      "  Average training loss: 0.22\n",
      "  Training epoch took: 0:03:26\n",
      "\n",
      "\n",
      "======== Epoch 10 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:50.\n",
      "\n",
      "  Average training loss: 0.21\n",
      "  Training epoch took: 0:03:26\n",
      "\n",
      "\n",
      "======== Epoch 11 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.21\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 12 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:50.\n",
      "\n",
      "  Average training loss: 0.20\n",
      "  Training epoch took: 0:03:26\n",
      "\n",
      "\n",
      "======== Epoch 13 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.19\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 14 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.19\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "Accuracy Score on validation data: 0.9262482821804856\n",
      "Recall value for validation data is : 0.9767225325884544\n",
      "F1 value for validation data is : 0.9287295263390881\n",
      "Confusion Matrix :\n",
      "[[ 973  136]\n",
      " [  25 1049]]\n",
      "Report on val data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.88      0.92      1109\n",
      "           1       0.89      0.98      0.93      1074\n",
      "\n",
      "    accuracy                           0.93      2183\n",
      "   macro avg       0.93      0.93      0.93      2183\n",
      "weighted avg       0.93      0.93      0.93      2183\n",
      "\n",
      "Running cumulative Classification Report on test data...\n",
      "Accuracy Score on test data: 0.9377061194576768\n",
      "Recall value for test data is : 0.9802052785923754\n",
      "F1 value for test data is : 0.940225035161744\n",
      "Confusion Matrix :\n",
      "[[1222  143]\n",
      " [  27 1337]]\n",
      "Report on test data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.90      0.93      1365\n",
      "           1       0.90      0.98      0.94      1364\n",
      "\n",
      "    accuracy                           0.94      2729\n",
      "   macro avg       0.94      0.94      0.94      2729\n",
      "weighted avg       0.94      0.94      0.94      2729\n",
      "\n",
      "======== Fold 2 / 5 ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.57\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 2 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:50.\n",
      "\n",
      "  Average training loss: 0.36\n",
      "  Training epoch took: 0:03:26\n",
      "\n",
      "\n",
      "======== Epoch 3 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.28\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 4 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.26\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 5 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.25\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 6 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.24\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 7 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.22\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 8 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.22\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 9 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.22\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 10 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.21\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 11 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.21\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 12 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.18\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 13 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.19\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 14 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.18\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "Accuracy Score on validation data: 0.9413650939074668\n",
      "Recall value for validation data is : 0.966726618705036\n",
      "F1 value for validation data is : 0.9438103599648814\n",
      "Confusion Matrix :\n",
      "[[ 980   91]\n",
      " [  37 1075]]\n",
      "Report on val data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.92      0.94      1071\n",
      "           1       0.92      0.97      0.94      1112\n",
      "\n",
      "    accuracy                           0.94      2183\n",
      "   macro avg       0.94      0.94      0.94      2183\n",
      "weighted avg       0.94      0.94      0.94      2183\n",
      "\n",
      "Running cumulative Classification Report on test data...\n",
      "Accuracy Score on test data: 0.9432026383290583\n",
      "Recall value for test data is : 0.967008797653959\n",
      "F1 value for test data is : 0.9445041174364484\n",
      "Confusion Matrix :\n",
      "[[1255  110]\n",
      " [  45 1319]]\n",
      "Report on test data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.92      0.94      1365\n",
      "           1       0.92      0.97      0.94      1364\n",
      "\n",
      "    accuracy                           0.94      2729\n",
      "   macro avg       0.94      0.94      0.94      2729\n",
      "weighted avg       0.94      0.94      0.94      2729\n",
      "\n",
      "======== Fold 3 / 5 ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.56\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 2 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.37\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 3 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:50.\n",
      "\n",
      "  Average training loss: 0.30\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 4 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.27\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 5 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:50.\n",
      "\n",
      "  Average training loss: 0.25\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 6 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.24\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 7 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.23\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 8 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.23\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 9 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.22\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 10 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.21\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 11 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.20\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 12 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.20\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 13 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.19\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 14 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.18\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "Accuracy Score on validation data: 0.9308291342189647\n",
      "Recall value for validation data is : 0.9700934579439252\n",
      "F1 value for validation data is : 0.9321957790749889\n",
      "Confusion Matrix :\n",
      "[[ 994  119]\n",
      " [  32 1038]]\n",
      "Report on val data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.89      0.93      1113\n",
      "           1       0.90      0.97      0.93      1070\n",
      "\n",
      "    accuracy                           0.93      2183\n",
      "   macro avg       0.93      0.93      0.93      2183\n",
      "weighted avg       0.93      0.93      0.93      2183\n",
      "\n",
      "Running cumulative Classification Report on test data...\n",
      "Accuracy Score on test data: 0.9347746427262733\n",
      "Recall value for test data is : 0.9750733137829912\n",
      "F1 value for test data is : 0.9372797744890768\n",
      "Confusion Matrix :\n",
      "[[1221  144]\n",
      " [  34 1330]]\n",
      "Report on test data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.89      0.93      1365\n",
      "           1       0.90      0.98      0.94      1364\n",
      "\n",
      "    accuracy                           0.93      2729\n",
      "   macro avg       0.94      0.93      0.93      2729\n",
      "weighted avg       0.94      0.93      0.93      2729\n",
      "\n",
      "======== Fold 4 / 5 ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.56\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 2 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.37\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 3 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.30\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 4 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.28\n",
      "  Training epoch took: 0:03:26\n",
      "\n",
      "\n",
      "======== Epoch 5 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.26\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 6 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.25\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 7 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:50.\n",
      "\n",
      "  Average training loss: 0.24\n",
      "  Training epoch took: 0:03:26\n",
      "\n",
      "\n",
      "======== Epoch 8 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.23\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 9 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.22\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 10 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.20\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 11 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.21\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 12 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.19\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 13 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.19\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 14 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.18\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "Accuracy Score on validation data: 0.9459211732355637\n",
      "Recall value for validation data is : 0.9848484848484849\n",
      "F1 value for validation data is : 0.9493127147766323\n",
      "Confusion Matrix :\n",
      "[[ 959  101]\n",
      " [  17 1105]]\n",
      "Report on val data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.90      0.94      1060\n",
      "           1       0.92      0.98      0.95      1122\n",
      "\n",
      "    accuracy                           0.95      2182\n",
      "   macro avg       0.95      0.94      0.95      2182\n",
      "weighted avg       0.95      0.95      0.95      2182\n",
      "\n",
      "Running cumulative Classification Report on test data...\n",
      "Accuracy Score on test data: 0.9454012458776109\n",
      "Recall value for test data is : 0.9809384164222874\n",
      "F1 value for test data is : 0.9472566371681417\n",
      "Confusion Matrix :\n",
      "[[1242  123]\n",
      " [  26 1338]]\n",
      "Report on test data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.91      0.94      1365\n",
      "           1       0.92      0.98      0.95      1364\n",
      "\n",
      "    accuracy                           0.95      2729\n",
      "   macro avg       0.95      0.95      0.95      2729\n",
      "weighted avg       0.95      0.95      0.95      2729\n",
      "\n",
      "======== Fold 5 / 5 ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.56\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 2 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.37\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 3 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.31\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 4 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.27\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 5 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.26\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 6 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.25\n",
      "  Training epoch took: 0:03:26\n",
      "\n",
      "\n",
      "======== Epoch 7 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.23\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 8 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:50.\n",
      "\n",
      "  Average training loss: 0.23\n",
      "  Training epoch took: 0:03:26\n",
      "\n",
      "\n",
      "======== Epoch 9 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.22\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 10 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.21\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 11 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:50.\n",
      "\n",
      "  Average training loss: 0.20\n",
      "  Training epoch took: 0:03:26\n",
      "\n",
      "\n",
      "======== Epoch 12 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.20\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 13 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.19\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 14 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.17\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "Accuracy Score on validation data: 0.9385884509624198\n",
      "Recall value for validation data is : 0.9759036144578314\n",
      "F1 value for validation data is : 0.9401785714285714\n",
      "Confusion Matrix :\n",
      "[[ 995  108]\n",
      " [  26 1053]]\n",
      "Report on val data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.90      0.94      1103\n",
      "           1       0.91      0.98      0.94      1079\n",
      "\n",
      "    accuracy                           0.94      2182\n",
      "   macro avg       0.94      0.94      0.94      2182\n",
      "weighted avg       0.94      0.94      0.94      2182\n",
      "\n",
      "Running cumulative Classification Report on test data...\n",
      "Accuracy Score on test data: 0.9410040307805057\n",
      "Recall value for test data is : 0.9750733137829912\n",
      "F1 value for test data is : 0.9429280397022333\n",
      "Confusion Matrix :\n",
      "[[1238  127]\n",
      " [  34 1330]]\n",
      "Report on test data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.91      0.94      1365\n",
      "           1       0.91      0.98      0.94      1364\n",
      "\n",
      "    accuracy                           0.94      2729\n",
      "   macro avg       0.94      0.94      0.94      2729\n",
      "weighted avg       0.94      0.94      0.94      2729\n",
      "\n",
      "Average Confusion Matrix for validation data:\n",
      "[[ 980.2  111. ]\n",
      " [  27.4 1064. ]]\n",
      "Average Confusion Matrix for test data:\n",
      "[[1235.6  129.4]\n",
      " [  33.2 1330.8]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ee47b688ef540088cbb7313db392bf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy on test data</td><td></td></tr><tr><td>Accuracy on validation data</td><td></td></tr><tr><td>f1_test</td><td></td></tr><tr><td>f1_validation</td><td></td></tr><tr><td>recall_test</td><td></td></tr><tr><td>recall_validation</td><td></td></tr><tr><td>val_loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy on test data</td><td>0.94042</td></tr><tr><td>Accuracy on validation data</td><td>0.93659</td></tr><tr><td>f1_test</td><td>0.94244</td></tr><tr><td>f1_validation</td><td>0.93885</td></tr><tr><td>recall_test</td><td>0.97566</td></tr><tr><td>recall_validation</td><td>0.97486</td></tr><tr><td>val_loss</td><td>0.31375</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">divine-sweep-6</strong> at: <a href='https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02/runs/b7oly0d6' target=\"_blank\">https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02/runs/b7oly0d6</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240203_195911-b7oly0d6\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: pmjxddqo with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 12\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-06\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find BERTcomparison.ipynb.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\AI projects\\Project-BERTcomparison\\wandb\\run-20240204_002250-pmjxddqo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02/runs/pmjxddqo' target=\"_blank\">fearless-sweep-7</a></strong> to <a href='https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02/sweeps/vpaxo9s7' target=\"_blank\">https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02/sweeps/vpaxo9s7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02' target=\"_blank\">https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02/sweeps/vpaxo9s7' target=\"_blank\">https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02/sweeps/vpaxo9s7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02/runs/pmjxddqo' target=\"_blank\">https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02/runs/pmjxddqo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Fold 1 / 5 ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.69\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 2 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.57\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 3 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.50\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 4 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.45\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 5 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.42\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 6 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.38\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 7 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:50.\n",
      "\n",
      "  Average training loss: 0.35\n",
      "  Training epoch took: 0:03:26\n",
      "\n",
      "\n",
      "======== Epoch 8 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.33\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 9 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:50.\n",
      "\n",
      "  Average training loss: 0.32\n",
      "  Training epoch took: 0:03:26\n",
      "\n",
      "\n",
      "======== Epoch 10 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.30\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 11 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.29\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 12 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.29\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 13 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.27\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 14 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:50.\n",
      "\n",
      "  Average training loss: 0.28\n",
      "  Training epoch took: 0:03:26\n",
      "\n",
      "Accuracy Score on validation data: 0.8447091158955565\n",
      "Recall value for validation data is : 0.9897579143389199\n",
      "F1 value for validation data is : 0.862474645030426\n",
      "Confusion Matrix :\n",
      "[[ 781  328]\n",
      " [  11 1063]]\n",
      "Report on val data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.70      0.82      1109\n",
      "           1       0.76      0.99      0.86      1074\n",
      "\n",
      "    accuracy                           0.84      2183\n",
      "   macro avg       0.88      0.85      0.84      2183\n",
      "weighted avg       0.88      0.84      0.84      2183\n",
      "\n",
      "Running cumulative Classification Report on test data...\n",
      "Accuracy Score on test data: 0.8779772810553316\n",
      "Recall value for test data is : 0.9970674486803519\n",
      "F1 value for test data is : 0.8909269570913855\n",
      "Confusion Matrix :\n",
      "[[1036  329]\n",
      " [   4 1360]]\n",
      "Report on test data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.76      0.86      1365\n",
      "           1       0.81      1.00      0.89      1364\n",
      "\n",
      "    accuracy                           0.88      2729\n",
      "   macro avg       0.90      0.88      0.88      2729\n",
      "weighted avg       0.90      0.88      0.88      2729\n",
      "\n",
      "======== Fold 2 / 5 ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.70\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 2 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.62\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 3 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.53\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 4 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.47\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 5 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.42\n",
      "  Training epoch took: 0:03:25\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 6 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n",
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.40\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 7 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.36\n",
      "  Training epoch took: 0:03:26\n",
      "\n",
      "\n",
      "======== Epoch 8 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.35\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 9 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.33\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 10 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.32\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 11 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.30\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 12 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:50.\n",
      "\n",
      "  Average training loss: 0.30\n",
      "  Training epoch took: 0:03:26\n",
      "\n",
      "\n",
      "======== Epoch 13 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.29\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 14 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.28\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "Accuracy Score on validation data: 0.912963811268896\n",
      "Recall value for validation data is : 0.9613309352517986\n",
      "F1 value for validation data is : 0.9183848797250859\n",
      "Confusion Matrix :\n",
      "[[ 924  147]\n",
      " [  43 1069]]\n",
      "Report on val data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.86      0.91      1071\n",
      "           1       0.88      0.96      0.92      1112\n",
      "\n",
      "    accuracy                           0.91      2183\n",
      "   macro avg       0.92      0.91      0.91      2183\n",
      "weighted avg       0.92      0.91      0.91      2183\n",
      "\n",
      "Running cumulative Classification Report on test data...\n",
      "Accuracy Score on test data: 0.9149871747893001\n",
      "Recall value for test data is : 0.966275659824047\n",
      "F1 value for test data is : 0.9191073919107391\n",
      "Confusion Matrix :\n",
      "[[1179  186]\n",
      " [  46 1318]]\n",
      "Report on test data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.86      0.91      1365\n",
      "           1       0.88      0.97      0.92      1364\n",
      "\n",
      "    accuracy                           0.91      2729\n",
      "   macro avg       0.92      0.92      0.91      2729\n",
      "weighted avg       0.92      0.91      0.91      2729\n",
      "\n",
      "======== Fold 3 / 5 ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:50.\n",
      "\n",
      "  Average training loss: 0.69\n",
      "  Training epoch took: 0:03:26\n",
      "\n",
      "\n",
      "======== Epoch 2 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.59\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 3 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.51\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 4 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:50.\n",
      "\n",
      "  Average training loss: 0.46\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 5 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:50.\n",
      "\n",
      "  Average training loss: 0.43\n",
      "  Training epoch took: 0:03:26\n",
      "\n",
      "\n",
      "======== Epoch 6 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:50.\n",
      "\n",
      "  Average training loss: 0.39\n",
      "  Training epoch took: 0:03:26\n",
      "\n",
      "\n",
      "======== Epoch 7 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.37\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 8 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.34\n",
      "  Training epoch took: 0:03:26\n",
      "\n",
      "\n",
      "======== Epoch 9 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.33\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 10 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:50.\n",
      "\n",
      "  Average training loss: 0.31\n",
      "  Training epoch took: 0:03:26\n",
      "\n",
      "\n",
      "======== Epoch 11 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:50.\n",
      "\n",
      "  Average training loss: 0.30\n",
      "  Training epoch took: 0:03:26\n",
      "\n",
      "\n",
      "======== Epoch 12 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.29\n",
      "  Training epoch took: 0:03:26\n",
      "\n",
      "\n",
      "======== Epoch 13 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.28\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 14 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:50.\n",
      "\n",
      "  Average training loss: 0.28\n",
      "  Training epoch took: 0:03:26\n",
      "\n",
      "Accuracy Score on validation data: 0.9028859367842419\n",
      "Recall value for validation data is : 0.9775700934579439\n",
      "F1 value for validation data is : 0.9079861111111112\n",
      "Confusion Matrix :\n",
      "[[ 925  188]\n",
      " [  24 1046]]\n",
      "Report on val data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.83      0.90      1113\n",
      "           1       0.85      0.98      0.91      1070\n",
      "\n",
      "    accuracy                           0.90      2183\n",
      "   macro avg       0.91      0.90      0.90      2183\n",
      "weighted avg       0.91      0.90      0.90      2183\n",
      "\n",
      "Running cumulative Classification Report on test data...\n",
      "Accuracy Score on test data: 0.9116892634664713\n",
      "Recall value for test data is : 0.9824046920821115\n",
      "F1 value for test data is : 0.9174940089010613\n",
      "Confusion Matrix :\n",
      "[[1148  217]\n",
      " [  24 1340]]\n",
      "Report on test data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.84      0.91      1365\n",
      "           1       0.86      0.98      0.92      1364\n",
      "\n",
      "    accuracy                           0.91      2729\n",
      "   macro avg       0.92      0.91      0.91      2729\n",
      "weighted avg       0.92      0.91      0.91      2729\n",
      "\n",
      "======== Fold 4 / 5 ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.71\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 2 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:50.\n",
      "\n",
      "  Average training loss: 0.65\n",
      "  Training epoch took: 0:03:26\n",
      "\n",
      "\n",
      "======== Epoch 3 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:50.\n",
      "\n",
      "  Average training loss: 0.54\n",
      "  Training epoch took: 0:03:26\n",
      "\n",
      "\n",
      "======== Epoch 4 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.48\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 5 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.43\n",
      "  Training epoch took: 0:03:26\n",
      "\n",
      "\n",
      "======== Epoch 6 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.40\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 7 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.37\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 8 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.36\n",
      "  Training epoch took: 0:03:26\n",
      "\n",
      "\n",
      "======== Epoch 9 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.34\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 10 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.32\n",
      "  Training epoch took: 0:03:26\n",
      "\n",
      "\n",
      "======== Epoch 11 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.31\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 12 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.30\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 13 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:50.\n",
      "\n",
      "  Average training loss: 0.29\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 14 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.28\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "Accuracy Score on validation data: 0.8987167736021998\n",
      "Recall value for validation data is : 0.9893048128342246\n",
      "F1 value for validation data is : 0.9094633346988938\n",
      "Confusion Matrix :\n",
      "[[ 851  209]\n",
      " [  12 1110]]\n",
      "Report on val data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.80      0.89      1060\n",
      "           1       0.84      0.99      0.91      1122\n",
      "\n",
      "    accuracy                           0.90      2182\n",
      "   macro avg       0.91      0.90      0.90      2182\n",
      "weighted avg       0.91      0.90      0.90      2182\n",
      "\n",
      "Running cumulative Classification Report on test data...\n",
      "Accuracy Score on test data: 0.9014290949065592\n",
      "Recall value for test data is : 0.9853372434017595\n",
      "F1 value for test data is : 0.9090294217111938\n",
      "Confusion Matrix :\n",
      "[[1116  249]\n",
      " [  20 1344]]\n",
      "Report on test data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.82      0.89      1365\n",
      "           1       0.84      0.99      0.91      1364\n",
      "\n",
      "    accuracy                           0.90      2729\n",
      "   macro avg       0.91      0.90      0.90      2729\n",
      "weighted avg       0.91      0.90      0.90      2729\n",
      "\n",
      "======== Fold 5 / 5 ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.70\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 2 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:50.\n",
      "\n",
      "  Average training loss: 0.62\n",
      "  Training epoch took: 0:03:26\n",
      "\n",
      "\n",
      "======== Epoch 3 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:50.\n",
      "\n",
      "  Average training loss: 0.52\n",
      "  Training epoch took: 0:03:26\n",
      "\n",
      "\n",
      "======== Epoch 4 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.47\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 5 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.42\n",
      "  Training epoch took: 0:03:26\n",
      "\n",
      "\n",
      "======== Epoch 6 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.38\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 7 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:50.\n",
      "\n",
      "  Average training loss: 0.37\n",
      "  Training epoch took: 0:03:26\n",
      "\n",
      "\n",
      "======== Epoch 8 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:50.\n",
      "\n",
      "  Average training loss: 0.34\n",
      "  Training epoch took: 0:03:26\n",
      "\n",
      "\n",
      "======== Epoch 9 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.33\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 10 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:50.\n",
      "\n",
      "  Average training loss: 0.31\n",
      "  Training epoch took: 0:03:26\n",
      "\n",
      "\n",
      "======== Epoch 11 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:50.\n",
      "\n",
      "  Average training loss: 0.31\n",
      "  Training epoch took: 0:03:26\n",
      "\n",
      "\n",
      "======== Epoch 12 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:50.\n",
      "\n",
      "  Average training loss: 0.30\n",
      "  Training epoch took: 0:03:26\n",
      "\n",
      "\n",
      "======== Epoch 13 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.28\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 14 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.27\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "Accuracy Score on validation data: 0.8803849679193401\n",
      "Recall value for validation data is : 0.989805375347544\n",
      "F1 value for validation data is : 0.8911138923654568\n",
      "Confusion Matrix :\n",
      "[[ 853  250]\n",
      " [  11 1068]]\n",
      "Report on val data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.77      0.87      1103\n",
      "           1       0.81      0.99      0.89      1079\n",
      "\n",
      "    accuracy                           0.88      2182\n",
      "   macro avg       0.90      0.88      0.88      2182\n",
      "weighted avg       0.90      0.88      0.88      2182\n",
      "\n",
      "Running cumulative Classification Report on test data...\n",
      "Accuracy Score on test data: 0.8886038842066691\n",
      "Recall value for test data is : 0.9912023460410557\n",
      "F1 value for test data is : 0.898936170212766\n",
      "Confusion Matrix :\n",
      "[[1073  292]\n",
      " [  12 1352]]\n",
      "Report on test data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.79      0.88      1365\n",
      "           1       0.82      0.99      0.90      1364\n",
      "\n",
      "    accuracy                           0.89      2729\n",
      "   macro avg       0.91      0.89      0.89      2729\n",
      "weighted avg       0.91      0.89      0.89      2729\n",
      "\n",
      "Average Confusion Matrix for validation data:\n",
      "[[ 866.8  224.4]\n",
      " [  20.2 1071.2]]\n",
      "Average Confusion Matrix for test data:\n",
      "[[1110.4  254.6]\n",
      " [  21.2 1342.8]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7870a48416a243a4bcb77875d1bb2dae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy on test data</td><td></td></tr><tr><td>Accuracy on validation data</td><td></td></tr><tr><td>f1_test</td><td></td></tr><tr><td>f1_validation</td><td></td></tr><tr><td>recall_test</td><td></td></tr><tr><td>recall_validation</td><td></td></tr><tr><td>val_loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy on test data</td><td>0.89894</td></tr><tr><td>Accuracy on validation data</td><td>0.88793</td></tr><tr><td>f1_test</td><td>0.9071</td></tr><tr><td>f1_validation</td><td>0.89788</td></tr><tr><td>recall_test</td><td>0.98446</td></tr><tr><td>recall_validation</td><td>0.98155</td></tr><tr><td>val_loss</td><td>0.37767</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fearless-sweep-7</strong> at: <a href='https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02/runs/pmjxddqo' target=\"_blank\">https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02/runs/pmjxddqo</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240204_002250-pmjxddqo\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ujov6bkl with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 12\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-06\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find BERTcomparison.ipynb.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\AI projects\\Project-BERTcomparison\\wandb\\run-20240204_044639-ujov6bkl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02/runs/ujov6bkl' target=\"_blank\">major-sweep-8</a></strong> to <a href='https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02/sweeps/vpaxo9s7' target=\"_blank\">https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02/sweeps/vpaxo9s7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02' target=\"_blank\">https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02/sweeps/vpaxo9s7' target=\"_blank\">https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02/sweeps/vpaxo9s7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02/runs/ujov6bkl' target=\"_blank\">https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02/runs/ujov6bkl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Fold 1 / 5 ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.57\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 2 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:50.\n",
      "\n",
      "  Average training loss: 0.37\n",
      "  Training epoch took: 0:03:26\n",
      "\n",
      "\n",
      "======== Epoch 3 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.31\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 4 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.27\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 5 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.25\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 6 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.24\n",
      "  Training epoch took: 0:03:26\n",
      "\n",
      "\n",
      "======== Epoch 7 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.24\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 8 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.22\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 9 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.23\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 10 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.21\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 11 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.21\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 12 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.20\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 13 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.18\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 14 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.19\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "Accuracy Score on validation data: 0.9175446633073752\n",
      "Recall value for validation data is : 0.9823091247672253\n",
      "F1 value for validation data is : 0.9213973799126638\n",
      "Confusion Matrix :\n",
      "[[ 948  161]\n",
      " [  19 1055]]\n",
      "Report on val data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.85      0.91      1109\n",
      "           1       0.87      0.98      0.92      1074\n",
      "\n",
      "    accuracy                           0.92      2183\n",
      "   macro avg       0.92      0.92      0.92      2183\n",
      "weighted avg       0.92      0.92      0.92      2183\n",
      "\n",
      "Running cumulative Classification Report on test data...\n",
      "Accuracy Score on test data: 0.9366068156834005\n",
      "Recall value for test data is : 0.9890029325513197\n",
      "F1 value for test data is : 0.9397422500870777\n",
      "Confusion Matrix :\n",
      "[[1207  158]\n",
      " [  15 1349]]\n",
      "Report on test data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.88      0.93      1365\n",
      "           1       0.90      0.99      0.94      1364\n",
      "\n",
      "    accuracy                           0.94      2729\n",
      "   macro avg       0.94      0.94      0.94      2729\n",
      "weighted avg       0.94      0.94      0.94      2729\n",
      "\n",
      "======== Fold 2 / 5 ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.54\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 2 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.36\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 3 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:50.\n",
      "\n",
      "  Average training loss: 0.30\n",
      "  Training epoch took: 0:03:26\n",
      "\n",
      "\n",
      "======== Epoch 4 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.27\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 5 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.27\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 6 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.25\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 7 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:50.\n",
      "\n",
      "  Average training loss: 0.23\n",
      "  Training epoch took: 0:03:26\n",
      "\n",
      "\n",
      "======== Epoch 8 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.23\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 9 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.22\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 10 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.22\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 11 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.19\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 12 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.20\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 13 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.19\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 14 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.18\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "Accuracy Score on validation data: 0.9326614750343564\n",
      "Recall value for validation data is : 0.9847122302158273\n",
      "F1 value for validation data is : 0.9370988446726571\n",
      "Confusion Matrix :\n",
      "[[ 941  130]\n",
      " [  17 1095]]\n",
      "Report on val data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.88      0.93      1071\n",
      "           1       0.89      0.98      0.94      1112\n",
      "\n",
      "    accuracy                           0.93      2183\n",
      "   macro avg       0.94      0.93      0.93      2183\n",
      "weighted avg       0.94      0.93      0.93      2183\n",
      "\n",
      "Running cumulative Classification Report on test data...\n",
      "Accuracy Score on test data: 0.9366068156834005\n",
      "Recall value for test data is : 0.9890029325513197\n",
      "F1 value for test data is : 0.9397422500870777\n",
      "Confusion Matrix :\n",
      "[[1207  158]\n",
      " [  15 1349]]\n",
      "Report on test data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.88      0.93      1365\n",
      "           1       0.90      0.99      0.94      1364\n",
      "\n",
      "    accuracy                           0.94      2729\n",
      "   macro avg       0.94      0.94      0.94      2729\n",
      "weighted avg       0.94      0.94      0.94      2729\n",
      "\n",
      "======== Fold 3 / 5 ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.54\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 2 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.35\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 3 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:50.\n",
      "\n",
      "  Average training loss: 0.30\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 4 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.28\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 5 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.27\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 6 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.26\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 7 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.24\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 8 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:50.\n",
      "\n",
      "  Average training loss: 0.23\n",
      "  Training epoch took: 0:03:26\n",
      "\n",
      "\n",
      "======== Epoch 9 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.23\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 10 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.21\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 11 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:50.\n",
      "\n",
      "  Average training loss: 0.21\n",
      "  Training epoch took: 0:03:26\n",
      "\n",
      "\n",
      "======== Epoch 12 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.20\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 13 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.18\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 14 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.20\n",
      "  Training epoch took: 0:03:26\n",
      "\n",
      "Accuracy Score on validation data: 0.9280806229958772\n",
      "Recall value for validation data is : 0.9719626168224299\n",
      "F1 value for validation data is : 0.9298167188198481\n",
      "Confusion Matrix :\n",
      "[[ 986  127]\n",
      " [  30 1040]]\n",
      "Report on val data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.89      0.93      1113\n",
      "           1       0.89      0.97      0.93      1070\n",
      "\n",
      "    accuracy                           0.93      2183\n",
      "   macro avg       0.93      0.93      0.93      2183\n",
      "weighted avg       0.93      0.93      0.93      2183\n",
      "\n",
      "Running cumulative Classification Report on test data...\n",
      "Accuracy Score on test data: 0.9336753389519971\n",
      "Recall value for test data is : 0.9772727272727273\n",
      "F1 value for test data is : 0.93642430628732\n",
      "Confusion Matrix :\n",
      "[[1215  150]\n",
      " [  31 1333]]\n",
      "Report on test data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.89      0.93      1365\n",
      "           1       0.90      0.98      0.94      1364\n",
      "\n",
      "    accuracy                           0.93      2729\n",
      "   macro avg       0.94      0.93      0.93      2729\n",
      "weighted avg       0.94      0.93      0.93      2729\n",
      "\n",
      "======== Fold 4 / 5 ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.55\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 2 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:50.\n",
      "\n",
      "  Average training loss: 0.37\n",
      "  Training epoch took: 0:03:26\n",
      "\n",
      "\n",
      "======== Epoch 3 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:50.\n",
      "\n",
      "  Average training loss: 0.30\n",
      "  Training epoch took: 0:03:26\n",
      "\n",
      "\n",
      "======== Epoch 4 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.28\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 5 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:50.\n",
      "\n",
      "  Average training loss: 0.26\n",
      "  Training epoch took: 0:03:26\n",
      "\n",
      "\n",
      "======== Epoch 6 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:50.\n",
      "\n",
      "  Average training loss: 0.25\n",
      "  Training epoch took: 0:03:26\n",
      "\n",
      "\n",
      "======== Epoch 7 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:50.\n",
      "\n",
      "  Average training loss: 0.23\n",
      "  Training epoch took: 0:03:26\n",
      "\n",
      "\n",
      "======== Epoch 8 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:50.\n",
      "\n",
      "  Average training loss: 0.22\n",
      "  Training epoch took: 0:03:26\n",
      "\n",
      "\n",
      "======== Epoch 9 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.22\n",
      "  Training epoch took: 0:03:26\n",
      "\n",
      "\n",
      "======== Epoch 10 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.21\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 11 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:50.\n",
      "\n",
      "  Average training loss: 0.21\n",
      "  Training epoch took: 0:03:26\n",
      "\n",
      "\n",
      "======== Epoch 12 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:50.\n",
      "\n",
      "  Average training loss: 0.20\n",
      "  Training epoch took: 0:03:26\n",
      "\n",
      "\n",
      "======== Epoch 13 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:50.\n",
      "\n",
      "  Average training loss: 0.20\n",
      "  Training epoch took: 0:03:26\n",
      "\n",
      "\n",
      "======== Epoch 14 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:50.\n",
      "\n",
      "  Average training loss: 0.19\n",
      "  Training epoch took: 0:03:26\n",
      "\n",
      "Accuracy Score on validation data: 0.9450045829514208\n",
      "Recall value for validation data is : 0.9750445632798574\n",
      "F1 value for validation data is : 0.9480069324090121\n",
      "Confusion Matrix :\n",
      "[[ 968   92]\n",
      " [  28 1094]]\n",
      "Report on val data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.91      0.94      1060\n",
      "           1       0.92      0.98      0.95      1122\n",
      "\n",
      "    accuracy                           0.95      2182\n",
      "   macro avg       0.95      0.94      0.94      2182\n",
      "weighted avg       0.95      0.95      0.94      2182\n",
      "\n",
      "Running cumulative Classification Report on test data...\n",
      "Accuracy Score on test data: 0.9439355075119091\n",
      "Recall value for test data is : 0.9728739002932552\n",
      "F1 value for test data is : 0.9454934093338084\n",
      "Confusion Matrix :\n",
      "[[1249  116]\n",
      " [  37 1327]]\n",
      "Report on test data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.92      0.94      1365\n",
      "           1       0.92      0.97      0.95      1364\n",
      "\n",
      "    accuracy                           0.94      2729\n",
      "   macro avg       0.95      0.94      0.94      2729\n",
      "weighted avg       0.95      0.94      0.94      2729\n",
      "\n",
      "======== Fold 5 / 5 ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.56\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 2 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:50.\n",
      "\n",
      "  Average training loss: 0.36\n",
      "  Training epoch took: 0:03:26\n",
      "\n",
      "\n",
      "======== Epoch 3 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.31\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 4 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:50.\n",
      "\n",
      "  Average training loss: 0.27\n",
      "  Training epoch took: 0:03:26\n",
      "\n",
      "\n",
      "======== Epoch 5 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.26\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 6 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.26\n",
      "  Training epoch took: 0:03:26\n",
      "\n",
      "\n",
      "======== Epoch 7 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.23\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 8 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:50.\n",
      "\n",
      "  Average training loss: 0.23\n",
      "  Training epoch took: 0:03:26\n",
      "\n",
      "\n",
      "======== Epoch 9 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:50.\n",
      "\n",
      "  Average training loss: 0.22\n",
      "  Training epoch took: 0:03:26\n",
      "\n",
      "\n",
      "======== Epoch 10 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:50.\n",
      "\n",
      "  Average training loss: 0.21\n",
      "  Training epoch took: 0:03:26\n",
      "\n",
      "\n",
      "======== Epoch 11 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.20\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 12 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.19\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 13 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.19\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 14 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.18\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "Accuracy Score on validation data: 0.9427131072410633\n",
      "Recall value for validation data is : 0.9814643188137164\n",
      "F1 value for validation data is : 0.9442710655372268\n",
      "Confusion Matrix :\n",
      "[[ 998  105]\n",
      " [  20 1059]]\n",
      "Report on val data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.90      0.94      1103\n",
      "           1       0.91      0.98      0.94      1079\n",
      "\n",
      "    accuracy                           0.94      2182\n",
      "   macro avg       0.95      0.94      0.94      2182\n",
      "weighted avg       0.95      0.94      0.94      2182\n",
      "\n",
      "Running cumulative Classification Report on test data...\n",
      "Accuracy Score on test data: 0.9435690729204836\n",
      "Recall value for test data is : 0.9780058651026393\n",
      "F1 value for test data is : 0.9454287739192062\n",
      "Confusion Matrix :\n",
      "[[1241  124]\n",
      " [  30 1334]]\n",
      "Report on test data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.91      0.94      1365\n",
      "           1       0.91      0.98      0.95      1364\n",
      "\n",
      "    accuracy                           0.94      2729\n",
      "   macro avg       0.95      0.94      0.94      2729\n",
      "weighted avg       0.95      0.94      0.94      2729\n",
      "\n",
      "Average Confusion Matrix for validation data:\n",
      "[[ 968.2  123. ]\n",
      " [  22.8 1068.6]]\n",
      "Average Confusion Matrix for test data:\n",
      "[[1223.8  141.2]\n",
      " [  25.6 1338.4]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcb36cc658a84c569b3a8541c316500b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.066 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.016773"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy on test data</td><td></td></tr><tr><td>Accuracy on validation data</td><td></td></tr><tr><td>f1_test</td><td></td></tr><tr><td>f1_validation</td><td></td></tr><tr><td>recall_test</td><td></td></tr><tr><td>recall_validation</td><td></td></tr><tr><td>val_loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy on test data</td><td>0.93888</td></tr><tr><td>Accuracy on validation data</td><td>0.9332</td></tr><tr><td>f1_test</td><td>0.94137</td></tr><tr><td>f1_validation</td><td>0.93612</td></tr><tr><td>recall_test</td><td>0.98123</td></tr><tr><td>recall_validation</td><td>0.9791</td></tr><tr><td>val_loss</td><td>0.3312</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">major-sweep-8</strong> at: <a href='https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02/runs/ujov6bkl' target=\"_blank\">https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02/runs/ujov6bkl</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240204_044639-ujov6bkl\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: wblrjvnd with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 12\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-06\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find BERTcomparison.ipynb.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\AI projects\\Project-BERTcomparison\\wandb\\run-20240204_091033-wblrjvnd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02/runs/wblrjvnd' target=\"_blank\">warm-sweep-9</a></strong> to <a href='https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02/sweeps/vpaxo9s7' target=\"_blank\">https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02/sweeps/vpaxo9s7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02' target=\"_blank\">https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02/sweeps/vpaxo9s7' target=\"_blank\">https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02/sweeps/vpaxo9s7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02/runs/wblrjvnd' target=\"_blank\">https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02/runs/wblrjvnd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Fold 1 / 5 ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.56\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 2 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:57.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:50.\n",
      "\n",
      "  Average training loss: 0.35\n",
      "  Training epoch took: 0:03:26\n",
      "\n",
      "\n",
      "======== Epoch 3 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:49.\n",
      "\n",
      "  Average training loss: 0.29\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 4 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:50.\n",
      "\n",
      "  Average training loss: 0.27\n",
      "  Training epoch took: 0:03:25\n",
      "\n",
      "\n",
      "======== Epoch 5 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:56.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:53.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:55.\n",
      "\n",
      "  Average training loss: 0.26\n",
      "  Training epoch took: 0:03:31\n",
      "\n",
      "\n",
      "======== Epoch 6 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:01:02.\n",
      "  Batch   400  of    728.    Elapsed: 0:02:04.\n",
      "  Batch   600  of    728.    Elapsed: 0:03:06.\n",
      "\n",
      "  Average training loss: 0.25\n",
      "  Training epoch took: 0:03:46\n",
      "\n",
      "\n",
      "======== Epoch 7 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:01:02.\n",
      "  Batch   400  of    728.    Elapsed: 0:02:05.\n",
      "  Batch   600  of    728.    Elapsed: 0:03:10.\n",
      "\n",
      "  Average training loss: 0.23\n",
      "  Training epoch took: 0:03:54\n",
      "\n",
      "\n",
      "======== Epoch 8 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:01:09.\n",
      "  Batch   400  of    728.    Elapsed: 0:02:17.\n",
      "  Batch   600  of    728.    Elapsed: 0:03:25.\n",
      "\n",
      "  Average training loss: 0.22\n",
      "  Training epoch took: 0:04:09\n",
      "\n",
      "\n",
      "======== Epoch 9 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:01:09.\n",
      "  Batch   400  of    728.    Elapsed: 0:02:17.\n",
      "  Batch   600  of    728.    Elapsed: 0:03:25.\n",
      "\n",
      "  Average training loss: 0.22\n",
      "  Training epoch took: 0:04:09\n",
      "\n",
      "\n",
      "======== Epoch 10 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:01:09.\n",
      "  Batch   400  of    728.    Elapsed: 0:02:17.\n",
      "  Batch   600  of    728.    Elapsed: 0:03:26.\n",
      "\n",
      "  Average training loss: 0.22\n",
      "  Training epoch took: 0:04:09\n",
      "\n",
      "\n",
      "======== Epoch 11 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:01:09.\n",
      "  Batch   400  of    728.    Elapsed: 0:02:17.\n",
      "  Batch   600  of    728.    Elapsed: 0:03:26.\n",
      "\n",
      "  Average training loss: 0.21\n",
      "  Training epoch took: 0:04:10\n",
      "\n",
      "\n",
      "======== Epoch 12 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:01:08.\n",
      "  Batch   400  of    728.    Elapsed: 0:02:17.\n",
      "  Batch   600  of    728.    Elapsed: 0:03:26.\n",
      "\n",
      "  Average training loss: 0.20\n",
      "  Training epoch took: 0:04:09\n",
      "\n",
      "\n",
      "======== Epoch 13 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:01:08.\n",
      "  Batch   400  of    728.    Elapsed: 0:02:16.\n",
      "  Batch   600  of    728.    Elapsed: 0:03:24.\n",
      "\n",
      "  Average training loss: 0.19\n",
      "  Training epoch took: 0:04:08\n",
      "\n",
      "\n",
      "======== Epoch 14 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:01:08.\n",
      "  Batch   400  of    728.    Elapsed: 0:02:16.\n",
      "  Batch   600  of    728.    Elapsed: 0:03:24.\n",
      "\n",
      "  Average training loss: 0.18\n",
      "  Training epoch took: 0:04:08\n",
      "\n",
      "Accuracy Score on validation data: 0.9042601923957856\n",
      "Recall value for validation data is : 0.9823091247672253\n",
      "F1 value for validation data is : 0.9098749460974559\n",
      "Confusion Matrix :\n",
      "[[ 919  190]\n",
      " [  19 1055]]\n",
      "Report on val data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.83      0.90      1109\n",
      "           1       0.85      0.98      0.91      1074\n",
      "\n",
      "    accuracy                           0.90      2183\n",
      "   macro avg       0.91      0.91      0.90      2183\n",
      "weighted avg       0.91      0.90      0.90      2183\n",
      "\n",
      "Running cumulative Classification Report on test data...\n",
      "Accuracy Score on test data: 0.9237816049835105\n",
      "Recall value for test data is : 0.9919354838709677\n",
      "F1 value for test data is : 0.9286204529855869\n",
      "Confusion Matrix :\n",
      "[[1168  197]\n",
      " [  11 1353]]\n",
      "Report on test data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.86      0.92      1365\n",
      "           1       0.87      0.99      0.93      1364\n",
      "\n",
      "    accuracy                           0.92      2729\n",
      "   macro avg       0.93      0.92      0.92      2729\n",
      "weighted avg       0.93      0.92      0.92      2729\n",
      "\n",
      "======== Fold 2 / 5 ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:01:08.\n",
      "  Batch   400  of    728.    Elapsed: 0:02:16.\n",
      "  Batch   600  of    728.    Elapsed: 0:03:24.\n",
      "\n",
      "  Average training loss: 0.56\n",
      "  Training epoch took: 0:04:08\n",
      "\n",
      "\n",
      "======== Epoch 2 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:01:08.\n",
      "  Batch   400  of    728.    Elapsed: 0:02:16.\n",
      "  Batch   600  of    728.    Elapsed: 0:03:25.\n",
      "\n",
      "  Average training loss: 0.36\n",
      "  Training epoch took: 0:04:08\n",
      "\n",
      "\n",
      "======== Epoch 3 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:01:08.\n",
      "  Batch   400  of    728.    Elapsed: 0:02:16.\n",
      "  Batch   600  of    728.    Elapsed: 0:03:24.\n",
      "\n",
      "  Average training loss: 0.29\n",
      "  Training epoch took: 0:04:08\n",
      "\n",
      "\n",
      "======== Epoch 4 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:01:08.\n",
      "  Batch   400  of    728.    Elapsed: 0:02:16.\n",
      "  Batch   600  of    728.    Elapsed: 0:03:25.\n",
      "\n",
      "  Average training loss: 0.28\n",
      "  Training epoch took: 0:04:08\n",
      "\n",
      "\n",
      "======== Epoch 5 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:01:08.\n",
      "  Batch   400  of    728.    Elapsed: 0:02:16.\n",
      "  Batch   600  of    728.    Elapsed: 0:03:25.\n",
      "\n",
      "  Average training loss: 0.26\n",
      "  Training epoch took: 0:04:08\n",
      "\n",
      "\n",
      "======== Epoch 6 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:01:08.\n",
      "  Batch   400  of    728.    Elapsed: 0:02:16.\n",
      "  Batch   600  of    728.    Elapsed: 0:03:24.\n",
      "\n",
      "  Average training loss: 0.25\n",
      "  Training epoch took: 0:04:08\n",
      "\n",
      "\n",
      "======== Epoch 7 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:01:08.\n",
      "  Batch   400  of    728.    Elapsed: 0:02:16.\n",
      "  Batch   600  of    728.    Elapsed: 0:03:25.\n",
      "\n",
      "  Average training loss: 0.23\n",
      "  Training epoch took: 0:04:08\n",
      "\n",
      "\n",
      "======== Epoch 8 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:01:08.\n",
      "  Batch   400  of    728.    Elapsed: 0:02:16.\n",
      "  Batch   600  of    728.    Elapsed: 0:03:25.\n",
      "\n",
      "  Average training loss: 0.23\n",
      "  Training epoch took: 0:04:08\n",
      "\n",
      "\n",
      "======== Epoch 9 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:01:08.\n",
      "  Batch   400  of    728.    Elapsed: 0:02:17.\n",
      "  Batch   600  of    728.    Elapsed: 0:03:25.\n",
      "\n",
      "  Average training loss: 0.21\n",
      "  Training epoch took: 0:04:08\n",
      "\n",
      "\n",
      "======== Epoch 10 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:01:08.\n",
      "  Batch   400  of    728.    Elapsed: 0:02:16.\n",
      "  Batch   600  of    728.    Elapsed: 0:03:25.\n",
      "\n",
      "  Average training loss: 0.21\n",
      "  Training epoch took: 0:04:08\n",
      "\n",
      "\n",
      "======== Epoch 11 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:01:08.\n",
      "  Batch   400  of    728.    Elapsed: 0:02:16.\n",
      "  Batch   600  of    728.    Elapsed: 0:03:25.\n",
      "\n",
      "  Average training loss: 0.21\n",
      "  Training epoch took: 0:04:08\n",
      "\n",
      "\n",
      "======== Epoch 12 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:01:08.\n",
      "  Batch   400  of    728.    Elapsed: 0:02:16.\n",
      "  Batch   600  of    728.    Elapsed: 0:03:25.\n",
      "\n",
      "  Average training loss: 0.19\n",
      "  Training epoch took: 0:04:08\n",
      "\n",
      "\n",
      "======== Epoch 13 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:01:08.\n",
      "  Batch   400  of    728.    Elapsed: 0:02:16.\n",
      "  Batch   600  of    728.    Elapsed: 0:03:25.\n",
      "\n",
      "  Average training loss: 0.19\n",
      "  Training epoch took: 0:04:08\n",
      "\n",
      "\n",
      "======== Epoch 14 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:01:08.\n",
      "  Batch   400  of    728.    Elapsed: 0:02:17.\n",
      "  Batch   600  of    728.    Elapsed: 0:03:25.\n",
      "\n",
      "  Average training loss: 0.19\n",
      "  Training epoch took: 0:04:08\n",
      "\n",
      "Accuracy Score on validation data: 0.9358680714612918\n",
      "Recall value for validation data is : 0.97931654676259\n",
      "F1 value for validation data is : 0.9396031061259706\n",
      "Confusion Matrix :\n",
      "[[ 954  117]\n",
      " [  23 1089]]\n",
      "Report on val data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.89      0.93      1071\n",
      "           1       0.90      0.98      0.94      1112\n",
      "\n",
      "    accuracy                           0.94      2183\n",
      "   macro avg       0.94      0.94      0.94      2183\n",
      "weighted avg       0.94      0.94      0.94      2183\n",
      "\n",
      "Running cumulative Classification Report on test data...\n",
      "Accuracy Score on test data: 0.9377061194576768\n",
      "Recall value for test data is : 0.9816715542521994\n",
      "F1 value for test data is : 0.9403089887640449\n",
      "Confusion Matrix :\n",
      "[[1220  145]\n",
      " [  25 1339]]\n",
      "Report on test data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.89      0.93      1365\n",
      "           1       0.90      0.98      0.94      1364\n",
      "\n",
      "    accuracy                           0.94      2729\n",
      "   macro avg       0.94      0.94      0.94      2729\n",
      "weighted avg       0.94      0.94      0.94      2729\n",
      "\n",
      "======== Fold 3 / 5 ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:01:08.\n",
      "  Batch   400  of    728.    Elapsed: 0:02:16.\n",
      "  Batch   600  of    728.    Elapsed: 0:03:24.\n",
      "\n",
      "  Average training loss: 0.56\n",
      "  Training epoch took: 0:04:08\n",
      "\n",
      "\n",
      "======== Epoch 2 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:01:08.\n",
      "  Batch   400  of    728.    Elapsed: 0:02:16.\n",
      "  Batch   600  of    728.    Elapsed: 0:03:25.\n",
      "\n",
      "  Average training loss: 0.35\n",
      "  Training epoch took: 0:04:08\n",
      "\n",
      "\n",
      "======== Epoch 3 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:01:08.\n",
      "  Batch   400  of    728.    Elapsed: 0:02:17.\n",
      "  Batch   600  of    728.    Elapsed: 0:03:25.\n",
      "\n",
      "  Average training loss: 0.30\n",
      "  Training epoch took: 0:04:08\n",
      "\n",
      "\n",
      "======== Epoch 4 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:01:08.\n",
      "  Batch   400  of    728.    Elapsed: 0:02:16.\n",
      "  Batch   600  of    728.    Elapsed: 0:03:25.\n",
      "\n",
      "  Average training loss: 0.27\n",
      "  Training epoch took: 0:04:08\n",
      "\n",
      "\n",
      "======== Epoch 5 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:01:08.\n",
      "  Batch   400  of    728.    Elapsed: 0:02:17.\n",
      "  Batch   600  of    728.    Elapsed: 0:03:25.\n",
      "\n",
      "  Average training loss: 0.25\n",
      "  Training epoch took: 0:04:08\n",
      "\n",
      "\n",
      "======== Epoch 6 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:01:08.\n",
      "  Batch   400  of    728.    Elapsed: 0:02:17.\n",
      "  Batch   600  of    728.    Elapsed: 0:03:25.\n",
      "\n",
      "  Average training loss: 0.24\n",
      "  Training epoch took: 0:04:08\n",
      "\n",
      "\n",
      "======== Epoch 7 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:01:08.\n",
      "  Batch   400  of    728.    Elapsed: 0:02:16.\n",
      "  Batch   600  of    728.    Elapsed: 0:03:25.\n",
      "\n",
      "  Average training loss: 0.23\n",
      "  Training epoch took: 0:04:08\n",
      "\n",
      "\n",
      "======== Epoch 8 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:01:08.\n",
      "  Batch   400  of    728.    Elapsed: 0:02:16.\n",
      "  Batch   600  of    728.    Elapsed: 0:03:25.\n",
      "\n",
      "  Average training loss: 0.22\n",
      "  Training epoch took: 0:04:08\n",
      "\n",
      "\n",
      "======== Epoch 9 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:01:08.\n",
      "  Batch   400  of    728.    Elapsed: 0:02:17.\n",
      "  Batch   600  of    728.    Elapsed: 0:03:25.\n",
      "\n",
      "  Average training loss: 0.21\n",
      "  Training epoch took: 0:04:08\n",
      "\n",
      "\n",
      "======== Epoch 10 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:01:08.\n",
      "  Batch   400  of    728.    Elapsed: 0:02:17.\n",
      "  Batch   600  of    728.    Elapsed: 0:03:25.\n",
      "\n",
      "  Average training loss: 0.21\n",
      "  Training epoch took: 0:04:08\n",
      "\n",
      "\n",
      "======== Epoch 11 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:01:08.\n",
      "  Batch   400  of    728.    Elapsed: 0:02:16.\n",
      "  Batch   600  of    728.    Elapsed: 0:03:25.\n",
      "\n",
      "  Average training loss: 0.19\n",
      "  Training epoch took: 0:04:08\n",
      "\n",
      "\n",
      "======== Epoch 12 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:01:08.\n",
      "  Batch   400  of    728.    Elapsed: 0:02:16.\n",
      "  Batch   600  of    728.    Elapsed: 0:03:25.\n",
      "\n",
      "  Average training loss: 0.20\n",
      "  Training epoch took: 0:04:12\n",
      "\n",
      "\n",
      "======== Epoch 13 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:01:13.\n",
      "  Batch   400  of    728.    Elapsed: 0:02:27.\n",
      "  Batch   600  of    728.    Elapsed: 0:03:40.\n",
      "\n",
      "  Average training loss: 0.19\n",
      "  Training epoch took: 0:04:21\n",
      "\n",
      "\n",
      "======== Epoch 14 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:01:02.\n",
      "  Batch   400  of    728.    Elapsed: 0:02:03.\n",
      "  Batch   600  of    728.    Elapsed: 0:03:05.\n",
      "\n",
      "  Average training loss: 0.18\n",
      "  Training epoch took: 0:03:44\n",
      "\n",
      "Accuracy Score on validation data: 0.9312872194228127\n",
      "Recall value for validation data is : 0.9766355140186916\n",
      "F1 value for validation data is : 0.9330357142857143\n",
      "Confusion Matrix :\n",
      "[[ 988  125]\n",
      " [  25 1045]]\n",
      "Report on val data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.89      0.93      1113\n",
      "           1       0.89      0.98      0.93      1070\n",
      "\n",
      "    accuracy                           0.93      2183\n",
      "   macro avg       0.93      0.93      0.93      2183\n",
      "weighted avg       0.94      0.93      0.93      2183\n",
      "\n",
      "Running cumulative Classification Report on test data...\n",
      "Accuracy Score on test data: 0.9347746427262733\n",
      "Recall value for test data is : 0.9787390029325513\n",
      "F1 value for test data is : 0.9374999999999999\n",
      "Confusion Matrix :\n",
      "[[1216  149]\n",
      " [  29 1335]]\n",
      "Report on test data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.89      0.93      1365\n",
      "           1       0.90      0.98      0.94      1364\n",
      "\n",
      "    accuracy                           0.93      2729\n",
      "   macro avg       0.94      0.93      0.93      2729\n",
      "weighted avg       0.94      0.93      0.93      2729\n",
      "\n",
      "======== Fold 4 / 5 ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:01:02.\n",
      "  Batch   400  of    728.    Elapsed: 0:02:04.\n",
      "  Batch   600  of    728.    Elapsed: 0:03:06.\n",
      "\n",
      "  Average training loss: 0.56\n",
      "  Training epoch took: 0:03:46\n",
      "\n",
      "\n",
      "======== Epoch 2 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:01:02.\n",
      "  Batch   400  of    728.    Elapsed: 0:02:04.\n",
      "  Batch   600  of    728.    Elapsed: 0:03:05.\n",
      "\n",
      "  Average training loss: 0.37\n",
      "  Training epoch took: 0:03:45\n",
      "\n",
      "\n",
      "======== Epoch 3 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:01:02.\n",
      "  Batch   400  of    728.    Elapsed: 0:02:04.\n",
      "  Batch   600  of    728.    Elapsed: 0:03:06.\n",
      "\n",
      "  Average training loss: 0.31\n",
      "  Training epoch took: 0:03:45\n",
      "\n",
      "\n",
      "======== Epoch 4 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:01:02.\n",
      "  Batch   400  of    728.    Elapsed: 0:02:04.\n",
      "  Batch   600  of    728.    Elapsed: 0:03:06.\n",
      "\n",
      "  Average training loss: 0.27\n",
      "  Training epoch took: 0:03:45\n",
      "\n",
      "\n",
      "======== Epoch 5 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:01:02.\n",
      "  Batch   400  of    728.    Elapsed: 0:02:04.\n",
      "  Batch   600  of    728.    Elapsed: 0:03:05.\n",
      "\n",
      "  Average training loss: 0.25\n",
      "  Training epoch took: 0:03:45\n",
      "\n",
      "\n",
      "======== Epoch 6 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:01:02.\n",
      "  Batch   400  of    728.    Elapsed: 0:02:03.\n",
      "  Batch   600  of    728.    Elapsed: 0:03:05.\n",
      "\n",
      "  Average training loss: 0.24\n",
      "  Training epoch took: 0:03:45\n",
      "\n",
      "\n",
      "======== Epoch 7 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:55.\n",
      "\n",
      "  Average training loss: 0.22\n",
      "  Training epoch took: 0:03:32\n",
      "\n",
      "\n",
      "======== Epoch 8 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:58.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:55.\n",
      "\n",
      "  Average training loss: 0.23\n",
      "  Training epoch took: 0:03:32\n",
      "\n",
      "\n",
      "======== Epoch 9 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:58.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:55.\n",
      "\n",
      "  Average training loss: 0.22\n",
      "  Training epoch took: 0:03:32\n",
      "\n",
      "\n",
      "======== Epoch 10 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:58.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:55.\n",
      "\n",
      "  Average training loss: 0.21\n",
      "  Training epoch took: 0:03:32\n",
      "\n",
      "\n",
      "======== Epoch 11 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:58.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:55.\n",
      "\n",
      "  Average training loss: 0.21\n",
      "  Training epoch took: 0:03:32\n",
      "\n",
      "\n",
      "======== Epoch 12 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:55.\n",
      "\n",
      "  Average training loss: 0.20\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 13 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:55.\n",
      "\n",
      "  Average training loss: 0.20\n",
      "  Training epoch took: 0:03:32\n",
      "\n",
      "\n",
      "======== Epoch 14 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:58.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:55.\n",
      "\n",
      "  Average training loss: 0.17\n",
      "  Training epoch took: 0:03:32\n",
      "\n",
      "Accuracy Score on validation data: 0.9408799266727773\n",
      "Recall value for validation data is : 0.9759358288770054\n",
      "F1 value for validation data is : 0.944372574385511\n",
      "Confusion Matrix :\n",
      "[[ 958  102]\n",
      " [  27 1095]]\n",
      "Report on val data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.90      0.94      1060\n",
      "           1       0.91      0.98      0.94      1122\n",
      "\n",
      "    accuracy                           0.94      2182\n",
      "   macro avg       0.94      0.94      0.94      2182\n",
      "weighted avg       0.94      0.94      0.94      2182\n",
      "\n",
      "Running cumulative Classification Report on test data...\n",
      "Accuracy Score on test data: 0.9432026383290583\n",
      "Recall value for test data is : 0.9772727272727273\n",
      "F1 value for test data is : 0.945054945054945\n",
      "Confusion Matrix :\n",
      "[[1241  124]\n",
      " [  31 1333]]\n",
      "Report on test data: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.91      0.94      1365\n",
      "           1       0.91      0.98      0.95      1364\n",
      "\n",
      "    accuracy                           0.94      2729\n",
      "   macro avg       0.95      0.94      0.94      2729\n",
      "weighted avg       0.95      0.94      0.94      2729\n",
      "\n",
      "======== Fold 5 / 5 ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:58.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:57.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:55.\n",
      "\n",
      "  Average training loss: 0.55\n",
      "  Training epoch took: 0:03:33\n",
      "\n",
      "\n",
      "======== Epoch 2 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:02:03.\n",
      "  Batch   600  of    728.    Elapsed: 0:03:05.\n",
      "\n",
      "  Average training loss: 0.36\n",
      "  Training epoch took: 0:03:43\n",
      "\n",
      "\n",
      "======== Epoch 3 / 14 ========\n",
      "Training...\n",
      "learning_rate = 5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    728.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    728.    Elapsed: 0:01:59.\n",
      "  Batch   600  of    728.    Elapsed: 0:02:59.\n",
      "\n",
      "  Average training loss: 0.30\n",
      "  Training epoch took: 0:03:37\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Ctrl-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57df3aad5f3c410f8e949a710266b942",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">warm-sweep-9</strong> at: <a href='https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02/runs/wblrjvnd' target=\"_blank\">https://wandb.ai/jon-l-collins/allenaiscibert_scivocab_uncased_2024-02-02/runs/wblrjvnd</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240204_091033-wblrjvnd\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.agent(sweep_id, train_folds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bestpytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
