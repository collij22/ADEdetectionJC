{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file variables\n",
    "\n",
    "#pretrained_bert = 'allenai/scibert_scivocab_uncased'\n",
    "#pretrained_bert = 'dmis-lab/biobert-base-cased-v1.2'\n",
    "pretrained_bert = 'cimm-kzn/endr-bert'\n",
    "#pretrained_bert = 'SpanBERT/spanbert-base-cased'\n",
    "#pretrained_bert = 'emilyalsentzer/Bio_ClinicalBERT'\n",
    "#pretrained_bert = 'dmis-lab/biobert-v1.1'\n",
    "#pretrained_bert = 'allenai/biomed_roberta_base' \n",
    "#pretrained_bert = 'bert-large-uncased'\n",
    "#pretrained_bert = 'microsoft/BiomedNLP-BiomedBERT-large-uncased-abstract'\n",
    "#pretrained_bert = 'michiyasunaga/BioLinkBERT-base'\n",
    "#pretrained_bert = 'microsoft/BiomedNLP-BiomedBERT-large-uncased-abstract'\n",
    "#pretrained_bert = 'microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext'\n",
    "#pretrained_bert = 'bert-base-uncased'\n",
    "#pretrained_bert = 'monologg/biobert_v1.1 pubmed'\n",
    "\n",
    "#TRAINING FILE\n",
    "use_huggingface = False\n",
    "training_file = \"PsyTAR_JC.csv\"\n",
    "# training_file = \"**training**.csv\"\n",
    "column1_AEtext = \"test\" #\"Example\"\n",
    "column2_AEflag = \"label\" #\"AE Flag\"\n",
    "train_test_split_percent = 0.3\n",
    "balance_multiplier = 1.0\n",
    "#batch_size = 3\n",
    "balance_me = True\n",
    "#balance_me = False\n",
    "\n",
    "#TEST FILE\n",
    "test_file = \"**test**.csv\"\n",
    "# test_file = \"**test**.csv\"\n",
    "test_column1_AEtext = \"test\"\n",
    "test_column2_AEflag = \"label\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "#os.environ['WANDB_NOTEBOOK_NAME'] = 'BERTcomparison.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjon-l-collins\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'random'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = {\n",
    "        'name': 'val_loss',\n",
    "        'goal': 'minimize'\n",
    "}\n",
    "sweep_config['metric'] = metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-02 02:56:06.941683\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "print(str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 16v8r0sf\n",
      "Sweep URL: https://wandb.ai/jon-l-collins/class_psytar_cimm-kznendr-bert_2024-03-02/sweeps/16v8r0sf\n"
     ]
    }
   ],
   "source": [
    "parameters_dict = {\n",
    "    'learning_rate': {\n",
    "        'values': [5e-6,5e-5]\n",
    "    },\n",
    "    'batch_size': {\n",
    "        'values': [4,8,16]\n",
    "        },\n",
    "    \n",
    "    'dropout': {\n",
    "          'values': [0.15,0.2,0.25]\n",
    "        },\n",
    "    \n",
    "    'epochs': {\n",
    "        'values': [8]},\n",
    "    \n",
    "    'weight_decay': {\n",
    "        'values': [0.001,0.01]\n",
    "    }   \n",
    " \n",
    "}\n",
    "sweep_config['parameters'] = parameters_dict\n",
    "pretrained_bert2 = pretrained_bert.replace('/','')\n",
    "sweep_id = wandb.sweep(sweep_config, project=f\"class_psytar_{pretrained_bert2}_{str(datetime.datetime.now())[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "#from tqdm.notebook import tqdm\n",
    "#from tqdm import tqdm\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, BertConfig, AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "if use_huggingface == True:\n",
    "    dataset = load_dataset(\"ade_corpus_v2\",\"Ade_corpus_v2_classification\")\n",
    "else:\n",
    "    df = pd.read_csv(training_file, encoding='unicode_escape') #changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>extreme weight gain, short-term memory loss, h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I am detoxing from Lexapro now.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I slowly cut my dosage over several months and...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I am now 10 days completely off and OMG is it ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I have flu-like symptoms, dizziness, major moo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                test  label\n",
       "0  extreme weight gain, short-term memory loss, h...      1\n",
       "1                    I am detoxing from Lexapro now.      0\n",
       "2  I slowly cut my dosage over several months and...      0\n",
       "3  I am now 10 days completely off and OMG is it ...      0\n",
       "4  I have flu-like symptoms, dizziness, major moo...      0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_huggingface == True:\n",
    "    #convert to pandas dataframe\n",
    "    df = pd.DataFrame(dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    3841\n",
       "1    2168\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[column2_AEflag].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_AEs = df[column2_AEflag].value_counts()[1]\n",
    "balance_number = int(num_AEs * balance_multiplier)\n",
    "\n",
    "# num_AEs = 3000\n",
    "# balance_number = int(num_AEs * balance_multiplier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_AEs = 2168\n",
      "balance_number = 2168\n",
      "balance_multiplier = 1.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"num_AEs = {num_AEs}\\nbalance_number = {balance_number}\\nbalance_multiplier = {balance_multiplier}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(balance_me == True):\n",
    "    d_NoAE = df[df[column2_AEflag]==0]\n",
    "    shuffle_d_NoAE = d_NoAE.sample(frac=1,random_state=42)[:balance_number]\n",
    "    d_AE = df[df[column2_AEflag]==1]\n",
    "    #d_AE = df[df[column2_AEflag]==1][:3000]\n",
    "    df = pd.concat([shuffle_d_NoAE,d_AE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    2168\n",
       "1    2168\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[column2_AEflag].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[column1_AEtext]\n",
    "y = df[column2_AEflag]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train, y_test = train_test_split(X,y, test_size=train_test_split_percent,stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1301,), (1301,))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape , y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val,X_test_data,y_val, y_test_data = train_test_split(X_test,y_test, test_size=0.5,stratify=y_test, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3035,), (651,), (3035,), (651,), (650,), (650,))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape,X_test_data.shape, y_train.shape,y_test_data.shape, X_val.shape,y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = BertTokenizer.from_pretrained(pretrained_bert, do_lower_case=True)\n",
    "#tokenizer = AutoTokenizer.from_pretrained(pretrained_bert, do_lower_case=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_bert, add_prefix_space=True, use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['I liked that it made me seem emotionally detached events happened that would normally make me burst into tears but with Cymbalta I was emotionally free.',\n",
       "       \"I couldn't talk without stuttering, was very confused, couldn't concentrate.\",\n",
       "       'I LOVE IT!', ...,\n",
       "       'Doctor mentioned me being on the cymbalta for about a year.',\n",
       "       'It has only been two weeks since I stopped taking the med.',\n",
       "       'This drug has worked really well for my depression, and I feel is worth the side effects.'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "encoded_data_train = tokenizer.batch_encode_plus(X_train.values,\n",
    "                                                    add_special_tokens=True,\n",
    "                                                    return_attention_mask=True,\n",
    "                                                    pad_to_max_length=True,\n",
    "                                                    max_length=512,\n",
    "                                                    return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data_val = tokenizer.batch_encode_plus(X_val.values,\n",
    "                                                    add_special_tokens=True,\n",
    "                                                    return_attention_mask=True,\n",
    "                                                    pad_to_max_length=True,\n",
    "                                                    max_length=512,\n",
    "                                                    return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data_test = tokenizer.batch_encode_plus(X_test_data.values,\n",
    "                                                    add_special_tokens=True,\n",
    "                                                    return_attention_mask=True,\n",
    "                                                    pad_to_max_length=True,\n",
    "                                                    max_length=512,\n",
    "                                                    return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_train = encoded_data_train['input_ids']\n",
    "attention_masks_train = encoded_data_train['attention_mask']\n",
    "#labels = torch.tensor(dataset['AE Flag'].replace(label_dict).values)\n",
    "labels_train = torch.tensor(y_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_val = encoded_data_val['input_ids']\n",
    "attention_masks_val = encoded_data_val['attention_mask']\n",
    "#labels = torch.tensor(dataset['AE Flag'].replace(label_dict).values)\n",
    "labels_val = torch.tensor(y_val.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_test = encoded_data_test['input_ids']\n",
    "attention_masks_test = encoded_data_test['attention_mask']\n",
    "#labels = torch.tensor(dataset['AE Flag'].replace(label_dict).values)\n",
    "labels_test = torch.tensor(y_test_data.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, random_split\n",
    "dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n",
    "dataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)\n",
    "dataset_test = TensorDataset(input_ids_test, attention_masks_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ret_dataloader():\n",
    "    batch_size = wandb.config.batch_size\n",
    "    print(f\"batch_size = {batch_size}\")\n",
    "    dataloader_train = DataLoader(dataset_train, sampler=RandomSampler(dataset_train), batch_size=batch_size)\n",
    "    dataloader_validation = DataLoader(dataset_val, sampler = SequentialSampler(dataset_val), batch_size=batch_size)\n",
    "    #dataloader_test = DataLoader(dataset_test, sampler = SequentialSampler(dataset_test), batch_size=batch_size)\n",
    "    return dataloader_train, dataloader_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ret_model():\n",
    "    \n",
    "    configuration = AutoConfig.from_pretrained(pretrained_bert)\n",
    "    configuration.hidden_dropout_prob = wandb.config.dropout\n",
    "    configuration.attention_probs_dropout_prob = wandb.config.dropout\n",
    "    configuration.num_labels = 2\n",
    "    configuration.output_attentions = False\n",
    "    configuration.output_hidden_states = False\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(pretrained_bert,config=configuration)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "def ret_optim(model):\n",
    "    print(f\"learning_rate = {wandb.config.learning_rate}\")\n",
    "    optimizer = AdamW(model.parameters(),\n",
    "                      lr = wandb.config.learning_rate, \n",
    "                      eps = 1e-8,weight_decay = wandb.config.weight_decay)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ret_scheduler(optimizer, dataloader_train):\n",
    "    epochs = wandb.config.epochs\n",
    "    total_steps = len(dataloader_train) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                                num_warmup_steps = 0, \n",
    "                                                num_training_steps = total_steps)\n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(elapsed):\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "# seed_val = 17\n",
    "# random.seed(seed_val)\n",
    "# np.random.seed(seed_val)\n",
    "# torch.manual_seed(seed_val)\n",
    "# torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def f1_score_func(preds,labels):\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return f1_score(labels_flat, preds_flat, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def train():\n",
    "    wandb.init()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model=ret_model()\n",
    "    model.to(device)\n",
    "    dataloader_train, dataloader_validation = ret_dataloader()\n",
    "    optimizer = ret_optim(model)\n",
    "    scheduler = ret_scheduler(optimizer, dataloader_train)\n",
    "    training_stats = []\n",
    "    total_t0 = time.time()\n",
    "    epochs = wandb.config.epochs\n",
    "    for epoch_i in range(0, epochs):\n",
    "        print(\"\")\n",
    "        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "        print('Training...')\n",
    "        t0 = time.time()\n",
    "        total_train_loss = 0\n",
    "        model.train()\n",
    "        for step, batch in enumerate(dataloader_train):\n",
    "            if step % 200 == 0 and not step == 0:\n",
    "                elapsed = format_time(time.time() - t0)\n",
    "                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(dataloader_train), elapsed))\n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "            model.zero_grad()\n",
    "            outputs = model(b_input_ids,\n",
    "                            token_type_ids=None,\n",
    "                            attention_mask=b_input_mask,\n",
    "                            labels=b_labels)\n",
    "            loss = outputs[0]\n",
    "            wandb.log({\"train_batch_loss\":loss.item()})\n",
    "            total_train_loss += loss.item()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        avg_train_loss = total_train_loss / len(dataloader_train)\n",
    "        training_time = format_time(time.time() - t0)\n",
    "        wandb.log({\"train_loss\": avg_train_loss, \"epoch\": epoch_i})\n",
    "        print(\"\")\n",
    "        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "        print(\"  Training epoch took: {:}\".format(training_time))\n",
    "        print(\"\")\n",
    "        print(\"Running Validation...\")\n",
    "        t0 = time.time()\n",
    "        model.eval()\n",
    "        total_eval_loss = 0\n",
    "        predictions, true_vals = [], []\n",
    "        #nb_eval_steps = 0\n",
    "        for batch in dataloader_validation:\n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(b_input_ids,\n",
    "                                token_type_ids=None,\n",
    "                                attention_mask=b_input_mask,\n",
    "                                labels=b_labels)\n",
    "                loss=outputs[0] #checkleul from here downwards\n",
    "                logits=outputs[1] \n",
    "            total_eval_loss += loss.item()\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "            predictions.append(logits)\n",
    "            true_vals.append(label_ids)\n",
    "\n",
    "        predictions = np.concatenate(predictions, axis=0) #added\n",
    "        true_vals = np.concatenate(true_vals, axis=0) #added\n",
    "        val_f1 = f1_score_func(predictions, true_vals) #added\n",
    "        wandb.log({\"val_f1\": val_f1, \"epoch\": epoch_i}) #added\n",
    "        print(\"  Average validation f1: {0:.2f}\".format(val_f1))\n",
    "        avg_val_loss = total_eval_loss / len(dataloader_validation)\n",
    "        wandb.log({\"val_loss\": avg_val_loss,\"epoch\": epoch_i})\n",
    "        print(\"  Average validation loss: {0:.2f}\".format(avg_val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: y8xqyf9r with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.15\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-06\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.01\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\AI projects\\Project-BERTcomparison\\wandb\\run-20240302_025614-y8xqyf9r</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jon-l-collins/class_psytar_cimm-kznendr-bert_2024-03-02/runs/y8xqyf9r' target=\"_blank\">wandering-sweep-1</a></strong> to <a href='https://wandb.ai/jon-l-collins/class_psytar_cimm-kznendr-bert_2024-03-02' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/jon-l-collins/class_psytar_cimm-kznendr-bert_2024-03-02/sweeps/16v8r0sf' target=\"_blank\">https://wandb.ai/jon-l-collins/class_psytar_cimm-kznendr-bert_2024-03-02/sweeps/16v8r0sf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jon-l-collins/class_psytar_cimm-kznendr-bert_2024-03-02' target=\"_blank\">https://wandb.ai/jon-l-collins/class_psytar_cimm-kznendr-bert_2024-03-02</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/jon-l-collins/class_psytar_cimm-kznendr-bert_2024-03-02/sweeps/16v8r0sf' target=\"_blank\">https://wandb.ai/jon-l-collins/class_psytar_cimm-kznendr-bert_2024-03-02/sweeps/16v8r0sf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jon-l-collins/class_psytar_cimm-kznendr-bert_2024-03-02/runs/y8xqyf9r' target=\"_blank\">https://wandb.ai/jon-l-collins/class_psytar_cimm-kznendr-bert_2024-03-02/runs/y8xqyf9r</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cimm-kzn/endr-bert and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size = 8\n",
      "learning_rate = 5e-06\n",
      "\n",
      "======== Epoch 1 / 8 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    380.    Elapsed: 0:01:07.\n",
      "\n",
      "  Average training loss: 0.43\n",
      "  Training epoch took: 0:01:49\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.85\n",
      "  Average validation loss: 0.38\n",
      "\n",
      "======== Epoch 2 / 8 ========\n",
      "Training...\n",
      "  Batch   200  of    380.    Elapsed: 0:00:46.\n",
      "\n",
      "  Average training loss: 0.31\n",
      "  Training epoch took: 0:01:27\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.87\n",
      "  Average validation loss: 0.41\n",
      "\n",
      "======== Epoch 3 / 8 ========\n",
      "Training...\n",
      "  Batch   200  of    380.    Elapsed: 0:00:47.\n",
      "\n",
      "  Average training loss: 0.26\n",
      "  Training epoch took: 0:01:28\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.87\n",
      "  Average validation loss: 0.45\n",
      "\n",
      "======== Epoch 4 / 8 ========\n",
      "Training...\n",
      "  Batch   200  of    380.    Elapsed: 0:00:46.\n",
      "\n",
      "  Average training loss: 0.20\n",
      "  Training epoch took: 0:01:27\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.87\n",
      "  Average validation loss: 0.57\n",
      "\n",
      "======== Epoch 5 / 8 ========\n",
      "Training...\n",
      "  Batch   200  of    380.    Elapsed: 0:00:46.\n",
      "\n",
      "  Average training loss: 0.18\n",
      "  Training epoch took: 0:01:27\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.87\n",
      "  Average validation loss: 0.61\n",
      "\n",
      "======== Epoch 6 / 8 ========\n",
      "Training...\n",
      "  Batch   200  of    380.    Elapsed: 0:00:46.\n",
      "\n",
      "  Average training loss: 0.14\n",
      "  Training epoch took: 0:01:27\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.88\n",
      "  Average validation loss: 0.65\n",
      "\n",
      "======== Epoch 7 / 8 ========\n",
      "Training...\n",
      "  Batch   200  of    380.    Elapsed: 0:00:46.\n",
      "\n",
      "  Average training loss: 0.12\n",
      "  Training epoch took: 0:01:28\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.87\n",
      "  Average validation loss: 0.68\n",
      "\n",
      "======== Epoch 8 / 8 ========\n",
      "Training...\n",
      "  Batch   200  of    380.    Elapsed: 0:00:46.\n",
      "\n",
      "  Average training loss: 0.11\n",
      "  Training epoch took: 0:01:27\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.87\n",
      "  Average validation loss: 0.69\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇▇███</td></tr><tr><td>train_batch_loss</td><td>▇▄▄▂▃▂▂▄▅█▁▅▁▄▁▁▁▃▁▁▂▁▁▅▁▁▂▇▁▆▆▁▂▁▁▄▁▂▁▁</td></tr><tr><td>train_loss</td><td>█▅▄▃▃▂▁▁</td></tr><tr><td>val_f1</td><td>▁▇▆▆███▇</td></tr><tr><td>val_loss</td><td>▁▂▂▅▆▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>7</td></tr><tr><td>train_batch_loss</td><td>0.00137</td></tr><tr><td>train_loss</td><td>0.10802</td></tr><tr><td>val_f1</td><td>0.87053</td></tr><tr><td>val_loss</td><td>0.69119</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">wandering-sweep-1</strong> at: <a href='https://wandb.ai/jon-l-collins/class_psytar_cimm-kznendr-bert_2024-03-02/runs/y8xqyf9r' target=\"_blank\">https://wandb.ai/jon-l-collins/class_psytar_cimm-kznendr-bert_2024-03-02/runs/y8xqyf9r</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240302_025614-y8xqyf9r\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 9g68lmoi with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-06\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.01\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\AI projects\\Project-BERTcomparison\\wandb\\run-20240302_030918-9g68lmoi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jon-l-collins/class_psytar_cimm-kznendr-bert_2024-03-02/runs/9g68lmoi' target=\"_blank\">different-sweep-2</a></strong> to <a href='https://wandb.ai/jon-l-collins/class_psytar_cimm-kznendr-bert_2024-03-02' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/jon-l-collins/class_psytar_cimm-kznendr-bert_2024-03-02/sweeps/16v8r0sf' target=\"_blank\">https://wandb.ai/jon-l-collins/class_psytar_cimm-kznendr-bert_2024-03-02/sweeps/16v8r0sf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jon-l-collins/class_psytar_cimm-kznendr-bert_2024-03-02' target=\"_blank\">https://wandb.ai/jon-l-collins/class_psytar_cimm-kznendr-bert_2024-03-02</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/jon-l-collins/class_psytar_cimm-kznendr-bert_2024-03-02/sweeps/16v8r0sf' target=\"_blank\">https://wandb.ai/jon-l-collins/class_psytar_cimm-kznendr-bert_2024-03-02/sweeps/16v8r0sf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jon-l-collins/class_psytar_cimm-kznendr-bert_2024-03-02/runs/9g68lmoi' target=\"_blank\">https://wandb.ai/jon-l-collins/class_psytar_cimm-kznendr-bert_2024-03-02/runs/9g68lmoi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cimm-kzn/endr-bert and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size = 16\n",
      "learning_rate = 5e-06\n",
      "\n",
      "======== Epoch 1 / 8 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonlc\\anaconda3\\envs\\bestpytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Average training loss: 0.51\n",
      "  Training epoch took: 0:16:10\n",
      "\n",
      "Running Validation...\n",
      "  Average validation f1: 0.84\n",
      "  Average validation loss: 0.39\n",
      "\n",
      "======== Epoch 2 / 8 ========\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Ctrl-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e998bbaf782c4ae4b68ca04cab60f9cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.017 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.065962…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁</td></tr><tr><td>train_batch_loss</td><td>████▇▆▇▇▆▆▅▆▅▇▅▄▄▂▆▅▄▇▅▄▄▅▂▂▁▂▅▁█▅▂▃▅▄▂▂</td></tr><tr><td>train_loss</td><td>▁</td></tr><tr><td>val_f1</td><td>▁</td></tr><tr><td>val_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>0</td></tr><tr><td>train_batch_loss</td><td>0.56653</td></tr><tr><td>train_loss</td><td>0.51435</td></tr><tr><td>val_f1</td><td>0.83524</td></tr><tr><td>val_loss</td><td>0.38752</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">different-sweep-2</strong> at: <a href='https://wandb.ai/jon-l-collins/class_psytar_cimm-kznendr-bert_2024-03-02/runs/9g68lmoi' target=\"_blank\">https://wandb.ai/jon-l-collins/class_psytar_cimm-kznendr-bert_2024-03-02/runs/9g68lmoi</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240302_030918-9g68lmoi\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.agent(sweep_id, train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bestpytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
