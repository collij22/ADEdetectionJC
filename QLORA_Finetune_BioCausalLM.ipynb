{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "DXH8sUFrTY60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers trl peft huggingface_hub datasets bitsandbytes accelerate"
      ],
      "metadata": {
        "id": "z77hvgOP8RP2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a83c2df-3f92-4d8b-c3e9-092606361044"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/155.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m153.6/155.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.3/155.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/190.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.9/190.9 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.0/280.0 kB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "datasetname = \"collij22/adesplit\"\n",
        "#datasetname = \"collij22/jcpsytar\"\n",
        "\n",
        "#model_id=\"microsoft/BioGPT-Large-PubMedQA\"\n",
        "#model_id = \"BioMistral/BioMistral-7B\"\n",
        "#model_id = \"stanford-crfm/BioMedLM\"\n",
        "#model_id = \"PharMolix/BioMedGPT-LM-7B\"\n",
        "\n",
        "# model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "# model_id = \"facebook/opt-2.7b\"\n",
        "# model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "\n",
        "epoch=5\n",
        "learning_rate = 3e-4"
      ],
      "metadata": {
        "id": "OdAYl16L8GzG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if model_id == \"microsoft/BioGPT-Large-PubMedQA\":\n",
        "  specific_modules = [\"q_proj\",\"v_proj\",\"k_proj\",\"out_proj\"]\n",
        "elif model_id == \"BioMistral/BioMistral-7B\":\n",
        "  specific_modules = [\"q_proj\",\"v_proj\",\"k_proj\",\"o_proj\"]\n",
        "elif model_id == \"stanford-crfm/BioMedLM\":\n",
        "  specific_modules = ['c_proj', 'c_attn']\n",
        "elif model_id == \"PharMolix/BioMedGPT-LM-7B\":\n",
        "  specific_modules = [\"q_proj\",\"v_proj\",\"k_proj\",\"o_proj\"]\n",
        "elif model_id ==  \"mistralai/Mistral-7B-Instruct-v0.1\":\n",
        "  specific_modules = ['k_proj', 'o_proj', 'v_proj', 'q_proj']\n",
        "elif model_id == \"facebook/opt-2.7b\":\n",
        "  specific_modules = ['out_proj', 'q_proj', 'v_proj', 'k_proj']\n",
        "elif model_id == \"mistralai/Mixtral-8x7B-v0.1\":\n",
        "  specific_modules = ['o_proj', 'v_proj', 'q_proj', 'k_proj']\n",
        "else:\n",
        "  specific_modules = ['c_proj', 'c_attn']"
      ],
      "metadata": {
        "id": "G5NpX9FdKNvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(datasetname)"
      ],
      "metadata": {
        "id": "N2xWXm6I8NZh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269,
          "referenced_widgets": [
            "36028a4b411d4d3394931142eb863fcc",
            "44a7a2bd29374ceeb0a888d813011532",
            "a7a5b239b768445e84b6449dad92cc8f",
            "cf3626edd8934e6d8744bd0c2dde1b29",
            "ee318f151158418b8d1303bdbcc698d9",
            "63e0879aba2c4275989f231dd3302207",
            "2bc128cab4b9404ab01f96a200832206",
            "90fe61fc440b43608bc70b695990c207",
            "4d229b7ef42f4dec9e8ee220f1d2ec33",
            "82a33bbcd13941f6b96daa15850398d3",
            "1519419a099f469eb48d321c77be2152",
            "4ef85cfae98c4eac8ff728c511e2b5f5",
            "cdb141b33b8b4b9eadfb3d349d851aca",
            "d545f713c00c4a86a67f5ecb6e70b53e",
            "7b49e21437c44fd898724ee91ad4b1c6",
            "f9e218b57d5e4bd4bcd75bd78738a8d3",
            "9e87872a83a54a8181dc3071357ac466",
            "cff2e9798d5242e980e2444cd07f7f6c",
            "2e35f3deeb0f456987ffe1880351df2c",
            "97b5e4d103354ea391530d355464a94c",
            "852403f8ff6f4ca998477568b17d2885",
            "3ca6180934f748d3939b5958769979bc",
            "939612fd0f684ac4a8ec493b84c099ca",
            "9e05b7a63876443e82b742988eb976bf",
            "2649ca87b6184933825301b7d134e61e",
            "9965a7e274284edd8649a3387d55a509",
            "aff3c1f0682945548f5ddea2b9f9604e",
            "ff827a1fb9f445b993c9fa74941eb7ff",
            "b306e9a47227469097ac977368961826",
            "86d4e28c83d049ae95916c20ec3dfffd",
            "c17a4d4d73144e27acf4b2cbc3bf39be",
            "93dac97efdb340d2b02288f02fe88c1a",
            "5200ac9398f747aeb0dba831963af24c",
            "603419d75de44feda787afbd61dd81db",
            "4ee114e2c66c48878de5e27cd99562c1",
            "88c09340703d4711974a04fcbca924bc",
            "661179dda4994980aead6e8fb32c060f",
            "a44bb0c302e9406a97d60758aa0049f8",
            "6b87821900d7452085fd299d151ff023",
            "5d910a3c9d714e05921184c2ff804a47",
            "e677ab5a791d4de583517743705a3432",
            "4e155b46a85646d38911347f14c81db8",
            "cbe749e9c9564860ad2e8d1750e69d76",
            "ac482e714d274456980c11bdc5b40295"
          ]
        },
        "outputId": "55c1d00d-877a-4a69-c2d9-ef723d3b162b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/288k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "36028a4b411d4d3394931142eb863fcc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/71.9k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4ef85cfae98c4eac8ff728c511e2b5f5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "939612fd0f684ac4a8ec493b84c099ca"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "603419d75de44feda787afbd61dd81db"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_id2 = model_id.split('/')[1]"
      ],
      "metadata": {
        "id": "z0JK4WYd8Wj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KimyflpVUVMv",
        "outputId": "e47f46ff-d5bc-4071-c28d-d72b45536674"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.3.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacremoses) (2023.12.25)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.3.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sacremoses) (4.66.2)\n",
            "Installing collected packages: sacremoses\n",
            "Successfully installed sacremoses-0.1.1\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-learn\n",
        "!pip install sacremoses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HckqrIjLem8w",
        "outputId": "e738e60d-3d36-4e43-d5a1-99dad269808b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.38.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.1.0+cu121)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.27.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (2.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->transformers[torch]) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->transformers[torch]) (1.3.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.27.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.25.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.6.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (23.2)\n",
            "Collecting responses<0.19 (from evaluate)\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.13.1)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (0.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.9.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2023.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n",
            "Installing collected packages: responses, evaluate\n",
            "Successfully installed evaluate-0.4.1 responses-0.18.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers[torch]\n",
        "!pip install accelerate -U\n",
        "!pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vvp9Ue0pb2_K"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEsPKo_wdn7N",
        "outputId": "4734d0d9-5534-44f5-eb16-3729029ef812"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 3468\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 868\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-T4RGeUKHvhU"
      },
      "outputs": [],
      "source": [
        "test_data=dataset[\"test\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZNtGjmwHvhU",
        "outputId": "d803df15-1abc-480b-f42d-563dad5348a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['text', 'label'],\n",
              "    num_rows: 868\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "test_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "70ea358c9d03432ea36de77b167f3861",
            "bd0b939c0fbc4ec7b3c1a87f879718de",
            "41d9c5f6d713446f8f6dc07c8919fc0b",
            "e4fba89a69d040cabaa70802e524b008",
            "b0c85d0ddaa2475b99387f3550c1696f",
            "59b0d0d2e7de40a0b5a3c9360ef79ef7",
            "a0bb7a378f9748d9b85a09d5ddf82666",
            "deebc34197954ef682dc095a7df0e8df",
            "9442a4e5c3784e53838f4715845c3bf9",
            "ec48bc4cb0be47169fc656e72b0f0cc1",
            "014e050aa331443b9958a261cfec0d68",
            "e34bfb4e5a1a46f794b4817340b5151a",
            "75a784aa07834c1a8b52ed479f7a8289",
            "859deb4ee797413aacbc8fc6d032a63d",
            "9aee943c61c34aa1b3755368c2b141d0",
            "6a836985a0a2441b95eeee5aaa4cd2d2",
            "2a74dfe0e1b64ea3a1ce113aaac56e2e",
            "7ff19bc33dee4e59abacdde1f9d8ef5d",
            "8d7fe74e64b440f1b320b0cb11e1be3b",
            "ea7d29f0f7c94051906a12f22b7607f8",
            "52db770eec4a48d6b4ef27851c9bc897",
            "612948a7fa0d46c8b954c697a3ee955a",
            "eef1cb7f3c6f4984aef27147c69cc60e",
            "fcbf12bee9234abeb425df524f3042f7",
            "e0a2564ff5dc403cb8ef8d536730ce4f",
            "0c4fe5ace9514bbeaf95e52f072ba2cc",
            "55c4d141e03246ab8e1fb83588e30974",
            "f59064dafd7c47909eeb506a6063ecc6",
            "6c5afc566246495d951c07bb8b194a01",
            "8c0899a02d4d42cbbbb1ac554401229b",
            "cd64cf9bee0848c9b0fd16b238a19455",
            "a0198c6221fd4b6eb749a9c57906c0b0",
            "71a11336a4a74051bb8ceb3de464e377",
            "5f32446ccbb64e358701c96b63657376",
            "abaf69583cdf4998b05d6223b8bdf74a",
            "f6f774e71a12479e8b5a47867589bd85",
            "6add36e3d53948c280483c0b90b76d54",
            "f12e7633ee744a46a86373e9fe1880c5",
            "699211c47e9f4441a2906ee7bd456180",
            "876da38d4c794f3ea7f8e4753bdef230",
            "b59df6f8760e468cb99038bf884ede22",
            "a43e5457c7944ec6aa4f499d0e72bce5",
            "d2b7d2c9b2d142e193464bb44b720354",
            "d7429bde389746c7acfc911ea109cdd7"
          ]
        },
        "id": "0p8C6lQ_UVM0",
        "outputId": "44308a79-b96d-427e-a702-c8394952fe40"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/272 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "70ea358c9d03432ea36de77b167f3861"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.24M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e34bfb4e5a1a46f794b4817340b5151a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/566k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eef1cb7f3c6f4984aef27147c69cc60e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/119 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5f32446ccbb64e358701c96b63657376"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "#tokenizer.pad_token = tokenizer.eos_token\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "a18bc377e112449484ad2da6f2daa2a0",
            "c104c5a7cbbe49e1a2582071d89e89a9",
            "6e4c624ad67f48179d29addbbcab5201",
            "5dd35bd585f244238e3e58ab399833b0",
            "87f2de2dc28b4e5eae57a5e74e611750",
            "26890b43686a42f6b6954664e0732677",
            "e22e4024e2174e36a83410b7e75ac841",
            "5f44e79275bf43568533edda0828beb8",
            "64ae1961a5d74d92862eca0a6cb9ca96",
            "5228af781ddf4f75be91bdcd87129aaf",
            "155475e0b46448a4b7cf27ce7570cf46",
            "6fed378bb6e640cc8cae4c9a890623ad",
            "406f9521438a4f36b616951d00402e16",
            "e0e2b96236484e3e8ce6a118b30e28b7",
            "b8094cce9f1d45e2a4bd98bf7fd26154",
            "7dddd7493f344016ada740b5784e41ab",
            "243364dd77e844b2a37f67df8ab8b196",
            "1dd5f53f8bd24c56a827a2c24facf655",
            "f4fe718d5e6b4b179fd4e7c0fc5132c6",
            "3d0aa2527b874249bc96167bfc38aad9",
            "244ca21a3a7647be96622f1b4b0ab155",
            "792b05fa45184729b9a6ee3dee08e237"
          ]
        },
        "id": "KJY4Xa1HznjZ",
        "outputId": "d84aebb1-649d-48c9-be98-c0c0aba40804"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/3468 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a18bc377e112449484ad2da6f2daa2a0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/868 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6fed378bb6e640cc8cae4c9a890623ad"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "def format_prompt(examples):\n",
        "    examples[\"text\"] = \"### Instruction: If the input contains both the drug name and its effect, classify it as 1; otherwise, classify it as 0 ### Input: \"+examples[\"text\"]+\"### Response: {}\".format(examples[\"label\"])\n",
        "    return examples\n",
        "dataset=dataset.map(format_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2XPWd04WX3Qf"
      },
      "outputs": [],
      "source": [
        "cutoff_len = 512\n",
        "train_on_inputs = True\n",
        "add_eos_token = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XA-NzcZgXoOb"
      },
      "outputs": [],
      "source": [
        "tokenizer.pad_token_id = 0  # unk. we want this to be different from the eos token\n",
        "tokenizer.padding_side = \"left\"  # Allow batched inference\n",
        "\n",
        "def tokenize(prompt, add_eos_token=True):\n",
        "    # there's probably a way to do this with the tokenizer settings\n",
        "    # but again, gotta move fast\n",
        "    result = tokenizer(\n",
        "        prompt,\n",
        "        truncation=True,\n",
        "        max_length=cutoff_len,\n",
        "        padding=False,\n",
        "        return_tensors=None,\n",
        "    )\n",
        "    if (\n",
        "        result[\"input_ids\"][-1] != tokenizer.eos_token_id\n",
        "        and len(result[\"input_ids\"]) < cutoff_len\n",
        "        and add_eos_token\n",
        "    ):\n",
        "        result[\"input_ids\"].append(tokenizer.eos_token_id)\n",
        "        result[\"attention_mask\"].append(1)\n",
        "\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "\n",
        "    return result\n",
        "\n",
        "def generate_and_tokenize_prompt(full_prompt):\n",
        "    tokenized_full_prompt = tokenize(full_prompt[\"text\"])\n",
        "    if not train_on_inputs:\n",
        "        tokenized_user_prompt = tokenize(tokenized_full_prompt, add_eos_token=add_eos_token)\n",
        "        user_prompt_len = len(tokenized_user_prompt[\"input_ids\"])\n",
        "\n",
        "        if add_eos_token:\n",
        "            user_prompt_len -= 1\n",
        "\n",
        "        tokenized_full_prompt[\"labels\"] = [\n",
        "            -100\n",
        "        ] * user_prompt_len + tokenized_full_prompt[\"labels\"][\n",
        "            user_prompt_len:\n",
        "        ]  # could be sped up, probably\n",
        "    return tokenized_full_prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "802bc98fa9044b9384254427a2f4d9bc",
            "dc8694ea040f4fc494a567c609ce7547",
            "26f567e439d64c148a8a727822cd3878",
            "0e3e1516a1f745e1ae82b22d555b0af4",
            "bc683fa93bd54192b249e7f912fed7bc",
            "27e8b41099b843e4b6d0f20505e3a192",
            "427be21c5a054627837596fddddf847e",
            "3643ff39c0824c74ad6826fe8884b99c",
            "5cffc015673d4039b684ab8ce92b085f",
            "00653befedb1481f91bd9900e20a85f7",
            "4f1c8907ca9848f9bb270d40188a66eb"
          ]
        },
        "id": "8r2qlZYgZerS",
        "outputId": "735db5f5-232e-4814-f551-40f5a01e3008"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/3468 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "802bc98fa9044b9384254427a2f4d9bc"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "train_data = dataset[\"train\"].shuffle().map(generate_and_tokenize_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UrF03jEiaYdh",
        "outputId": "e51c9eae-ab22-420c-f991-0b019e1eb843"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['text', 'label', 'input_ids', 'attention_mask', 'labels'],\n",
              "    num_rows: 3468\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "train_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "8223592c55524f119c6cd3b94a7be0c2",
            "f7748307327949fd96c6b3ffc351e3d2",
            "9e5d74dec9f7407dadb2b015818c01f3",
            "fe8d33f6123d41789fc3a6bfa94378a9",
            "14b01a03ab1b4990b59cad841b0bf159",
            "a97fc445a4bf4446af5ec05bc1bc3779",
            "f0cfc56ceb2e4b67914cd84bc50bc471",
            "1b0e2e8021934169a1827eca5988f9cd",
            "61180056bc4044d0966a727840b0a199",
            "236fd0f82b014c63b580fac9bf20baa2",
            "43028c4aba7d45d7a05347d5fe3a98e7"
          ]
        },
        "id": "ClgDc8Vuae9A",
        "outputId": "810b9b6a-b4b9-477d-8c6a-a410bcadd1ff"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/3468 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8223592c55524f119c6cd3b94a7be0c2"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "train_data = train_data.map(\n",
        "        remove_columns=[\"text\",\"label\"]\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A0LOAARmcERA"
      },
      "outputs": [],
      "source": [
        "small_train_data=train_data.shuffle()#.select(range(1000))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uC5jshWRfUGU"
      },
      "outputs": [],
      "source": [
        "from transformers import BitsAndBytesConfig\n",
        "import torch\n",
        "bnb_config = BitsAndBytesConfig(load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185,
          "referenced_widgets": [
            "50615d3df959474fba7ae16c48254b4f",
            "3c85b9c658f44176a45a513c7b546682",
            "cf9a20b1f79b4e72934600baee2829fa",
            "a3198ce75aab4720ba83879d15fd8a59",
            "72845d1d9af7400cbbb645d21c198d78",
            "38ab9322eef847b89e494975b8d312b4",
            "7a6453e1536e48eaad9316d8d01c613f",
            "be348d8230b543e0b21723679edbe292",
            "89f1fff74f5b4eb8adb2a2a80f4a8138",
            "4759b7ec3e224524b31f473254b8a0a4",
            "bcf8bfadaebb423784757af68d671a19",
            "e402e0988bb5452285977524e1c47d00",
            "064ceda18353498db239a04cb9a1d237",
            "a061ecb987a64f1eb9640fa7029e95c6",
            "53cdce8475294e9aa6a1f4d4a15d6b93",
            "9d9900eaa278498b9a8b597a6793eca0",
            "895c67315dae40eea924786297836035",
            "294a52faa0fa482d85649b6683971dcf",
            "16a06c3b9b69409892f2b03312ba2143",
            "57d82bae8f3e4b0c8a7170b382879089",
            "4a9a0e810f0b41789177709568202247",
            "2de1ad38ac084acbbe10a8c8dac01e3a",
            "82a4248dd1e34faaa2d0602701699c90",
            "cdffd921917b47858416c8811e04ee8e",
            "1757163ddc964c709845c8c185b6cbf5",
            "aade50e1ae3245d0b16fa60408e1c939",
            "3a02e25e269d4eb7983848b23d0baa0d",
            "6356d636f4f34931b42a92995e5680a9",
            "900afab4b1214edebe3cc883a6593d38",
            "5cf1de602efd48d999e723b72d126dbf",
            "55779c02c6674f3fb8bae0cc7a6ecac6",
            "2e9d830bd78748f29d77b6183a79bc3f",
            "3a732252e1c5464c87eb022cbb326a95"
          ]
        },
        "id": "BKKrio43eTz0",
        "outputId": "d8a9af46-362f-48d9-e402-9b9711d06e7e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/674 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "50615d3df959474fba7ae16c48254b4f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/6.29G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e402e0988bb5452285977524e1c47d00"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  return self.fget.__get__(instance, owner)()\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "82a4248dd1e34faaa2d0602701699c90"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "                                                model_id,\n",
        "                                                torch_dtype=torch.bfloat16,\n",
        "                                                quantization_config=bnb_config,\n",
        "                                                use_cache=False\n",
        "                                            )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHaxCiTTUVM9"
      },
      "outputs": [],
      "source": [
        "from peft import prepare_model_for_kbit_training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yeGSbBMaUVM9"
      },
      "outputs": [],
      "source": [
        "model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-ywiWmtedJc"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "training_args = TrainingArguments(output_dir=\"test_trainer2\",num_train_epochs=epoch,save_steps=500,per_device_train_batch_size=1,\n",
        "                                gradient_accumulation_steps=4,\n",
        "                                 learning_rate=learning_rate,\n",
        "                                 max_grad_norm=1.0,\n",
        "                                  lr_scheduler_type=\"linear\",\n",
        "                                  warmup_steps=5,\n",
        "                                  optim=\"paged_adamw_8bit\"\n",
        "                                 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "24e3b5069a764e4d91acc609c259d109",
            "2e9557f891e442579f94f56ecac3fd61",
            "ffc897e11de34211b0be1a7ef8a52072",
            "62f42f64cb714a86a09e20094d1c5db8",
            "0eca257fdf3740fcb5aa5fe20d582782",
            "c38f01bd93a34acf999b9a67e22737c4",
            "1a20137145cb4903861a2e8768658acc",
            "f67467f50060437f89307c41acfaf780",
            "da5d1cd5fd26442f99497133b3acba7d",
            "fcde4aca83ac4aeeaa481099c290858f",
            "19437398584b4692aa88a320b38bb143"
          ]
        },
        "id": "8e-XF0lHeKdT",
        "outputId": "c50e2134-1644-4c2d-c33f-38cc303b8bda"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "24e3b5069a764e4d91acc609c259d109"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "import numpy as np\n",
        "import evaluate\n",
        "metric = evaluate.load(\"accuracy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1GS0nIwLUVM_"
      },
      "outputs": [],
      "source": [
        "import bitsandbytes as bnb\n",
        "def find_all_linear_names(model):\n",
        "    cls = bnb.nn.Linear4bit #if args.bits == 4 else (bnb.nn.Linear8bitLt if args.bits == 8 else torch.nn.Linear)\n",
        "    lora_module_names = set()\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, cls):\n",
        "            names = name.split('.')\n",
        "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
        "\n",
        "\n",
        "    # lm_head is often excluded.\n",
        "    if 'lm_head' in lora_module_names:  # needed for 16-bit\n",
        "        lora_module_names.remove('lm_head')\n",
        "    return list(lora_module_names)\n",
        "\n",
        "\n",
        "modules = find_all_linear_names(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k64pUSS1UVM_"
      },
      "outputs": [],
      "source": [
        "#comment out to use all linear modules\n",
        "modules=specific_modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hcur0yIPUVM_"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig, TaskType\n",
        "dropout=0.2\n",
        "lora_config = LoraConfig(\n",
        "    task_type=\"CAUSAL_LM\", r=4,inference_mode=False, lora_alpha=8, lora_dropout=dropout,bias=\"none\",target_modules=modules\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRROj91bUVNA",
        "outputId": "323f4ba6-c611-48e9-eb3b-e510f3ef45d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BioGptForCausalLM(\n",
              "  (biogpt): BioGptModel(\n",
              "    (embed_tokens): Embedding(57726, 1600, padding_idx=1)\n",
              "    (embed_positions): BioGptLearnedPositionalEmbedding(2050, 1600)\n",
              "    (layers): ModuleList(\n",
              "      (0-47): 48 x BioGptDecoderLayer(\n",
              "        (self_attn): BioGptAttention(\n",
              "          (k_proj): Linear4bit(in_features=1600, out_features=1600, bias=True)\n",
              "          (v_proj): Linear4bit(in_features=1600, out_features=1600, bias=True)\n",
              "          (q_proj): Linear4bit(in_features=1600, out_features=1600, bias=True)\n",
              "          (out_proj): Linear4bit(in_features=1600, out_features=1600, bias=True)\n",
              "        )\n",
              "        (activation_fn): GELUActivation()\n",
              "        (self_attn_layer_norm): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (fc1): Linear4bit(in_features=1600, out_features=6400, bias=True)\n",
              "        (fc2): Linear4bit(in_features=6400, out_features=1600, bias=True)\n",
              "        (final_layer_norm): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "    )\n",
              "    (layer_norm): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (output_projection): Linear(in_features=1600, out_features=57726, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nUGUWAFbUVNB"
      },
      "outputs": [],
      "source": [
        "from peft import get_peft_model\n",
        "model = get_peft_model(model, lora_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BWmibXS_Hvhg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "361948b3-ad96-4e94-bd25-cf05bcbc4030"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainable params: 2457600 || All params: 836380800 || Trainable%: 0.29383744820541075\n"
          ]
        }
      ],
      "source": [
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(f\"Trainable params: {trainable_params} || All params: {all_param} || Trainable%: {100 * trainable_params / all_param}\")\n",
        "\n",
        "# Example usage (assuming a model variable exists)\n",
        "print_trainable_parameters(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ze3T3e3GfY6R"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForSeq2Seq\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=DataCollatorForSeq2Seq(\n",
        "            tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n",
        "        ),\n",
        "    train_dataset=small_train_data\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rj5zgifDHvhg"
      },
      "outputs": [],
      "source": [
        "def format_time(elapsed):\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# if model_id == \"stanford-crfm/BioMedLM\":\n",
        "#   model.config.pad_token_id=model.config.eos_token_id\n",
        "# elif model_id == \"BioMistral/BioMistral-7B\":\n",
        "#   model.config.pad_token_id=model.config.eos_token_id\n",
        "# elif model_id == \"stanford-crfm/BioMedLM\":\n",
        "#   model.config.pad_token_id=model.config.eos_token_id\n",
        "# elif model_id == \"stanford-crfm/BioMedLM\":\n",
        "#   model.config.pad_token_id=model.config.eos_token_id\n",
        "# elif model_id == \"stanford-crfm/BioMedLM\":\n",
        "#   model.config.pad_token_id=model.config.eos_token_id"
      ],
      "metadata": {
        "id": "5eiperN_l6FG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14RiJZoG0bBo",
        "outputId": "7b8d5f69-2d68-4ea0-c23b-67ff4f93ceb3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1734' max='1734' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1734/1734 55:00, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.354800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>1.100500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>1.063400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import datetime\n",
        "t0 = time.time()\n",
        "trainer.train()\n",
        "training_time = format_time(time.time() - t0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uqFvYyrDHvhg"
      },
      "outputs": [],
      "source": [
        "training_time = format_time(time.time() - t0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MIgoKhX6Hvhg",
        "outputId": "3cfb91c8-2124-428d-9dcf-fee026ce5d8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0:55:04'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "training_time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "now = datetime.now()\n",
        "date_time_string = now.strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
        "print(date_time_string)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "woaq6WW_5qmV",
        "outputId": "ea024bb9-8e2b-461c-a129-a4dbbcc0fc58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-03-02_17:12:46\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E4c_sjeUUVNF"
      },
      "outputs": [],
      "source": [
        "trainer.model.save_pretrained(\"finetuned_model\"+model_id2+date_time_string)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gD_iaSJ8UVNG"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "import torch\n",
        "device_map = {\"\": 0}\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    low_cpu_mem_usage=True,\n",
        "    return_dict=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=device_map,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVIBbfh5miJI"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig, PeftModel\n",
        "model = PeftModel.from_pretrained(base_model, \"finetuned_model\"+model_id2+date_time_string)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0tVZHlPmZfM"
      },
      "outputs": [],
      "source": [
        "model = model.merge_and_unload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YeAkPvePUVNI"
      },
      "outputs": [],
      "source": [
        "text = \"### Instruction: If the input contains both the drug name and its effect, classify it as 1; otherwise, classify it as 0 ### Input: \"+\"Unaccountable severe hypercalcemia in a patient treated for hypoparathyroidism with dihydrotachysterol.\t\t### Response:\"\n",
        "model_input = tokenizer(text, return_tensors=\"pt\").to(0)\n",
        "with torch.no_grad():\n",
        "    out = model.generate(**model_input, max_new_tokens=100)\n",
        "c=tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "#splitter=c[-17:-1]\n",
        "if \"###\" in c:\n",
        "  splitter=\"### Response: \"\n",
        "elif \"# # #\" in c:\n",
        "  splitter=\"# # # Response: \"\n",
        "else:\n",
        "  print(\"problem with the generation\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "splitter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ZJn4XnS0tb7W",
        "outputId": "77c6fa5b-2d46-42f9-bb90-59c9e4b01c06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'# # # Response: '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yN69lJXQ75jh"
      },
      "outputs": [],
      "source": [
        "#first time\n",
        "import pandas as pd\n",
        "columns=[\"model_name\",\"Training Time\",\"precision\",\"recall\",\"f1_Score\",\"True Positives\",\"False Positives\",\"True Negatives\",\"False Negatives\",\"Epochs\",\"Learning Rate\",\"Drop Out\",\"Batch Size\",\"Weight Decay\"]\n",
        "df = pd.DataFrame(columns=columns)\n",
        "df.to_csv(f\"results_{datasetname.split('/')[1]}_FT_{model_id2}.csv\",index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8hq58AU7lm2"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import recall_score,f1_score,precision_score\n",
        "c=test_data[\"text\"][:100]\n",
        "l=test_data[\"label\"][:100]\n",
        "predicted=[]\n",
        "err_predictions=[]\n",
        "for item in c:\n",
        "    text = \"### Instruction: If the input contains both the drug name and its effect, classify it as 1; otherwise, classify it as 0. Do not generate any thing else other than the labels ### Input: \"+item+ \"### Response:\"\n",
        "    model_input = tokenizer(text, return_tensors=\"pt\").to(0)\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(**model_input, max_new_tokens=100)\n",
        "    #print(tokenizer.decode(out[0], skip_special_tokens=True))\n",
        "    res = tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "    t = res.split(splitter)[1][0]\n",
        "    if t.isnumeric():\n",
        "      prediction=int(t)\n",
        "    else:\n",
        "      err_predictions.append(t)\n",
        "      prediction=0\n",
        "    predicted.append(prediction)\n",
        "conf = confusion_matrix(l,predicted)\n",
        "recall = recall_score(l,predicted, pos_label=1)\n",
        "f1 = f1_score(l,predicted, pos_label=1)\n",
        "precision = precision_score(l,predicted, pos_label=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# res.split(\"# # # Response: \")"
      ],
      "metadata": {
        "id": "RDPjbQoQqyxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#res"
      ],
      "metadata": {
        "id": "L_6k_d6RrA6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "njh0LwU1Hvhi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ede72bd-4732-407e-a373-c9d4619a3a45"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "len(err_predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMeKlXA-Hvhi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1e532f6-7cf2-46a0-f633-d7ea08559c93"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "len(predicted)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(l)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMepSuHqr1_4",
        "outputId": "9f201ea6-7df5-42e7-d37d-161c7a63965b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNLmY7gIHvhi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb0a12a6-022f-4fdd-d1de-c8b42f90fcc6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8282828282828283"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ],
      "source": [
        "f1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_id3=f\"{datasetname.split('/')[1]}_{model_id}\""
      ],
      "metadata": {
        "id": "R7wzcrFHWUwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B3bPyMPDjeHb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "def add_row():\n",
        "  df=pd.read_csv(f\"results_{datasetname.split('/')[1]}_FT_{model_id2}.csv\")\n",
        "  data=[model_id3,training_time,precision,recall,f1,conf[0][0],conf[0][1],conf[1][1],conf[1][0],epoch,learning_rate,dropout,1,0]\n",
        "  df.loc[len(df)] = data\n",
        "  df.to_csv(f\"results_{datasetname.split('/')[1]}_FT_{model_id2}.csv\",index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U6Pr-mKzHvhj"
      },
      "outputs": [],
      "source": [
        "add_row()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHAFW0yMjco9"
      },
      "outputs": [],
      "source": [
        "df_result= pd.DataFrame({\"text\":c,\"ground truth\":l,\"predictions\":predicted,\"model name\":model_id})\n",
        "df_result.to_csv(f\"testpreds_{datasetname.split('/')[1]}_FT_{model_id2}.csv\",index=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model.push_to_hub(f\"collij22/{datasetname.split('/')[1]}_FT_{model_id.split('/')[1]}\",token = \"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\")\n",
        "# tokenizer.push_to_hub(f\"collij22/{datasetname.split('/')[1]}_FT_{model_id.split('/')[1]}\",token = \"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\")"
      ],
      "metadata": {
        "id": "SwrLYjDSDvVt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}